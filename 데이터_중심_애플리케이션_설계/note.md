# 데이터 중심 애플리케이션 설계

* https://github.com/ept/ddia-references 에서 항상 레퍼런스를 볼 수 있다. 

데이터 양, 복잡성, 데이터 변하는 속도 등 데이터가 주요 도전 과제인 애플리케이션을 데이터 중심적이 라고 말한다.

CPU 사이클이 병목인 경우 계산 중심적 이라고 함. 



# 1장 신뢰할 수 있고 확장 가능하며 유지보수 하기 쉬운 애플리케이션

현대 많은 애플리케이션은 cpu 계산 대신 데이터 중심적으로, 데이터 양 때문에 문제가 생김

많은 애플리케이션은 다음과 같은 컴포넌트를 필요로 함

* 데이터베이스 : 데이터 저장

* 캐시 : 읽기 속도 향상을 위한 수행 결과를 기억
* search index : 키워드로 빠르게 데이터 검색
* 스트림 처리 : 비동기 처리를 위한 다른 프로세스의 호출
* 배치 처리 : 대량의 데이터를 분석

대부분의 소프트웨어 시스템에서 중요하게 여기는 3가지 관심사

* 신뢰성 : 시스템은 어떤 일이 있어도 지속적으로 올바르게 동작해야 함 
* 확장성 : 데이터 양, 트래픽 양, 복잡도가 증가해도 이를 적절하게 처리할 수 있어야 함 
* 유지보수성  : 현재 작업을 유지보수하고 새로운 사례를 시스템에 적용할 수 있어야 함

## 신뢰성(Reliability)

소프트웨어의 경우 100% 오류 없이 동작하는 경우는 없으므로 올바르게 동작함이란건 무언가 잘못되더라도 지속적으로 올바르게 동작함으로 해석 할 수 있다.

용어

* 결함(fault ) : 잘못될 수 있는 일.
* 내결함성 또는 탄력성 : 결함을 예측하고 대처할 수 있는 시스템
  * 내결함성을 이야기할때 모든 종류의 결함을 견딜 수 있는 시스템은 절대 없다.

결함 != 장애. 결함은 사양에서 벗어난 시스템의 한 구성 요소지만, 장애는 시스템 전체가 멈춘 경우.

넷플릭스에서 개발한 도구인 카오스 몽키는, 시스템의 내결함성을 테스트하는 도구로, 실제 운영 환경에서 무작위로 서버나 인스턴스를 종료시켜, 예상치 못한 장애 상황에 시스템이 어떻게 반응하는지 확인할 수 있도록 도와준다.

### 하드웨어 결함

하드디스크의 평균 장애시간은 10~50년. 10,000개의 디스크로 구성된 클러스터는 평균 하루에 1개씩 죽음. 

* 이말이 무슨뜻이냐면, 개별 디스크의 수명은 길지만 대량의 하드디스크 운영시 매일 고장날 확률이 높다는것

데이터 양이 늘어나면서 더 많은 수의 장비가 필요해졌고, 이는 하드웨어 결함율을 높이게 됌 

이런 하드웨어 결함은 하드웨어 중복성을 추가해 전체 장비의 손실을 견딜 수 있는 시스템으로 점점 변하고 있다.

그리고 소프트웨어 오류보다는 오류 처리가 더 쉽다

### 소프트웨어 오류

시스템 내 오류는 하드웨어 결함보다 더 오류를 많이 유발하며 예상하기 어렵다.

* 리눅스 커널 버그, 공유 자원 과도 서비스, 동시성 문제, 매우 느린 응답 서버, 연쇄 장애

이런 버그들은 신속한 해결책이 없다. 

때문에

* 시스템 상호작용 주의 깊게 생각하기, 빈틈없는 테스트, 프로세스 격리, 프로세스 재시작, 모니터링

등으로 문제 해결을 해야만 한다

## 확장성

동시 사용자 수가 많아질수록 현재 시스템이 미래에도 안정적으로 동작한다는 보장은 없음.

확장성은 증가한 부하에 대처하는 시스템 능력을 설명함.

확장성을 논한다는 것은, 시스템이 특정 방식으로 커지면 이에 대처하기 위한 선택이나 추가 부하를 다루기 위해 자원을 어떻게 투입할까 같은 질문을 고려한다

### 부하 기술하기

시스템의 현재 부하를 간결하게 계산 및 기술하여 부하가 몇배로 늘어날 경우 어떻게 될건지 논의 가능.

부하는 부하 매개변수라고 부를 숫자로 나타냄

* 웹 서버 초당 요청 수, 읽기 쓰기 비율, 동시 활성 사용자 수, 캐시 적중률 등이 될 수 있음.

2012년 트위터 데이터 토대로 계산 예시

* 트윗 작성 : 평균 초당 4.6k, 피크 초당 12k

초당 12,000건의 쓰기 처리는 쉽지만, 트위터의 트윗은 패아웃 방식으로 다수의 팔로워에게 전파되는 문제가 있음.

개별 사용자는 많은 사람을 팔로우 하고, 많은 사람이 개별 사용자를 팔로우 함.

2가지 방법

1. 트윗 작성시 전역 트윗 컬렉션에 삽입. 사용자가 자신의 홈 타임라인 요청시 팔로우하는 모든 사람을 찾고, 모든 트윗을 찾아 시간순으로 정렬해서 합침 

```sql
SELECT 
    tweets.*, 
    users.* 
FROM tweets
JOIN users ON tweets.sender_id = users.id
JOIN follows ON follows.followee_id = users.id
WHERE follows.follower_id = current_user;
```

2. 각 사용자용 타임라인을 만들고, 사용자가 트윗작성시, 해당 사용자를 팔로우하는 모든 사람을 찾아 각자의 타임라인에 새 트윗 삽입

![image-20250314170823166](./images//image-20250314170823166.png)

트위터는 초기 접근 방식 1을 사용, 그러나 유저가 늘어날수록 질의 부하가 어렵기 때문에 접근방식 2로 전환(각 사용자용 타임라인에 쓰기) 함.

근거: 트윗 게시 쓰기 요청량이 홈 타임라인 읽기 요청량보다 수백배 적음. 즉 쓰기 시점에 더 많은일을 하고, 읽기 시점에 적은일을 하게 함

> 잠깐, fan out의 종류
>
> 팬아웃(Fan-out)**이란, **한 사용자의 활동(예: 트윗 작성)이 다수의 팔로워에게 전파되는 방식**
>
> **쓰기 시 팬아웃(Write-time Fan-out)** → 데이터를 생성할 때 미리 전파
>
> * 데이터 생성시, 모든 수신자(팔로워)에게 복사하여 저장하는 방식 
>
> - 장점 : 팔로워가 타임라인 조회할때 빠름, 읽기부하 적음
> - 단점 : 팔로워 많을수록 쓰기부하 급증, 중복 데이터로 저장공간 증가 
> - 페이스북, 트위터(일반사용자)
>
> **읽기 시 팬아웃(Read-time Fan-out)** → 데이터를 가져올 때 전파
>
> * 읽기 요청 발생시에 그제서야 데이터를 불러오는 방식
> * 장점 : 쓰기 부하가 낮고, 팔로워 수가 많아도 부담 적음
> * 단점 : 팔로워가 타임라인 조회할때마다 DB 조회 부하 증가 
> * 트위터(셀럽계정), 인스타 피드조회 
>
> **하이브리드 팬아웃(Hybrid Fan-out)** → 상황에 따라 두 가지 방식을 조합
>
> * Write-time과 Read-time 팬아웃을 조합하여 최적화한 방식
> * 특정 기준(예: 팔로워 수, 인기도 등)에 따라 팬아웃 방식을 다르게 적용
>   * 일반 사용자는 **Write-time Fan-out** (팔로워가 적으므로 미리 저장)
>   * 셀럽이나 유명 계정은 **Read-time Fan-out** (팔로워가 많아서 실시간 조회)
>
> 트위터는 조합해서 사용함
>
> **팔로워가 적은 일반 유저** → **쓰기 시 팬아웃** (팔로워 타임라인에 미리 저장)
>
> **팔로워가 많은 유명인(셀럽)** → **읽기 시 팬아웃** (타임라인 조회 시 가져오기)
>
> - 예를 들어 **일론 머스크 같은 사용자가 트윗을 올리면**, 수천만 명의 타임라인을 업데이트하는 대신, **트윗 자체를 조회 시 불러오는 방식(Read-time Fan-out)을 사용**함.

그러나 적븐방식 2의 불리한점은, 트윗 작성이 많은 부가 작업을 필요로 함

* 평균 트윗이 75명의 팔로워에게 전달시, 초당 4.6k는 345k 쓰기.

* 팔로워 수가 늘어날수록 쓰기 부하는 당연히 늘어남.

그렇다면 사용자당 팔로워 분포랑 쓰기 량을 확인해서 팬아웃 부하를 줄여야 함. 

### 성능 기술하기

시스템 부하를 기술하면 부하 증가시 어떤일이 발생할지 조사 가능.

부하 매개변수: ex) 초당 4.6k, 피크 12k

* 부하 매개변수 증가시키고, CPU, 메모리 대역폭 등은 변경하지 않으면 시스템은 어떻게 될까
* 부하 매개변수 증가시, **성능이 변하지 않고 유지되길 원한다면 자원을 얼마나 많이 늘려야 할까**.
  * 이것은 어떻게 조정하여 개선했다는 뜻임.

하둡같은 일괄 시스템은 초당 throughtput이 중요

온라인 서비스는 response time이 더 중요 



response time 생각시 고려할점

* 모든 요청이 다 매번 응답 시간이 다름. 때문에 단일 숫자 대신 측정 가능한 값의 **분포로 생각해야 함**

* 분포란, p95, p99 처럼 전체 응답 시간중 n퍼의 요청이 이 ms 내에 처리됨을 의미함 

  * 평균은 좋지 않음. 얼마나 많은 사용자가 실제로 지연을 경험했는지 알려주는 값이 아니기 떄문  
  * 평균(Mean)은 데이터의 **전체적인 중심값을 보여주지만**, **극단적인 값(아웃라이어, Outlier)의 영향을 많이 받기 때문에**임. 높은 평균이 나머지 아주 안좋은 값들을 평균처럼 끌어올림 

  * 때문에 백분위를 사용하는게 좋음. 가장 빠른시간부터 느린시간까지 쭉 정렬하고 사용. 예를들어 중간 응답 시간이 200 ms면 요청의 반이 200ms 미만으로 반환되고 나머지는 그거보다 오래걸린다는것. 

* 95, 99, 99.9 분위가 일반적임.

  * p95 가 1.5s라면 100개의 요청중 95개는 1.5s 미만이고 나머지는 그 이상 
  * 그렇다고 100%를 할 순 없는게, 0.1만 올리려고 해도 어마어마한 비용이 들 확률이 높아 트레이드 오프임 

queueing delay도 응답시간의 상당 부분을 차지함. 소수의 느린 요청 때문에 후속 요청 처리가 지체되는 현상을 head-of-line blocking 이라 함. 

때문에 시스템의 확장성을 테스트하는 부하테스트는 응답시간과 독립적으로 지속적으로 요청을 보내야 더 정확한 결과를 알 수 있음. 

### 부하 대응 접근 방식

부하 매개변수가 어느정도 증가하더라도 좋은 성능을 유지하려면?

아키텍처를 결정하는 요소는 읽기,쓰기 양, 저장할 데이터의 양, 데이터 복잡도, 응답 시간 요구사항, 접근 패턴 등이 있다.

예를 들어 각 크기가 1kB인 초당 100,000건의 요청을 처리하도록 설계한 시스템과 각 크기가 2GB인 분당 3건의 요청을 처리하기 위해 설계한 시스템은 서로 같은 데이터 처리량이라 해도 매우 다르다.

1. 초당 10만건의 경우

   * 특징 

     - 초당 100,000건 → **고처리량(High Throughput)**

     - 요청당 1KB → **작은 요청 크기(Small Payload)**

     - **높은 동시성(Concurrency) 필요**

     - 짧은 응답 시간(Low Latency) 요구 가능성 큼

   * **로드 밸런싱 (Load Balancing)**

     - 여러 애플리케이션 서버로 트래픽을 분산
     - **NGINX, Envoy, HAProxy** 같은 L7 로드 밸런서 사용
     - 필요하면 **L4 로드 밸런서**(AWS ELB, GCP Load Balancer) 도입

     **비동기 처리 (Asynchronous Processing)**

     - 웹 서버에서 직접 처리하지 않고 **메시지 큐(Kafka, RabbitMQ, Redis Streams)** 에 태우고 백엔드에서 비동기 처리
     - 응답 속도가 중요한 경우 **빠른 응답 후 백그라운드 처리**

     **수평 확장 (Horizontal Scaling)**

     - 애플리케이션 서버를 여러 대 배포하여 병렬 처리
     - **Kubernetes(K8s) 기반의 오토스케일링(Auto-scaling)**
     - 무상태(Stateless) 설계 → 서버 간 부하 분산 용이

     **효율적인 DB 설계**

     - **읽기/쓰기 분리 (Read/Write Splitting)** → 읽기 요청이 많다면 **레플리카 DB(Read Replica)** 활용
     - **캐싱 (Redis, Memcached)** → 자주 조회되는 데이터는 DB 부하 감소
     - **Sharding (분할 저장)** 고려

     **고성능 네트워크 설계**

     - HTTP/2 또는 gRPC 사용 → 작은 요청을 빠르게 처리하는데 유리
     - 커넥션 풀링(Connection Pooling)으로 TCP 오버헤드 감소

     **로그 및 모니터링**

     - **Prometheus, Grafana, ELK Stack(Elasticsearch, Logstash, Kibana)** 사용
     - 높은 트래픽에서 장애 탐지를 위해 **분산 트레이싱(OpenTelemetry, Jaeger)** 도입

2. 분당 3건 요청 (각 크기가 클때)

   * 특징

     - 분당 3건 → **낮은 요청 빈도**

     - 요청당 2GB → **대용량 데이터 전송(High Payload)**

     - 실시간성이 중요하지 않을 가능성 큼

     - 데이터 무결성(Data Integrity) 보장이 중요할 가능성이 높음

   * **전용 대역폭 및 네트워크 튜닝**
     - HTTP보다는 **gRPC, WebSocket, 또는 FTP/SFTP** 방식 활용 가능
     - 대량 데이터 전송을 위해 **MTU 조정 및 TCP 튜닝** 수행
     - 필요 시 **CDN(Content Delivery Network) 활용**하여 전송 최적화
   * **Chunking(청킹) 및 스트리밍 처리**
     - 대량 데이터를 한 번에 보내는 것이 아니라 **스트리밍 방식** 적용
     - HTTP/2, gRPC Streaming, WebRTC DataChannel 같은 프로토콜 사용
     - 파일 업로드의 경우 **멀티파트 업로드 (AWS S3 Multipart Upload 등)** 고려
   * **스토리지 및 데이터 무결성 보장**
     - 대용량 데이터를 저장하기 위해 **분산 스토리지 (Ceph, HDFS, MinIO)** 활용
     - 데이터 무결성을 위해 **파일 해시(Hash, Checksum) 검증** 추가
   * **배치 처리 및 오프로드**
     - 요청이 많지 않다면 **배치 처리(Batch Processing) 또는 예약된 작업(Cron, Airflow)** 활용
     - 실시간이 필요 없다면 **데이터 수집 후 비동기 분석**
   * **데이터 전송 최적화**
     - **압축(Compression)**: Zstandard(Zstd), Gzip 등 적용
     - **데이터 전송 프로토콜 최적화**: HTTP Keep-Alive, QUIC, TLS 튜닝
   * **로깅 및 장애 감지**
     - 장애 발생 시 **재전송(Retry) 및 체크포인트(Checkpointing) 지원**
     - **ELK, Loki, Prometheus** 로 대형 데이터 처리의 병목 감지

## 유지보수성

모두 레거시 시스템 유지보수를 각자의 이유로 싫어한다. 때문에 다음과 같이 설계하여 고통을 최소화 하는것이 좋다

* 운용성 : 운영팀이 시스템을 원활하게 운영할 수 있게
* 단순성 : 복잡도를 제거해 새로운 엔지니어가 시스템을 이해하기 쉽게
* 발전성 : 쉽게 변경할 수 있도록 해야 함.



### 운용성

운영 중 일부 측면은 자동화 해야 하며, 시스템이 지속해서 원활하게 작동하려면 운영팀이 필수다. 다음과 같은 내용을 참고해서 자동화 혹은 시스템을 구축해보자.

- ﻿﻿좋은 모니터링으로 런타임(runtime) 동작과 시스템의 내부에 대한 가시성 제공
- ﻿﻿표준 도구를 이용해 자동화와 통합을 위한 우수한 지원을 제공

- ﻿﻿개별 장비 의존성을 회피, 유지보수를 위해 장비를 내리더라도 시스템 전체에 영향을 주지 않고 계속해서 운영 가능해야 함.
- ﻿﻿좋은 문서와 이해하기 쉬운 운영 모델(예를 들어 "X를 하면 Y가 발생한다") 제공
- ﻿﻿만족할 만한 기본 동작을 제공하고, 필요할 때 기본값을 다시 정의할 수 있는 자유를 관리자에게 부여
- ﻿﻿적절하게 자기 회복(sell-healing)이 가능할 뿐 아니라 필요에 따라 관리자가 시스템 상태를 수동으로 제어할 수 있게 함
- ﻿﻿예측 가능하게 동작하고 예기치 않은 상황을 최소화함



## 정리

애플리케이션이 유용하려면 다양한 요구사항을 충족시켜야 함.

자주 나오는 용어로

* 기능적 요구사항 
  * 필요한 기능, 저장 조회 검색 등
* 비기능적 요구사항
  * 보안, 신뢰성, 법규 준수, 확장성, 호환성, 유지보수성 

신뢰성은 결함이 발생해도 올바르게 동작한다는 의미

확장성은 부하가 증가해도 좋은 성능을 유지하기 위한 전략.

유지보수성은 엔지니어와 운영 팀의 삶을 개선하는데 있음. 