# 4장 처리율 제한 장치의 설계

[toc]



처리율 제한 장치 (`Rate Limiter`)

클라이언트 또는 서비스가 보내는 트리팩의 처리율을 제어하기 위한 장치

- 정의된 임계치(Threshold)를 넘어서면 추가로 도달한 모든 호출은 처리가 중단(block) 되야 한다.
- 예제
  - 사용자는 초당 2회 이상의 새 글을 올릴 수 없다.
  - 같은 IP 주소로는 하루에 10개 이상의 계정을 생성할 수 없다.



API에 처리율 제한 장치를 두면 좋은 점이 뭘까?

* Dos 공격에 의한 자원 고갈(resource starvation)을 방지할 수 있다. 추가 요청에 대해서는 처리를 중단하기 때문이다
* 비용을 절감한다. 처리를 제한해 서버를 많이 두지 않고, 우선순위가 높은 API에 더 많은 자원을 할당할 수 있다. 특히 요청 당 비용이 드는 Third party API를 사용하고 있는 경우, 횟수 제한을 통해 과도한 비용을 막을 수 있다.

* 서버 과부하를 막는다. bot 트래픽이나 잘못된 사용자 패턴으로 유발된 트래픽을 걸러내는데 사용될 수 있다



Resilience4j 학습 시, 제공되고 있는 RateLimiter 모듈이 존재

- https://resilience4j.readme.io/docs/ratelimiter

- Istio rate limit → envoy
  - Envoy는 Istio에서 자주 사용되는 고성능, 오픈 소스 및 확장 가능한 프록시입니다. Envoy는 서비스 간의 통신을 추상화하고, 로드 밸런싱, 타임아웃, 재시도, 모니터링 등과 같은 네트워크 관련 문제를 처리
  - https://istio.io/latest/docs/tasks/policy-enforcement/rate-limit/
  - https://github.com/envoyproxy/ratelimit



# 1단계 문제 이해 및 설계 범위 확정

처리율 제한 장치 알고리즘은 여러가지가 있는데 각각 장단점이 있다.

면접관과 소통해서 어떤 제한 장치를 구현해야 하는지 분명히 해보자

> **지원자**: 어떤 종류의 처리율 제한 장치를 설계해야 하나요? 클라이언트 측 제한 장치입니까, 아니면 서버 측 제한 장치입니까?
>
> 면접관: 좋은 질문이에요. 서버측 API를 위한 장치를 설계한다고 가정합시다.
>
> **지원자**: 어떤 기준을 사용해서 API 호출을 제어해야 할까요? IP 주소를 사용해 야 하나요? 아니면 사용자 ID? 아니면 생각하는 다른 어떤 기준이 있습니까?
>
> 면접관: 다양한 형태의 제어 규칙(throtling rules)을 정의할 수 있도록 하는, 유연한 시스템이어야 합니다.
>
> **지원자**: 시스템 규모는 어느 정도여야 할까요? 스타트업 정도 회사를 위한 시스템입니까 아니면 사용자가 많은 큰 기업을 위한 제품입니까?
>
> 면접관: 설계할 시스템은 대규모 요청을 처리할 수 있어야 합니다.
>
> **지원자**: 시스템이 분산 환경에서 동작해야 하나요?
>
> 면접관: 그렇습니다.
>
> **지원자**; 이 처리율 제한 장치는 독립된 서비스입니까 아니면 애플리케이션 코드에 포함될 수도 있습니까?
>
> 면접관: 그 결정은 본인이 내려주시면 되겠습니다.
>
> **지원자**: 사용자의 요청이 처리율 제한 장치에 의해 걸러진 경우 사용자에게 그 사실을 알려야 하나요?
>
> 면접관: 그렇습니다.

### 요구사항

- ﻿﻿설정된 처리율을 초과하는 요청은 정확하게 제한한다.
- ﻿﻿낮은 응답시간: 이 처리율 제한 장치는 HTTP 웅답시간에 나쁜 영향을 주어서는 곤란하다.
- ﻿﻿가능한 한 적은 메모리를 써야 한다.
- ﻿﻿분산형 처리율 제한(distributed rate limiting): 하나의 처리율 제한 장치를 여러 서버나 프로세스에서 공유할 수 있어야 한다.
- ﻿﻿예외 처리: 요청이 제한되었을 때는 그 사실을 사용자에게 분명하게 보여주어야 한다.
- ﻿﻿높은 결함 감내성(fault tolerance): 제한 장치에 장애가 생기더라도 전체 시스템에 영향을 주어서는 안 된다.



# 2단계 개략적 설계안 제시 및 동의 구하기

**처리율 제한 장치는 어디에 둘 것인가?**

* 서버-클라이언트 모델에서, 클라이언트에 둘 수도 있고, 서버에 둘 수도 있다

1. 클라이언트 측
   * ﻿﻿일반적으로 클라이언트는 처리율 제한을 안정적으로 걸 수 있는 장소가 못된다. 
   * 클라이언트 요청은 쉽게 위변조가 가능하고 모든 클라이언트를 통제할 수 없다.

2. 서버 측에 둔다면?

<img src="./images/4장 처리율 제한 장치의 설계//image-20230806202116839.png" >

3. 미들웨어 사용 

미들웨어로 하여금 API 서버로 가는 요청을 통제한다

<img src="./images/4장 처리율 제한 장치의 설계//image-20230806202751604.png">

* 미들웨어에 가로막혀 클라이언트는 429 http status를받게된다

Cloud MSA 구조에서 rate limiter는 보통 API Gateway라는 컴포넌트에 구현된다

 → Rate limiter, SSL termination, User Authentication, IP Whitelist 등 지원

서버/미들웨어 어디에 둘 것인가는 정답은 없다

- 하지만 어디에 둘 것인가를 결정할 때 고려해야할 수 있다
  1. 프로그래밍 언어, 캐시 서비스 등 기술 스택 점검 → 현재 서버 측 언어를 통해 구현이 가능한가? 충분한가?
  2. 현재 비즈니스에 맞는 처리율 제한 알고리즘 찾기 → 미들웨어 및 제 3자가 서비스하는 Gateway를 사용한다면 적절한 알고리즘이 있는지 확인.
  3. MSA 구조를 사용하고 있어 이미 API Gateway 사용하고 있다면, 기존 API Gateway에 Rate limiter 포함시키는 것 고려
  4. rate limiter를 구현하기에 충분한 인력이 없다면, 상용 api 게이트웨이를 쓰는것이 낫다.

## 처리율 제한 알고리즘

- ﻿﻿토큰 버킷(token bucket)
- ﻿﻿누출 버킷(leaky bucket)
- ﻿﻿고정 윈도 카운터(fixed window counter)
- ﻿﻿이동 윈도 로그(sliding window log)
- ﻿﻿이동 윈도 카운터(sliding window counter)

### 토큰 버킷 알고리즘

rate limiter의 구현에 폭넓게 이용되고 있다. 

토큰 버킷 알고리즘의 동작 원리

<img src="./images/4장 처리율 제한 장치의 설계//image-20230806203251529.png" width = 450 height =500>

이 예에서는 토큰 버킷의 크기는 4이며 토큰 공급률(refill rate)는 분당 4이다

* 토큰 버킷은 지정된 용량을 갖는 컨테이너다. 
  * 사전 설정된 양의 토큰이 주기적으로 채워진다. 
  * 꽉 찬 버킷에는 더이상 토큰은 추가되지 않는다. 
* 각 요청은 처리될때마다 하나의 토큰을 사용한다
  * 토큰이 있는 경우 해당 토큰을 하나 꺼내 요청을 전달
  * 토큰이 없는 경우 해당 요청을 버린다
* 지정한 시간이 지나면 새로 토큰을 리필한다

**토큰 버킷 알고리즘은 2개 인자를 받는다**

* 버킷 크기 : 버킷에 담을 수 있는 토큰의 최대 개수
* 토큰 공급률(refill rate) : 초당 몇개의 토큰이 버킷에 공급되는가

통상적으로 API endpoint마다 별도의 버킷을 둔다.

* ex) 사용자마다 하루에 한번 포스팅, 친구 추가기능, 좋아요는 5번까지만 -> 사용자마다 3개의 버킷 필요
* IP 주소별로 처리율 제한을 적용해야 한다면 IP 주소마다 버킷을 하나씩 할당해야 함
* 시스템의 초당 처리율을 초당 10,000개 요청으로 제한하고 싶다면 모든 요청이 하나의 버킷을 공유해도록 해야함

**토큰 버킷의 장점**

- ﻿﻿구현이 쉽다.
- ﻿﻿메모리 사용 측면에서도 효율적이다.
- ﻿﻿짧은 시간에 집중되는 트래픽(burst of traffic)도 처리 가능하다. 버킷에 남은 토큰이 있기만 하면 요청은 시스템에 전달될 것이다.

**단점:**

* 이 알고리즘은 버킷 크기와 토큰 공급률이라는 두 개 인자를 가지고 있는데, 이값을 적절하게 튜닝하는 것은 까다로운 일이 될 것이다.

### 누출 버킷 알고리즘

토큰 버킷 알고리즘과 비슷하지만 `요청 처리율이 고정`되어 있다는 점이 다르다. 

> 특히, 불규칙한 또는 버스트 트래픽을 일정한 데이터 전송률로 조절할 수 있으며, DDOS 공격과 같은 과도한 트래픽을 완화하는 데 유용하다

누출 버킷 알고리즘은 보통 FIFO (First-In-First-Out) 큐로 구현한다

<img src="./images/4장 처리율 제한 장치의 설계//image-20230806203722383.png" width = 900 height = 300>

**누출 버킷 알고리즘 동작 원리** 

- ﻿﻿요청이 도착하면 큐가 가득 차 있는지 본다. 빈자리가 있는 경우에는 큐에 요청을 추가한다.
- ﻿﻿큐가 가득 차 있는 경우에는 새 요청은 버린다.
- ﻿﻿지정된 시간마다 큐에서 요청을 꺼내어 처리한다.

**누출 버킷 알고리즘은 2개 인자를 받는다**

- ﻿﻿버킷 크기: 큐 사이즈와 같은 값이다. 큐에는 처리될 항목들이 보관 된다.
- ﻿﻿처리율(outhlow rate): 지정된 시간당 몇 개의 항목을 처리할지 지정하는 값 이다. 보통 초 단위로 표현된다.

**누출버킷의 장점**

- ﻿﻿큐의 크기가 제한되어 있어 메모리 사용량 측면에서 효율적이다.
- ﻿﻿고정된 처리율을 갖고 있기 때문에 안정적 출력(stable outflow rate)이 필요한 경우에 적합하다.

단점

- ﻿﻿단시간에 많은 트래픽이 몰리는 경우 큐에는 오래된 요청들이 쌓이게 되고, 그요청들을 제때 처리 못하면 최신 요청들은 버려지게 된다.
- ﻿﻿두개 인자를 갖고 있는데, 이들을 올바르게 튜닝하기가 까다로울 수 있다.



### 고정 윈도 카운터(fixed window counter) 알고리즘

특정 시간 윈도 내에서 발생하는 이벤트의 수를 계산하는 알고리즘. 

이 알고리즘은 트래픽 제어, API 요청 제한, 사용률 모니터링 등에 사용될 수 있다.

**고정 윈도 카운터의 동작 원리**

- 타임라인을 고정된 간격의 윈도로 나누고, 윈도마다 카운터를 붙임.
- 요청마다 카운터를 1씩 증가시킨다. 카운터의 값이 임계치(threshold)에 도달하면 새로운 요청은 새 윈도가 열릴 때까지 버려짐.
- 새 윈도가 열리면 카운터를 초기화

<img src="./images/4장 처리율 제한 장치의 설계//image-20230806210159912.png" width = 600 height = 350>

* 현재 그림상 타임라인의 시간 단위는 1초
* 시스템은 초당 3개만 허용하고, 3개를 넘어서면 요청들은 버려진다

그러나, 이 알고리즘에 큰 문제가 있다.

윈도의 경계 부근에 순간적으로 많은 트래픽이 집중될 경우 `윈도우에 할동된 양보다 더 많은 요청이 처리된다.`

<img src="./images/4장 처리율 제한 장치의 설계//image-20230806210415135.png" width = 600 height = 350>

그림은 분당 최대 5개의 요청만을 허용하는 시스템이다. 

카운터는 매분마다 초기화 된다 

문제가 무엇일까?

* 2:00:00 ~ 2:01:00, 2:01:00 ~ 2:02:00 를 보면 1분동안 각각 처리한 것은 5개이다
* 그러나, 2:00:30 ~ 2:01:30 1분 간격을 보면 10개를 처리한것이다. 허용 한도의 2배이다

즉 ,윈도 경계부분에 요청이 몰리면, 제한하고 싶은 한도보다 많은 요청을 처리하는 버그가 발생한다

**장점**

- ﻿﻿메모리 효율이 좋다.
- ﻿﻿이해하기 쉽다.
- ﻿﻿윈도가 닫히는 시점에 카운터를 초기화하는 방식은 특정한 트래픽 패턴을 처리하기에 적합하다.

**단점**

* 윈도 경계부분에 요청이 몰리면, 제한하고 싶은 한도보다 많은 요청을 처리하는 버그가 발생한다 

### 이동 윈도 로그(sliding window log)

고정 윈도 카운터 알고리즘의 문제를 해결하기 위해 고안

* 고정 윈도 알고리즘 문제 : 윈도 경계부분에 요청이 몰리면, 제한하고 싶은 한도보다 많은 요청을 처리하는 버그가 발생한다 



**이동 윈도 로깅 알고리즘의 동작 원리**

<img src="./images/4장 처리율 제한 장치의 설계//image-20230806210853323.png" width = 500 height = 400>

- ﻿﻿이 알고리즘은 요청의 timestamp를 추적한다. 
  - timestamp data는 보통 레디스의 정렬 집합(sorted set) 같은 캐시에 보관한다.
- ﻿﻿새 요청이 오면 만료된 타임스탬프는 제거한다. 만료된 타임스탬프는 그 값 이 현재 윈도의 시작 시점보다 오래된 타임스탬프를 말한다.
- ﻿﻿새 요청의 타임스탬프를 log에 추가한다.
- ﻿﻿로그의 크기가 허용치보다 같거나 작으면 요청을 시스템에 전달한다. 
- 허용치보다 크면 처리 거부

**장점**

* 이 알고리즘이 구현하는 처리율 제한 메커니즘은 아주 정교하다. 어느 순간의 윈도를 보더라도, 허용되는 요청의 개수는 시스템의 처리율 한도를 넘지 않는다.

**단점**

* 거부된 요청의 타임스탬프도 보관하기 때문에 다량의 메모리를 사용한다.

### 이동 윈도 카운터(sliding window counter)

고정 윈도 카운터와 이동 윈도 로깅 알고리즘을 결합한 형태.

<img src="./images/4장 처리율 제한 장치의 설계//image-20230806211244283.png" width = 700 height = 500>>

현재 윈도우와 현재 시간을 보고, 들어온 요청의 비율을 따져서 처리할지 판단하는 알고리즘.

- ex) 윈도우 크기 1분
  - `현재 1분간의 요청 수 + 직전 1분간의 요청 수 * 이동 윈도(현재 요청으로부터 1분 간)와 직전 1분이 겹치는 비율`
- 다양한 알고리즘이 있다

장점

- 이전 윈도의 평균 처리율과 현재 윈도 상태를 측정해 계산하므로 짧은 시간에 몰리는 트래픽에도 잘 대응
- 메모리 효율이 좋다

단점

- 직전 윈도(시간)에 도착한 요청이 균등하게 분포되어있다고 가정하고 위와 같은 식을 사용. 
- 이것도 고정 윈도 카운터처럼 경계 부분에 몰렸을 경우, 계산이 예상한 결과와 다르게 동작할 수 있다. 
- 그러나 클라우드플레어에서 수행한 실험에 의하면 이런 경우는 0.003%에 불과했다고 함.



## 개략적인 아키텍처(처리율 제한 알고리즘)

처리율 제한 알고리즘의 기본 아이디어는 단순하다

얼마나 많은 요청이 접수되었는지 추적할 수 있는 카운터를 추적 대상별로 두고 한도를 넘어서면 거부하는 것

* 사용자별 추적
* IP별 추적
* API 엔드포인트별 or 서비스 단위

### rate limiter의 카운터 값은 어디에 보관하는 것이 좋을까?

* DB는 디스크 접근 때문에 느리다

메모리상에서 동작하는 캐시가 바람직하다

* 메모리라 빠르다
* 시간에 기반한 만료 정책을 지원할 수 있다.

`일례로 Redis는 처리율 제한 장치를 구현할 대 자주 사용된다.`

INCR, EXPIRE 두가지 명령어를 지원한다. 

- `INCR`: 메모리에 저장된 카운터 1증가
- `EXPIRE`: 카운터에 저장한 타임아웃 값 설정 → 만료되면 자동으로 삭제

<img src="./images/4장 처리율 제한 장치의 설계//image-20230806211725966.png" width = 800 height =350>

레디스를 이용한 rate limiter 동작 원리

- ﻿﻿클라이언트가 처리율 제한 미들웨어(rate limiting middleware)에게 요청을 보낸다.
  - 특정 서버나, 엔진엑스나 api gateway가 될 수도 있다.
- ﻿﻿처리율 제한 미들웨어는 레디스의 지정 버킷에서 카운터를 가져와서 한도에 도달했는지 아닌지를 검사한다.
  - ﻿﻿한도에 도달했다면 요청은 거부된다.
- 한도에 도달하지 않았다면 요청은 API 서버로 전달하고 미들웨어는 카운터의 값을 증가시킨 후 다시 레디스에 저장한다.

# 3단계 상세 설계 

위의 개략적 설계를 봐서는 다음과 같은 사항은 알 수가 없다.

- ﻿﻿처리율 제한 규칙은 어떻게 만들어지고 어디에 저장되는가?
- ﻿﻿처리가 제한된 요청들은 어떻게 처리되는가?



## 처리율 한도 초과 트래픽의 처리

- HTTP 429 응답 (Too many requests): 어떤 요청이 한도 제한에 걸릴때 응답
- 경우에 따라서 한도 제한에 걸린 메시지를 나중에 처리하기 위해 큐에 보관할 수 있다.
  - 예를 들면 어떤 주문이 과부하에 걸렸다든가 등

### 처리율 제한 장치가 사용하는 HTTP 헤더

클라이언트가 자기 요청이 처리율 제한에 걸리고 있는지에 대한 정보를 아래 HTTP 헤더를 통해 전달한다.

- X-Ratelimit-Remaining: 윈도 내에 남은 처리 가능 요청의 수
- X-Ratelimit-Limit: 매 윈도마다 클라이언트가 전송할 수 있는 요청의 수
- X-Ratelimit-Retry-After: 한도 제한에 걸리지 않으려면 몇 초 뒤에 요청을 다시 보내야 하는지 알림

사용자가 너무 많은 요청을 보내면 `429 too many requests 오류`를 `X-Ratelimit-Retry-After 헤더`와 함께 반환하도록 한다.

## 상세 설계

<img src="./images/4장 처리율 제한 장치의 설계//image-20230806212533921.png" width = 700 height = 600>

* `처리율 제한 규칙은 디스크에 보관한다`. 작업 프로세스(workers)는 `수시로 규칙을 디스크에서 읽어 캐시에 저장`한다.

- 클라이언트가 요청을 보내면 먼저 Rate limiter 미들웨어가 받는다. 
- Rate limiter 미들웨어는 규칙을 캐시로부터 읽어와 카운터 또는 타임스탬프를 Redis 캐시에서 가져온 값들에 근거해 요청을 처리한다.
  - 처리율 제한에 걸리지 않은 경우, API 서버로 전송
  - 처리율 제한에 걸린 경우, Http status 429 등 헤더 정보를 설정해 클라이언트에게 전송.
    - 요청을 버릴 수도 있고, 큐에 등록해놓고 나중에 처리할 수도 있음

## 분산 환경에서의 처리율 제한 장치의 구현

단일 서버를 지원하는 rate limiter의 구현은 어렵지 않다.

하지만 여러 대의 서버와 병렬 스레드를 지원하도록 시스템을 확장하는 것은 다음과 같은 어려운 문제를 풀어야 한다.

- ﻿﻿경쟁 조건(race condition)
- ﻿﻿동기화(synchronization)

### 경쟁 조건 이슈가 발생하는 동작 - 분산환경

- 레디스에서 카운터의 값을 읽는다.
- counter +1 의 값이 처리율 임계치를 넘는지 본다.
- 넘지 않는다면 레디스에 보관된 카운터 값을 1만큼 증가시킨다.

경쟁 조건 문제를 해결할 수 있는 가장 쉬운 방법은 Lock이지만, 성능 이슈가 커진다. 

이를 해결할 수 있는 다른 방안은 `루아 스크립트`와 sorted-set 레디스 자료구조이다.

### 경쟁 조건 이슈 해결 방법 - set then get

* https://engineering.classdojo.com/blog/2015/02/06/rolling-rate-limiter/

* https://goalgorithm.wordpress.com/2019/06/08/designing-an-api-rate-limiter/

get then set (레디스에서 가져오고 값을 증가시키기)지 말고

set then get(값을 증가시키고 가져오는것이다)

원자 연산자에 의존하여 원자 연산을 방해하지 않고 카운터 값을 빠르게 증가시키고 확인할 수 있다

위에서 설명한 "get then set" 접근 방식은 데이터 저장소에서 값을 가져와서("get") 변경한 다음 다시 설정하는( "set") 과정인데, 
이 과정은 한 요청이 값을 가져와 변경하는 동안, 다른 요청이 동일한 값을 가져와 변경하면 잘못된 값이 설정되는 문제가 발생하는것이다 

더 나은 접근 방식은 "set-then-get" (값을 증가시키고 가져오는것)방식을 사용하는 것이다. 

이 방식은 원자 연산을 이용하는데,

원자 연산자는 한 번에 하나의 연산만 수행하도록 보장하므로, 다른 연산이 그 과정을 방해하지 않는다 

이런 방식을 통해 카운터 값을 빠르게 증가시키고 확인할 수 있으며, 경쟁 조건 문제를 방지할 수 있다

### 예시

고전적인 "get-then-set" 방식에서는 다음과 같은 문제가 발생할 수 있다.

1. 클라이언트 A가 레디스에서 카운터 값을 가져온다 (예: 5).
2. 클라이언트 B가 동시에 레디스에서 같은 카운터 값을 가져온다 (예: 5).
3. 클라이언트 A와 B 모두 값을 1 증가시키고, 레디스에 설정한다 (6).
4. 결과적으로, 두 클라이언트가 동시에 값을 증가시켰지만 카운터는 실제로 1만 증가한다.(2가 증가해야한다)

"set-then-get" 방식을 사용하면 레디스의 `INCR`와 같은 원자 연산을 활용하여 이러한 문제를 해결할 수 있다.

1. 클라이언트 A가 `INCR` 명령을 사용하여 카운터를 증가시킨다.
2. 클라이언트 B가 동시에 `INCR` 명령을 사용하여 카운터를 증가시킨다.
3. 레디스는 `INCR` 명령을 원자적으로 처리하므로, 경쟁 조건 없이 올바른 카운터 값을 얻는다 (7).

### 동기화 이슈

수백만 사용자를 지원하려면 1대의 rate limiter 서버로는 충분하지 않을 수 있따.

때문에 여러 대 두게 되면 동기화가 필요해진다 

그림처럼 이런 여러대가 존재하는 상황에서 웹 계층은 기본적으로 무상태이기 때문에 한 클라이언트라도 요청이 각기 다른 처리율 제한장치로 갈 수 있다.

<img src="./images/4장 처리율 제한 장치의 설계//image-20230806214049966.png" >

* rate limiter 1은 클라 1에 대해선 알지만 클라 2에 대해선 모르므로 처리율을 올바르게 제한할 수 없다

#### 해결 방법

Sticky session 을 사용하여 같은 클라이언트로부터의 요청은 항상 같은 처리율 제한 장치로 보낼 수 있도록 할 수 있다.

> 하지만 이 방법은 추천하고 싶지 않은데, 규모면에서 확장 가능하지도 않 고 유연하지도 않기 때문이다.

더 나은 해결법은 레디스와 같은 중앙 집중형 데이터 저장소를 사용하여 동기화 하는것이다.

<img src="./images/4장 처리율 제한 장치의 설계//image-20230806214220128.png">



### 성능 최적화 - 분산환경

- 지연시간을 줄이기 위해 사용자의 트래픽을 가장 가까운 Edge 서버로 전달하여 지연시간을 줄인다.



### 모니터링 - 분산환경

rate limiter를 설치한 이후에는 효과적으로 동작하고 있는지 보기 위해 데이터를 모니터링을 통해 확인하려는 것은 다음 두 가지다.

- 채택된 처리율 제한 알고리즘이 효과적인가?
- 정의한 처리율 제한 규칙(파라미터)이 효과적인가?

예를 들어 처리율 제한 규칙이 너무 빡빡하게 설정되었다면 많은 유효 요청이 처리되지 못하고 버려질 수 있다.. 

* 그런 일이 벌어진다면 규칙을 다소 완화할 필 요가 있다.

깜짝 세일 같은 이벤트 때문에 트래픽이 급중할 때 처리율 제한 장치가 비효 율적으로 동작한다면, 

그런 트래픽 패턴을 잘 처리할 수 있도록 알고리즘을 바꾸는 것을 생각해 봐야 한다. 

그런 상황에는 토큰 버킷이 적합할 것이다.



# 4단계 마무리

다음과 같은 부분도 생각해보자

- 경성 또는 연성 처리율 제한
  - 경성(Hard): 요청의 개수는 임계치를 절대 넘을 수 없음.
  - 연성(Soft): 요청의 개수는 잠시동안 임계치를 넘을 수 있음.
- 다양한 계층 처리율 제한
  - HTTP는 애플리케이션 계층에서의 처리율 제한이다  
  - 다른계층인 3계층 에서 Iptables를 사용하면 IP주소의 처리율 제한도 적용하는것이 가능하다.
  - https://blog.programster.org/rate-limit-requests-with-iptables
- 처리율 제한 회피 방법? 클라이언트를  어떻게 설계 하는것이 최선인가?
  - 캐시를 사용해 API 호출 횟수를 줄인다
  - 클라이언트가 처리율 제한 임계치 이해하고 짧은 시간동안 너무 많은 메시지를 보내지 않도록 한다.
  - 처리율 제한이 되었을 때, 에러나 예외를 처리하는 코드를 도입하여 클라이언트가 Gracefully 복구할 수 있도록 고려한다
  - Retry 로직을 구현할 때는 충분한 back-off 시간을 둔다 
    - "back-off"는 네트워크 통신 또는 컴퓨터 연산에서 일시적인 실패에 대응하는 방법 중 하나로, 재시도 간의 대기 시간을 점차 늘려가는 전략
    - https://projectreactor.io/docs/core/release/reference/#faq.exponentialBackoff
    - https://projectreactor.io/docs/core/release/api/reactor/util/retry/Retry.html#backoff-long-java.time.Duration-
    - https://medium.com/@odysseymoon/spring-webflux에서-error-처리와-retry-전략-a6bd2c024f6f