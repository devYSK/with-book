

# SQL 레벨업



1장 DBMS 아키텍처

2장 SQL 기초

3장 SQL의 조건 분기

4장 집약과 자르기

5장 반복문

6장 결합

7장 서브쿼리

8장 SQL의 순서

9장 갱신과 데이터 모델

10장 인덱스 사용







# 1장 DBMS 아키텍처

![image-20230201180033073](/Users/ysk/study/study_repo/SQL, Database/postgresql/images//image-20230201180033073.png)

일반적인 DBMS 아키텍처는 위와 같은 구조를 가지고 있다.
DBMS 내부의 기능을 알아보자.

### 쿼리 평가 엔진

- 사용자로부터 입력받은 SQL 구문을 분석, **실행 계획**(Explain Plan, 어떤 순서로 데이터에 접근하는지)을 결정한다.
- 실행 계획을 기반으로 데이터에 접근하는 방법을 **접근 메서드**(Access Method)라고 한다.



> 쿼리 평가 엔진 = 계획을 세우고 실행하는 DBMS의 핵심 기능을 담당하는 모듈

### 버퍼 매니저

- DBMS는 **버퍼**라는 메모리 영역을 확보하는데, 이를 관리한다.
- 디스크를 관리하는 디스크 용량 매니저와 연동되어 작동한다.

### 디스크 용량 매니저

- 데이터를 어디에 어떻게 저장할지 관리한다.
- 데이터의 I/O를 제어한다.
- 데이터베이스는 데이터를 영구적으로 저장해야 하므로 디스크 용량 매니저가 어디에 어떻게 데이터를 저장할지 관리하고, 읽고 쓰기를 제어한다.

### 트랜잭션 매니저와 락 매니저

- DBMS에서 각각의 처리는 DBMS 내부에서 트랜잭션이라는 단위로 관리되는데, 이를 정합성(무모순성)을 유지하면서 실행시킨다.
- 필요한 경우 데이터에 락을 절어 다른 사람의 요청을 대기시킨다.

### 리커버리 매니저

- 시스템의 장애를 대비한다.
- 데이터를 정기적으로 백업하고, 문제가 일어났을 때 데이터를 복구한다.



## 2. DBMS와 버퍼

메모리는 한정된 희소 자원인 반면에 데이터는 굉장히 많기 때문에 버퍼에 어떤 식으로 확보할 것인가 하는 부분에서 트레이드오프가 발생한다.
DBMS는 대부분 용량, 비용, 성능의 관점에서 HDD를 사용한다.

**성능 향상을 위해 자주 참조되는 데이터를 메모리 위에 올려둔다**

-> 성능 향상을 목적으로 데이터를 저장하는 메모리를 버퍼(buffer), 캐시(cache)라고 부른다.

> 버퍼에 어떻게, 어느 정도의 기간 동안 올릴지를 관리하는 것이 버퍼 매니저이다.

![image-20230201180813064](/Users/ysk/study/study_repo/SQL, Database/postgresql/images//image-20230201180813064.png)

기억장치의 분류를 계층으로 나타낸 그림이다.
여기서 기억비용은 데이터를 저장하는데 소모되는 비용을 말한다.



DBMS는 데이터 저장을 목적으로 하는 미들웨어이다.

#### HDD, SDD

- 현재 DBMS는 데이터를 HDD 또는 SSD에 저장한다.

#### 메모리

- 디스크에 비해 기억비용이 비싸다. (대충 수십만~수백만 배 차이)
- 따라서 DB의 내부 데이터를 모두 올려놓는 것은 불가능하다.

#### 버퍼를 활용한 속도 향상

- 그럼에도 메모리를 사용하는 이유는 성능 향상 떄문이다.
- 일반적인 SQL 구문의 실행 시간 대부분을 I/O에 사용한다.
- 따라서, 자주 접근하는 데이터를 메모리 위에 올려놓으면 성능이 무척 향상된다.

![image-20230201181038212](/Users/ysk/study/study_repo/SQL, Database/postgresql/images//image-20230201181038212.png)

* 같은 SQL 구문을 실행한다고 해도 디스크에서 데이터를 가져올 필요 없이 곧바로 메모리에서 읽어 빠르게 데이터를 검색하는것이 더 좋다.
* 디스크의 접근을 줄일 수 있다면 큰 폭의 성능 향상이 가능하다. 
* 일반적인 SQL 구문의 실행 시간 대부분을 저장소 I/O에 사용하기 때문 



이렇게 성능 향상을 목적으로 데이터를 저장하는 메모리를 버퍼 또는 캐시라고 부른다.



> 버퍼에 데이터를 어떻게, 어느정도의 기간 동안 올릴지를 관리하는 것이 버퍼 매니저이다.



### 메모리 위의 두 버퍼

DBMS가 사용하는 버퍼는 크게 두 종류다.

- 데이터 캐시
- 로그 버퍼
  이러한 버퍼는 사용자가 용도에 따라 크기를 변경할 수 있다.



#### 데이터 캐시

- 디스크에 있는 데이터의 일부를 메모리에 유지하기 위해 사용하는 메모리 영역이다.
- 데이터 캐시에 필요로 하는 SQL 구문의 데이터가 전부 있다면 디스크를 접근하지 않고 처리를 수행한다.

#### 로그 버퍼

- 로그 버퍼는 갱신처리(INSERT, DELETE, UPDATE, MERGE)와 관련있다.
- 갱신처리가 이루어지는 과정
  - 갱신과 관련된 SQL 구문을 받으면 로그 버퍼에 변정 정보를 보낸다.
  - 이후 디스크에 변경을 비동기적으로 수행한다.
- 갱신처리는 시간소요가 많이 되기에 이처럼 수행된다.

> 곧바로 저장소에 있는 데이터를 변경하지 않고, 로그 버퍼 위에 변경 정보를 보내고 이후에 디스크 내용 수정

![image-20230201181440375](/Users/ysk/study/study_repo/SQL, Database/postgresql/images//image-20230201181440375.png)

* 갱신 처리는 비동기로 이루어진다

**갱신 처리는 SQL 실행 시점과 저장소에 갱신하는 시점에 차이가 있는 비동기 처리이다.**

* 갱신 시 상당한 시간이 소모되므로 갱신 정보를 받은 시점에 로그를 쌓고, 내부적으로 관련된 처리를 수행한다.



### 메모리의 성질이 초래하는 트레이드 오프

메모리의 휘발성때문에 로그 버퍼의 데이터가 DBMS가 다운될 때 사라지는 현상이 생길 수 있다.

로그 버퍼 위의 데이터가 디스크에 저장되기 전 사라저버리면 복구가 불가능하다.
이는 비즈니스적인 관점에서 심각한 문제이다.

이를 회피하기 위해 커밋 시점에 갱신정보를 로그 파일(영속적인 저장소에 존재함)에 작성해 정합성을 지키고자 한다.

이 때문에 새로운 트레이드 오프가 발생하는데, `커밋 시에는 디스크에 동기 접속이 일어난다.`

* (일부 DBMS는 설정으로 비동기 접속으로 변환 가능하지만... 극단적인 트레이드오프이다.)

* 커밋은 갱신 처리를 확정하는것. DBMS는 커밋된 데이터를 영속화 한다. -> 동기 접속 발생



> - 비동기 처리시 : 데이터 정합성 ↓, 성능 ↑
> - 동기 처리시 : 데이터 정합성 ↑, 성능 ↓



### 시스템 특성에 따른 트레이드 오프

![image-20230201181906527](/Users/ysk/study/study_repo/SQL, Database/postgresql/images//image-20230201181906527.png)

#### 데이터 캐시와 로그 버퍼의 크기

DBMS에서 제공하는 데이터 캐시에 비해 로그 버퍼의 초깃값이 굉장히 작다.
-> 데이터베이스는 기본적으로 검색을 메인으로 처리한다고 가정하기 때문이다.

* 실제로도 많은 DBMS가 물리 메모리에 여유가 있다면 데이터 캐시를 되도록 많이 할당하라고 권장한다.
  (MySQL 5.7 공식문서에서도 DB전용 서버에서는 물리 메모리의 80%를 버퍼 풀이 차지해도 괜찮다고 쓰여있다.)

* 로그 버퍼는 갱신처리에 쌓이고, 갱신처리는 검색(조회)처리보다 규모가 적다.

검색 시에 레코드가 수천만 건에 달하는 경우도 있지만, 갱신 처리는 많아봤자 수만 건 정도밖에 안된다.

> 갱신 처리에 값비싼 메모리를 많이 사용하는 것보다는, 자주 검색하는 데이터를 캐시에 올려놓는 것이 더 좋은 성능을 낼 수 있다.

검색에 비해 갱신이 많다면, 로그의 버퍼의 크기를 늘려주는 성능 튜닝(최적화)가 필요하다 



### 메모리 크기를 트레이드오프 하자

자신의 시스템에 검색보다 업데이트가 많다 : 데이터 캐시 < 로그버퍼 더 크게 잡기
업데이트보다 검색이 많다 : 데이터 캐시를 더 크게 잡기 > 로그 버퍼



## 추가적인 메모리 영역 '워킹 메모리'

#### 워킹 메모리란?

- DBMS는 일반적으로 2개의 버퍼 외에 워킹 메모리라는 영역을 가지고 있다.
- 이는 정렬(일반적으로 ORDER BY), 해시(테이블 조인) 관련 처리에 사용되는 작업용 영역이다.
- 가변적인 용량을 가지고 정렬, 해시가 필요할 때 사용되고, 종료하면 해제되는 임시 영역이다.
- 여러개의 SQL 구문들이 나눠서 사용하므로 동시에 실행하면 메모리의 범위를 넘어가는 일이 생기기도 한다.



**워킹 메모리를 부르는 명칭**

| DBMS           | 명칭                     | 매개변수             | 기본값                         |
| -------------- | ------------------------ | -------------------- | ------------------------------ |
| Oracle 11g R2  | PGA(Program Global Area) | PGA_AGGREGAGE_TARGET | 10MB or SGA크기의 20% 중 큰 것 |
| PostgreSQL 9.3 | 워크 버퍼                | work_mem             | 8MB                            |
| MySQL 5.7      | 정렬 버펴                | sort_buffer_size     | 256kb                          |

#### 워킹 메모리가 부족하다면?

워킹 메모리가 다루려는 데이터보다 양이 부족하다면 디스크를 사용한다. 즉, 처리가 굉장히 느려지게 된다.

![image-20230201183442690](/Users/ysk/study/study_repo/SQL, Database/postgresql/images//image-20230201183442690.png)

##### 워킹 메모리가 부족할 때 사용하는 임시적인 영역

- Oracle : 임시 테이블 스페이스(TEMP Tablespace)
- Microsoft SQL Server : TEMPDB
- PostgreSQL : 일시 영역(pgsql_tmp)



> 데이터베이스는 메모리가 부족하다는 이유로 SQL 구문에 오류를 절대 발생 시키지 않고 느려지더라도 끝까지 처리하려 노력한다.



## 3. DBMS와 실행 계획

성능 문제 등 내부 절차를 확인해야 할 때 실행계획을 사용한다.



### DBMS의 쿼리 처리 흐름

![image-20230201183656818](/Users/ysk/study/study_repo/SQL, Database/postgresql/images//image-20230201183656818.png)

#### 1. 파서 (parser) : 사용자로부터 SQL구문이 올바른지 검사한다.

#### 2. 옵티마이저 (optimizer, 최적화)

- 인덱스 유무, 데이터 분산 또는 편향도, 매개변수 등의 조건을 고려해 **선택 가능한 많은 실행 계획을 작성**한다.
- 많은 실행 계획들의 비용을 계산하여 가장 낮은 비용을 가진 실행 계획을 선택한다. 

#### 3. 카탈로그 매니저 (catalog manager)

* 옵티마이저가 비용을 계산하는데 도움을 준다.
* DBMS의 내부 정보(카탈로그)를 모아놓은 테이블로, 테이블 또는 인덱스의 통계 정보가 저장되어 있다. -> '통계 정보' 라 칭한다.

#### 4. 플랜 평가 (plan evaluation)

* 옵티마이저가 여러 개의 실행 계획을 세운 뒤 그것을 받아 **최적의 실행 결과를 선택**한다.

* 이는 사람이 쉽게 읽을 수 있도록 작성 된 일종의 '계획서'이다.
* 실행계획이 선택되면 실행계획을 절차적인 코드로 변환하고 데이터 접근을 수행한다.

**주의**
플랜 선택을 옵티마이저에게 맡길 경우, 최적의 플랜이 선택되지 않을 수 있다.
대표적 이유 : 통계 정보 부족 (카탈로그 매니저)
-> 데이터의 수정이 일어났을 때 카탈로그 정보가 갱신되지 않아 과거의 정보로 선택할 수 있다.

> 테이블의 데이터가 많이 바뀌면 카탈로그의 정보도 업데이트 해야 하는 것이 상식이다. ( 업데이트 실행 비용이 많이 든다. )



### 옵티마이저와 통계 정보

옵티마이저가 효율적인 플랜을 선택하지 않는 경우가 있는데, 주로 통계 정보가 부족해서다.

구현에 따라 차이는 있찌만 카탈로그에 포함되어 있는 통계 정보는 다음과 같다.

#### 카탈로그에 포함되는 정보

- 테이블의 레코드(row) 수
- 테이블의 필드 수와 필드 크기
- 필드의 카디널리티(값의 중복되는 개수)
- 필드 값의 히스토그램(어떤 값이 분포되어 있는가)
- 필드 내부의 NULL 수
- 인덱스 정보

이러한 정보를 사용해 카탈로그를 만들고 이를 기반으로 옵티마이저가 계획을 세우게 된다.
실제 테이블에 변화가 일어나도 카탈로그를 갱신하지 않으면 옵티마이저는 갱신되지 않은 카탈로그를 기준으로 계획을 세우기 때문에 잘못된 계획을 세우게 될 수 있다.

올바른 통계 정보가 모이는 것은 SQL 성능에 중요한 문제이다.
따라서 테이블의 데이터가 많이 변하면 카탈로그의 통계 정보도 함께 갱신해야 한다.



> 통계 정보 갱신은 많은 시간을 소요하는 작업이므로 갱신 시점을 잘 설계해야 한다.



#### 대표적인 DBMS의 통계 정보 갱신 명령어

| 이름       | 명령어                                                       |
| ---------- | ------------------------------------------------------------ |
| Oracle     | exec DBMS STATS.GATHER_TABLE_STATS(OWNNAME<br />=> [스키마 이름], TABNAME => [테이블 이름)); |
| SQL Server | UPDATE STATISTICS [테이블 이름]                              |
| DB2        | RUNSTATS ON TABLE [스키마 이름].[테이블 이름];               |
| PostgreSQL | ANALYZE [스키마 이름].[테이블 이름];                         |
| MySQL      | ANALYZE TABLE [스키마 이름].[테이블 이름];                   |



## 4. 실행 계획이 SQL 구문의 성능을 결정



### 1. 실행 계획 확인 방법

| 이름                 | 명령어                                 |
| -------------------- | -------------------------------------- |
| Oracle               | set autotrace traceonly                |
| Microsoft SQL Server | SET SHOWPLAN TEXT ON                   |
| DB2                  | EXPLAIN ALL WITH SNAPSHOT FOR SQL 구문 |
| PostgreSQL           | EXPLAIN SQL 구문                       |
| MySQL                | EXPLAIN EXTENDED SQL 구문              |



실행계획의 시간복잡도

- 테이블 풀 스캔(Full Scan) : O(n)
- 인덱스 스캔 : O(log n)



### 테이블 풀 스캔의 실행 계획

```sql
select * from shops
```

```
|Id| Operation        | Name  | Rows | Bytes| Cost (%CPU) | Time
------------------------------------------------------------
|0| SELECT STATEMENT  |       |  60  | 1260 | 3 (0) | 00:00:01 Time
|1| TABLE ACCESS FULL | SHOPS |  60  | 1260 | 3 (0) | 00:00:01 Time
```





#### 실행 계획 해석

실행 계획의 출력 포맷은 DBMS에 따라 다르지만 공통적으로 포함하는 값이 있다.

1. `조작 대상 객체`
   - 어떤 객체(테이블)를 조작하는지 알려준다.
2. `객체에 대한 조작의 종류`
   - Oracle의 "TABLE ACCESS FULL", PostgreSQL의 "Seq Scan"
3. `조작 대상이 되는 레토드 수`
   - 어느정도의 레코드가 처리되는지 알려준다.
   - SQL 구문 전체의 실행 비용을 파악하는데 중요한 지표가 된다.



### 인덱스 스캔의 실행 계획

인덱스 스캔시, 키의 수 만큼 row를 조회한다.

Operation도 INDEX SCAN 이란 비슷한 단어들로 변화된다.



### 테이블 결합(Join)의 실행 계획

**결합의 실행 계획 알고리즘**

1. Nested Loops (중첩 반복)
   한쪽 테이블을 읽으며 레코드 하나마다 결합 조건에 맞는 레코드를 다른 쪽 테이블에서 찾는 방식 -> 이중 for문
2. Sort Merge
   결합 키로 레코드를 정렬한 뒤, 순차적으로 두 개의 테이블을 결합하는 방법
   결합 전에 전처리로 정렬을 수행해야 한다 -> 워킹 메모리 사용
3. Hash
   해시 테이블을 만듦 -> 작업용 메모리 영역 필요

Oracle : NESTED LOOPS
PostgerSQL : NESTED LOOPS

실행 계획은 일반적으로 트리 구조이다. 중첩 단계가 깊을수록 먼저 실행된다.

옵티마이저가 완벽하지 않기 때문에 Hint 구를 사용해 최적의 선택을 할 수 있게 수동으로 조절할 수 있다.

#### 실행 계획의 실행 비용과 실행 시간

실행 계획에서 표시하는 시간이나 조작 레코드 수는 어디까지나 추정 값일 뿐 지표로 사용할 수 없다.
다만, 일부 DBMS는 구문을 실행해서 실제 시간과 실제 조작 레코드 수를 표시하는 기능을 지원한다.



### 5. 실행 계획의 중요성

최근의 옵티마이저는 우수하지만 완벽하지 않다.
실행 계획이 잘못 설계되었다고 판단되는 경우, 튜닝을 한다. (수동으로 실행 계획 조작)
사실 실행 계획을 본다던가, 변경하는 것은 물리계층을 은폐한다는 RDB의 목표와 반대되는 일이지만 기술이 완벽하지 않은 지금은 필요한 일이다.



- DB는 다양한 트레이드 오프의 균형을 잡는 미들웨어
- 데이터를 디스크와 메모리 중 어디 위치 시킬 지 트레이드 오프가 중요



# 2장 SQL 기초

### ELECT 구와 FROM 구

검색을 위해 사용하는 SQL 구문을 SELECT 구문이라고 한다.

기본적으로 SELECT 구와 FROM 구로 이루어저 있다.

여기서 SQL의 특징을 알 수 있는데 데이터를 '어떤 방법으로' 선택할지 쓰여있지 않다는 것이다.
어떤 데이터가 필요한지 정하기만 하면 DBMS가 프로그래밍에서 절차 지향 같은 부분은 알아서 처리해준다.

### WHERE 구

WHERE 구를 사용해 추가적인 조건을 지정한다.

- WHERE 구에서 사용하는 연산자 (생략)

NULL은 데이터 값이 아니므로, 데이터에 사용하는 연산자(=)를 사용할 수 없다.

SELECT 구문은 절차 지향형 언어의 함수와 동일한 역할을 한다.
SELECT 구문의 입력과 출력 자료형은 **테이블** 뿐이다. 이러한 성질 때문에 폐쇄성(closure property, 관계가 닫혀있다는 의미)을 띈다고 부른다.

### GROUP BY 구

GROUP BY 구를 사용해 합계, 평균과 같은 집계 연산을 수행한다.

GROUP BY 구문을 사용해 여러 **그룹**을 만들고 숫자 관련 함수를 이용해 집계한다.

- GROUP BY 구에서 사용하는 집계함수 (생략)
  GROUP BY 구는 "GROUP BY ()"(생략 가능)를 이용해 테이블 전부를 하나의 그룹으로 만들 수 있다.

### HAVING 구

GROUP BY를 이용해 구한 그룹에 조건을 건다.

WHERE 구는 '레코드'에 HAVING 구는 '집합'에 조건을 지정한다.

### ORDER BY 구

명시적으로 순서를 정할 때 ORDER BY 구를 사용한다.

ASC(ascending order, 오름차순), DESC(descending order, 내림차순)을 키워드로 사용해 차순을 정한다. (명시하지 않으면 ASC)

### 뷰(View)

SELECT 구문을 DB에 저장하는 것을 **뷰**(View)라고 한다.

다만 테이블과 달리 내부에 데이터를 저장하지 않는다. 뷰는 어디까지나` 'SELECT 구문'을 저장`한 것이다.
따라서 SELECT 구문의 FROM 구에 뷰가 있다면 내부적으론 SELECT 구문이 중첩(nested)된 상태이다.

### 서브쿼리

SELECT 구문의 FROM 구에 직접 지정하는 SELECT 구문을 **서브쿼리**(subquery)라고 부른다.

IN 내부에서 서브쿼리를 사용하면 데이터가 변경되어도 따로 수정할 필요가 없다는 점에서 효율적이다.



## 7. 조건 분기, 집합 연산, 윈도우 함수, 갱신

### SQL과 조건 분기

일반적인 절차 지향형 프로그래밍 언어에는 if, switch 조건문 등이 있다.

SQL은 프로그래밍 언어와 달리 절차적으로 기술하지 않기 때문에 '문장'이 아닌 '식'을 기준으로 조건 분기를 정한다.
SQL에서 조건 분기를 실현하는 기능이 **CASE 식**이다.

CASE 식은 절차 지향의 switch 문과 거의 동일한 방식으로 작동한다.

CASE 식의 강점은 '식'이라는 것이다. 따라서 SELECT, WHERE, GROUP BY, HAVING, ORDER BY 구와 같은 곳 어디에나 작성할 수 있다.

```sql
CASE WHEN [평가식] THEN [식]
		 WHEN [평가식] THEN [식]
		 ...
		 ELSE [식]
END
```



### SQL의 집합 연산

SQL에는 테이블을 활용해 집합 연산을 할 수 있다.

#### UNION

UNION을 이용해 테이블 간 `합집합`을 구할 수 있다.
UNION은 합집합을 구할 때 중복을 제거한다. (INTERSECT, EXCEPT도 동일함)

UNION ALL을 이용하면 중복을 제외하지 않는다.

```sql
SELECT *
	FROM Address
UNION
SELECT *
	FROM Address2
```



#### INTERSECT

INTERSECT를 이용해 테이블 간 `교집합`을 구할 수 있다.

```sql
SELECT *
	FROM Address
INTERSECT
SELECT *
	FROM Address2
```

#### EXCEPT

EXCEPT를 이용해 테이블 간 `차집합`을 구할 수 있다.

UNION, INTERSECT와 달리 테이블의 순서에 따라 결과가 달라진다.

```sql
SELECT *
	FROM Address
EXCEPT
SELECT *
	FROM Address2
```

> Address - Address2



### 윈도우 함수

'집약 기능이 없는 GROUP BY 구'이다. GROUP BY 구는 자르기(개인적으로는 나누기라는 표현을 좋아함)와 집약이라는 2개의 기능을 가진다.

기본적인 구문은 집약 함수 뒤에 OVER 구를 작성하고 `PARTITION BY` 혹은 `ORDER BY` 구를 입력하는 것이다.

```sql
# 주소별 숫자
SELECT Address
	COUNT(*) OVER(PARTITION BY Adress)
	FROM Address
```



```sql
# 나이별 순위
SELECT Address
	RANK() OVER(PARTITION BY Adress)
	FROM Address
```

> 건너뛰는 작업 없이 순위를 구하고 싶다면 DENSE_RANK() 사용



### 트랜잭션과 갱신

위에서 말했듯이 DB는 검색 기능이 주가 되는 만큼 SQL의 검색 기능은 복잡하지만 반대로 SQL의 갱신 기능은 간단하다.

기본적으로 SQL의 갱신 작업은 아래와 같이 분류한다.

1. 삽입(insert)
2. 제거(delete)
3. 갱신(update)

1과 3을 합친 MERGE라는 기능도 있지만 일반적으로는 3가지로 분류한다.



# 3장 SQL의 조건 분기

> 식'을 바탕으로 하는 SQL에서 '구문'을 바탕으로 하는 절차 지향형 사고를 사용하면 생기는 문제점



## 쓸데없는 UNION의 사용

UNION은 외부적으로 하나의 SQL 구문을 실행하는 것처럼 보이지만, 내부적으로는 여러 개의 SELECT 구문을 실행하는 실행 계획으로 해석된다. 따라서 I/O 비용이 크게 증가한다.
UNION을 사용해도 좋을지 여부는 신중히 검토해야한다.

> 외부적으로는 하나의 SQL 구문을 사용하는 것처럼 보이지만 실제로는 여러 개의 구문이 실행하는 실행 계획으로 해석되기 때문이다.





다음과 같은 쿼리가 있다고 가정해보자.

```sql
SELECT item_name, year, price_tax_ex AS price
	FROM Items
 WHERE year <= 2001
UNION ALL
SELECT item_name, year, price_tax_ex AS price
	FROM Items
 WHERE year >= 2002;
```

거의 같은 쿼리를 두 번이나 실행하고 있다는 점도 문제지만, 성능적으로 문제가 된다.
UNION을 사용 했을 때의 실행 계획에서 **Item 테이블에 2회 접근**한다.
-> TABLE ACCESS FULL(index없이 테이블을 모두 스캔하는 것)도 2번 발생한다. ***읽어오는 비용도 테이블의 크기에 따라 선형적으로 증가하게 된다.***

UNION은 간단하게 레코드를 합칠 수 있다는 점에서 편리하지만, 물리 자원과 SQL의 성능을 나쁘게 만드므로 정확한 판단 하에 사용해야 한다.

#### 개선된 쿼리

```sql
SELECT item_name, year,
	CASE WHEN year <= 2001 THEN price_tax_ex
    	 WHEN year >= 2002 THEN price_tax_in END AS price
 FROM Items;
```

UNION을 사용한 쿼리와 같은 결과를 출력하지만 성능적으로 CASE를 쓴 쿼리가 훨씬 좋다.
Items 테이블 접근 횟수 : 1회
TABLE ACCESS FULL : 1회
**-> UNION을 사용한 구문보다 성능이 2배 좋아짐\**

UNION의 기본 단위는 SELECT '구문'이다. 이는 아직 절차 지향형의 발상을 벗어나지 못한 방법이다. 

반면, CASE의 기본 단위는 '식'이다.

> ***'구문'에서 '식'으로 사고를 변경하는 것이 SQL을 마스터하는 열쇠 중 하나이다.***





## UNION이 필요한 경우

### UNION을 사용할 수 밖에 없는 경우

머지 대상이 되는 SELECT 구문에서 사용하는 `테이블이 다른 경우`에는 UNION을 사용한다.

```sql
SELECT com_1
	FROM Table_A
 WHERE col_2 = 'A'
UNION ALL
SELECT col_3
	FROM Table_B
 WHERE col_4= 'B';
```

CASE 식을 사용할 수 없는 건 아니지만 이 경우 필요없는 결합이 발생해 성능적으로 악영향이 발생.

따라서 실행 계획을 확인해서 어떤 것이 더 좋은지 확인해야 한다.



### UNION이 성능이 더 좋은 경우

UNION을 사용했을 때 좋은 인덱스(압축을 잘 하는 인덱스)를 사용하지만, 이외의 경우에는 테이블 풀 스캔이 발생한다면, UNION을 사용한 방법이 성능적으로 더 좋을 수 있다.

이러한 경우 N회의 인덱스 스캔 VS 1회의 테이블 풀 스캔 중에서 어느 것이 더 빠른지에 대한 문제가 된다.
이 경우 테이블이 크고, 선택되는 레코드의 수가 적다면 UNION이 더 빠르다.

**예제 테이블**

| key  | name | date_1     | flg_1 | date_2     | flg_2 | date_3     | flg_3 |
| ---- | ---- | ---------- | ----- | ---------- | ----- | ---------- | ----- |
| 1    | a    | 2013-11-01 | T     |            |       |            |       |
| 2    | b    |            |       | 2013-11-01 | T     |            |       |
| 3    | c    |            |       | 2013-11-01 | F     |            |       |
| 4    | d    |            |       | 2013-12-30 | T     |            |       |
| 5    | e    |            |       |            |       | 2013-11-01 | T     |
| 6    | f    |            |       |            |       | 2013-12-01 | F     |

* 빈 값은 모두 NULL

```null
CREATE INDEX IDX_1 ON ThreeElements (date_1, flg_1);
CREATE INDEX IDX_2 ON ThreeElements (date_2, flg_2);
CREATE INDEX IDX_3 ON ThreeElements (date_3, flg_3);
```

이러한 인덱스를 만들어 테이블의 풀스캔보다 훨씬 빠른 접근 속도를 기대할 수 있다고 한다.



요점은 다음과 같다. OR, IN, CASE를 활용했을 경우 1회의 테이블 풀 스캔이 일어난다.

> 3회의 인덱스 스캔 VS 1회의 테이블 풀 스캔 중에 어떤 것이 빠른지 트레이드오프 해야한다.

테이블의 크기와 검색 조건에 따른 선택 비율에 따라 답이 달라진다.
테이블이 크고, WHERE 조건으로 선택되는 레코드의 수가 충분히 작다면 UNION이 더 빠르다.



다음 테이블에서 date가 2013-11-01이고, flg가 T인 레코드들만 추출하고 싶다.



## 절차 지향형과 선언형

### 구문 기반과 식 기반

원래 UNION은 조건 분기를 위해 만들어지지 않았으므로 조건 분기를 위해 만들어진 CASE 식과 비교하면 비효율적인 것이 당연한 결과다.

그럼에도 초보자들이 UNION을 사용해 조건 분기를 하는 이유는 절차 지향형 언어에 익숙해져 있기 때문이다. 하지만 SQL의 기본적인 체계는 선언형이다.

절차 지향형 프로그래밍 언어에서 생각의 기본 단위는 '구문'이다. 하지만 SQL에서의 기본 단위는 '식'이다. 실제로 SQL 구문의 각 부분(SELECT, FROM, WHERE, GROUP BY, ...)에 작성하는 것은 모두 '식'이다.

SQL 능력을 향상시키기 위해선 `선언형`에 익숙해저야 한다.



> 구문 에서 식으로 



# 4장 집약과 자르기

> 집합 지향(set-oriented) - 레코드 단위가 아닌 '집합' 단위로 처리를 기술하는 방식 
>
> **집합 지향**의 특징이 가장 잘 드러나는 상황은 GROUP BY 구, HAVING 구와 SUM, COUNT와 같은 집약 함수를 사용할 때다. 



### 집약 함수 (aggregate function)

집약 함수의 종류에는 다음 5가지 종류가 있다.
5가지에 대한 설명은 다들 알 것이라고 생각하고 생략하겠다.

- COUNT
- SUM
- AVG
- MAX
- MIN

### GROUP BY의 실행 계획

GROUP BY를 사용해 집약을 수행할 경우에 **정렬보다 해시 알고리즘**을 많이 사용한다.
경우에 따라서 정렬을 사용하기도 하지만, 보다 빠른 방법인 해시를 많이 사용한다.
해시의 성질상 GROUP BY의 유일성이 높으면 더 효율적으로 작동한다.

충분한 해시용(or 정렬용) 워킹 메모리가 확보되지 않으면 ***스왑***이 발생한다.
=> 저장소 위의 파일이 사용되면서 굉장히 느려짐



Oracle에서는 정렬과 해시를 위해 ***PGA***라는 메모리 영역을 사용하고, 집약 대상 데이터양에 비해 부족하면, 일시 영역(저장소)를 사용해 부족한 만큼 채운다.

> 이런 현상을 **'TEMP 탈락'**이라고 한다.

스왑이 일어나면 저장소에 TEMP(임시 영역)을 사용하는데 TEMP까지 부족해지면 SQL 구문이 비정상적으로 종료된다.

### PARTITION BY

GROUP BY 구에서 집약 기능을 제외하고 자르는 기능만 남긴 것이 윈도우 함수의 'PARTITION BY' 구이다.

PATITION BY 구는 자르기 부분에서는 GROUP BY와 별다른 부분 없이 동작한다.



# 5장 반복문

5장 요약

> SQL에서 함부로 반복문을 사용하지 마라.



**RDB에는 반복문이 없다. 이는 SQL에서 반복문이 없는 게 좋다고 판단했기 때문이다.**

> Orcal에서는 내부적으로 CASCADE DELETE 또는 CASCADE UPDATE에서 반복문이 사용된다.
>
> 부모 테이블이 변경될 떄 CASCADE 옵션이 적용된 자식 테이블을 갱싱할 때 반복게 코드를 사용한다.
>
> 따라서 대량의 데이터를 갱신시에 성능적 문제가 발생하지만, DBMS 내부적으로 일어나므로 사용자가 제어 또는 튜닝할 수 없다. 

## 반복계의 단점

#### 1. 성능

반복계로 구현한 코드는 **포장계(반복을 사용하지 않은)**로 구현한 코드에 성능적으로 완벽하게 진다.
레코드 수가 적을 때에는 반복계가 빠른 경우도 있지만, 레코드 수가 많아질수록 성능 차이가 더욱 벌어진다.



SQL을 실행할 때는 데이터를 검색하거나 연산하는 실제의 SQL 처리이외에도 다양한 처리가 이루어진다

> SQL 실행의 오버헤드

- 전처리
  1. SQL 구문을 네트워크로 전송
  2. 데이터베이스 연결
  3. SQL 구문 파스
  4. SQL 구문의 실행 계획 생성 / 평가
- 후처리
  1. 결과 집합을 네트워크로 전송

1번과 5번의 과정인 네트워크의 경우 내부의 동일 LAN 위에 있으므로 전송 속도 자체는 거의 밀리 sec이다. 

* -> 오버헤드가 딱히 일어나지 않음

2번 연결같은 경우 **커넥션 풀**이라는 기술을 사용해 거의 문제되지 않는다.

문제가 되는 경우는 3번 구문 파싱(구문 분석, parse)과 4번 실행계획 생성 / 평가 과정이다.

> **파스(parse, 구문 분석)는 SQL을 받을 때 마다 실행되므로 작은 SQL을 여러 번 반복하는 반복계에서는 오버헤드가 높아**질 수 밖에 없다.
>
> - SQL은 실행 시간과 관계 없이 일정한 오버헤드가 필요하다.

![image-20230210225150038](/Users/ysk/study/study_repo/SQL, Database/postgresql/images//image-20230210225150038.png)

#### 2. 병렬 분산의 어려움

반복계는 반복 1회마다의 처리를 굉장히 단순화한다. 따라서 리소스를 분산해서 병렬 처리하는 최적화가 되지 않는다. 

데이터베이스는 대부분 **RAID 디스크**로 구성되어 I/O 부하를 분산화할 수 있게 되어있지만, **반복계에서 실행하는 SQL 구문은 너무 단순해 1회의 접근하는 데이터양이 적다.**

> 반복계는 반복 1회마다 처리가 단순하기 때문에, 리소스를 분산해 I/O를 부하를 분산하는 병렬 처리하는 최적화가 불가능하다.

![image-20230210225316038](/Users/ysk/study/study_repo/SQL, Database/postgresql/images//image-20230210225316038.png)

#### 3. 데이터베이스 업그레이드

DBMS의 버전이 오를수록 옵티마이저는 보다 효율적으로 실행 계획을 세우며, 데이터에 고속으로 접근할 수 있는 아키텍처를 구현한다.
DB **업데이트의 중심은 대규모 데이터를 다루는 복잡한 SQL구문을 빠르게 만들기 위해**서이다.

> 반복계는 미들웨어의 진화의 혜택을 거의 받을 수 없다.

포장계(반복을 사용하지 않은)가 반복계보다 성능이 좋다는 가정은 포장계의 SQL이 충분히 튜닝되어 있다는 가정이 있어야한다.
일반적으로 굉장히 단순한 반복계의 SQL은 튜닝 가능성이 거의 없지만, 포장계의 SQL은 매우 복잡하기 때문에 튜닝의 가능성이 매우 크다. (이것은 포장계의 단점이기도 하다.-복잡하니까)

* 튜닝을 하지 않은 포장계는 반복계보다 느릴 수 있다.
* 하지만 포장계의 SQL 구문은 튜닝할 수 있으므로 제대로 한다면 반복계를 이긴다

> 반복계는 단지 느리기만 한 것이 아닌, 느린 구문을 튜닝할 수 있는 가능성이 거의 없다.
>
> 포장계의 장점은 반대로 반복계의 단점이라고 할 수 있다.



### 반복계를 빠르게 만드는 방법

반복계는 튜닝의 선택지가 한정적이다.

수백 개 정도의 요청정도는 성능이 괜찮게 나온다. 하지만 수백, 수천 개의 처리가 기본인 일괄 처리(batch)같은 경우는 절대로 사용하면 안된다. 따라서 무조건 반복계를 적대시하기보단 상황에 따라서 사용해야 한다.



**반복계를 포장계(반복을 사용하지 않은)로 수정**

이는 애플리케이션 수정을 의미한다. 하지만 실제 상황에서는 여러 이유로 불가능한 경우가 많다.



**각각의 SQL을 수정**

반복계에서 사용하는 SQL 구문을 수정하는 것인데, SQL 구문이 단순한 편이라 **튜닝의 효과를 받기 어렵다.**



**다중화 처리**

CPU 또는 디스크와 같은 리소스에 여유가 있고 처리를 나눌 키가 명확하다면, 성능을 선형에 가깝게 스케일할 수 있다.

### 반복계의 장점

반복계도 다양한 장점이 있다. 따라서 처리 방식을 선택할 때는 장점과 단점의 **트레이드오프에** 대한 신중한 고려가 필요하다.



**실행 계획의 안정성**

실행 계획이 단순하기 떄문에 실행 계획의 **변동의 위험이 거의 없다**.



**예상 처리 시간의 정밀도**

실행 계획이 단순하고 성능이 안정적인 반복계는 예상 처리 시간의 정밀도가 높다.

> <처리 시간> = <한 번의 실행 시간> x <실행 횟수>

포장계는 실행 계획에 따라 성능이 전혀 달라지므로 사전에 예상하기 힘들다.



**트랜잭션 제어의 편리성**

트랜잭션의 정밀도를 미세하게 제어할 수 있다.

만약 중간에 오류가 발생한다면 중간에 커밋을 했으므로 이어서 진행하면 된다. 하지만 포장계는 오류가 발생하면 처음부터 다시해야 한다.



## SQL에서의 반복 표현

반복문을 반복을 사용하지 않은(포장계)로 바꾸는 방법

### 포인트는 CASE 식과 윈도우 함수

SQL에서 반복을 대신하는 수단은 CASE 식과 윈도우 함수다. CASE 식은 IF-THEN-ELSE에 대응한다.

- 작년도와 매출액을 비교해 +, -, = 을 표시하는 SQL

```sql
INSERT INTO Sales2
SELECT company, year, sale,
	CASE SIGN(sale - MAX(sale)
    		OVER(PARTITION BY company
            		ORDER BY year
                    ROWS BETWEEN 1 PRECEDING
                    	AND 1 PRECEDING) )
   WHEN 0 THEN '='
   WHEN 1 THEN '+'
   WHEN -1 THEN '-'
   ELSE NULL END AS var
FROM Sales;
```

* SIGN 함수 : 음수일 경우 -1, 0일 경우 0을 반환
* ROWS BETWEEN 1 PRECEDING AND 1 PRECEDING : 현재 레코드에서 1개 이전부터 1개 이전까지



### 반복 횟수가 정해지지 않은 경우

- 인접 리스트 모델과 재귀 쿼리

이사를 갈 때마다 필드를 추가해야 하는 경우가 있다고 가정.
제일 오래전에 주소를 찾고 싶다.
우편번호를 키로 삼아 이전 주소의 데이터를 줄줄이 연결해 놓았음.
-> 포인터 체인이라고 한다.
-> 포인터 체인을 사용하는 테이블 형식을 '인접 리스트 모델'이라고 부른다.

SQL에서 계층 구조를 찾는 방법 중 하나는 재귀 공통 테이블 식을 사용하는 방법이다.

```sql
WITH RECURSIVE Explosion (name, pcdoe, new_pcode, depth)
AS
(SELECT name, pcode, new_pcode, 1
	FROM PostalHistory
  WHERE name = 'A'
  	AND new_pcode IS NULL // 검색 시작
UNION
SELECT Child.name, Child.pcode, Child.new_pcode, depth + 1
	FROM Explosion AS Parent, PostalHistory AS Child
  WHERE Parent.pcode = Child.new_pcode
  	AND Parent.name = Child.name)

// 메인 SELECT 구문
SELECT name, pcode, new_pcode
	FROM Explosion
  WHERE depth = (SELECT MAX(depth)
  					FROM Explostion);
```

RECURSIVE =  재귀

```sql
WITH RECURSIVE [VIEWNAME]
AS
초기식
UNION
SELECT ~ FROM ~
WHERE RECURSIVE 종료 조건
```

Parent.pcode = Child.new_pcode AND Parent.name = Child.name
즉 Parent의 자식이 있는지 검사해서 있으면 depth를 +1 해준다고 생각하면 된다.
depth의 MAX를 구하면, 그곳이 가장 오래 전에 살은 곳이다.



SQL에서 계층 구조를 나타내는 방법은 크게 3가지가 있다.

1. 인접 리스트 모델
   - **RDB 탄생 전에도 쓰였던 고전적인 방식**
2. 중첩 집합 모델
   - **각 레코드의 데이터를 원(집합)으로 봄**
   - **계층 구조를 집합의 중첩 관계로 나타냄**
3. 경로 열거 모델
   - **갱신이 거의 발생하지 않은 테이블에서 사용함**



> RDB에서 고성능을 실현하고 싶다면, 절차 지향적인 바이어스를 떼어내고 자유로워질 필요가 있다.
>
> 동시에 반복계와 포장계의 장점과 단점을 고려하고, 어느 것을 채택할지 냉정하게 판단해야 한다.
>
> SQL이 가진 강력한 도구와 튜닝 방법을 활용하려면 반드시 **집합 지향**의 사고방식을 가져야한다.



# 6장 결합(Join)

여러 테이블의 데이터를 통합하려면 역정규화(반정규화)나 조인(결합)을 사용한다.

* 결합 알고리즘(조인 알고리즘)은 성능을 결정하고 SQL 전체의 성능을 좌우하는 요인이므로 매우 중요하다.

![image-20230210232331602](/Users/ysk/study/study_repo/SQL, Database/postgresql/images//image-20230210232331602.png)

> 크로스 조인, 내부 조인, 외부 조인, 자기 조인, 등가 조인 비등가 조인, 네츄럴 조인, 셀프 조인 등이 있다.



### 크로스 결합 - CROSS JOIN

- 크로스 결합은 실무에서 거의 사용되지 않는다. - **데카르트 곱 (모든 조합)**
  - 비용이 많이 드는 비효율적인 연산이다.
  - 이러한 결과가 필요한 경우가 거의 없다.
- 크로스 결합은 수학에서 데카르트 곱이라고 불리는 연산으로, 가능한 **모든 조합**을 구하는 연산이다.



### 내부 결합 - INNER JOIN

- 내부 결합의 **'내부**'는 '**데카르트 곱의 부분 집합'**이라는 의미이다.
- 내부 결합은 처음부터 결합 대상을 최대한 축소하는 형태로 작동한다.

### 외부 결합 - OUTER JOIN

- 외부 결합의 **'외부'**는 '데카르트 곱의 부분 집합이 아니다'라는 의미다.
  - 경우에 따라서 테카르트 곱의 부분 집합이 되기도 한다.
- 조인 주인이 되는(left - left table, right - right table, full - left + right) 쪽에 존재하는 키는 모두 포함한다
- 외부 결합은 세 가지 종류가 있다
  - **왼쪽 외부 결합 - LEFT OUTER JOIN**
  - **오른쪽 외부 결합 - RIGHT OUTER JOIN**
  - **완전 내부 결합 - OUTER JOIN**

### 자기 결합 - SELF JOIN

- 자기 결합은 문자 그대로 자신과 결합하는 것이다. 따라서 앞선 세 결합과는 분류가 자체가 다르다.
- 결합 연산의 대상에 자기 자신이 있으면 자기 결합 연산이 된다.
- 논리적으로는 자기 자신과 똑같이 생긴 다른 테이블을 결합한다고 생각해도 상관 없다.

## 결합 알고리즘과 성능

옵티마이저가 선택 가능한 결합 알고리즘의 종류

- Nested Loops
- Hash
- Sort Merge

### 1. Nested Loops 알고리즘

이름 그대로 중첩 반복(2중 포문)을 사용하는 조인 알고리즘이다.

![image-20230210233027902](/Users/ysk/study/study_repo/SQL, Database/postgresql/images//image-20230210233027902.png)

#### 세부 처리방식

1. **결합 대상 테이블(Table_A)**에서 레코드를 하나씩 반복하며 스캔한다. 
   * 이 테이블을(A 테이블) **구동 테이블(driving table)** 또는 **외부 테이블(outer_table)**이라고 한다. 
   * **다른 테이블(B 테이블)**은 **내부 테이블(inner table)**이라고 부른다.

2. 구동 테이블이 레코드를 하나마다 **내부 테이블의 레코드를 스캔**해서 **결합 조건에 맞으면 리턴**한다.
3. 2를 구동 테이블의 모든 레코드에 반복한다.

#### 특징

- **Table_A**, **Table_B**의 결합 대상 레코드를 **R(A)**, **R(B)**라고 하면 접근되는 레코드 수는 **R(A) x R(B)**가 된다.
-  **Nested Loops**의 **실행 시간**은 이러한 레코드 수에 비례한다.
- 한 번의 단계에서 처리하는 레코드 수가 적어, Hash나 Sort Merge에 비해 메모리 소비가 적다.
- 모든 DMBS에서 지원한다.

#### 구동 테이블(결합 대상 왼쪽 테이블)의 중요성

NL 결합에서 구동 테이블으로 작은 테이블을 선택하는 것이 성능 개선이 된다는 말이 있다. 하지만 이 말에는 한 가지 조건이 필요하다. 

바로 '**내부 테이블(오른쪽 테이블 B)의 결합 키 필드에 인덱스가 존재**해야 한다'는 것이다.

인덱스가 존재하면 반복없이 찾을 수 있기 때문에 반복을 생략할 수 있다 . 

민약 구동 테이블의 한 레코드에 내부 테이블의 한 레코드가 대응하고 이를 **인덱스로 찾을 수 있다면,** 

접근하는 레코드 수는 **R(A) x N(리프 노드까지의 거리)**가 된다.

> 내부 테이블(오른쪽 대상 테이블)의 반복을 완전하게 생략할 수 있는 경우는, 결합 키(key)가 내부 테이블에 대해 유일성을 띄면 반복을 완전히 생략할 수 있다. 
>
> **INDEX UNIQUE SCAN**

![image-20230210233451584](/Users/ysk/study/study_repo/SQL, Database/postgresql/images//image-20230210233451584.png)

다만 한 레코드가 여러 레코드에 대응하는 경우 여러개의 레코드에 반복을 적용해야 한다. ( index range scan )



이상적인 경우 : 구동 테이블의 레코드 한 개에 내부 테이블의 레코드 한 개가 대응

- 내부 테이블의 인덱스를 사용해 찾을 수 있으므로 접근하는 레코드 수는
  ***Table_A \* 2***가 된다.

내부 테이블이 클수록 인덱스 사용으로 인한 반복 생략 효과가 커진다.

> '구동 테이블이 작은 Neted Loops + 내부 테이블의 결합 키에 인덱스' 조합은 SQL 튜닝의 기본 중에 기본이다.

#### Nested Loops의 단점

내부 테이블의 **결합 키가 유일하지 않고**, 여러개의 레코드를 **히트(hit)**하게 되어 선택률이 높으면 성능이 악화된다.

* 히트된 여러개의 레코드에 반복을 적용해야 하므로. 
* 이럴 때는 **INDEX RANGE SCAN** 라고 한다. 

![image-20230210233731442](/Users/ysk/study/study_repo/SQL, Database/postgresql/images//image-20230210233731442.png)

해결 방안 :

- 구동 테이블(왼쪽) 로 큰 테이블을 선택하는 역설적인 방법
  - 내부 테이블에 대한 점포 테이블의 접근이 기본 키로 수행되므로, 항상 하나의 레코드로 접근하는 것이 보장된다.
  - **-> 큰 내부 테이블에 계속해서 접근하는 것이 아닌 보다 작은 테이블에 접근하는 것이 효율적이므로 이런 방식이 성능 저하를 막을 수 있는 것 같다.**
- 해시

>최종적으로 SQL의 성능은 처리하는 데이터양에 의존한다.*

### 2. Hash (해시) 알고리즘

![image-20230210234205056](/Users/ysk/study/study_repo/SQL, Database/postgresql/images//image-20230210234205056.png)

입력에 대해 어느정도 유일성과 균일성을 가진 값을 출력하는 해시 함수를 이용한 알고리즘

> 두 테이블 중 **더 작은 테이블**을 **해시 테이블**로 만든다.

**해시의 처리 순서**

1. 작은 테이블을 스캔
2. 결합 키에 해시 함수를 적용해 해시값으로 반환
3. 다른 테이블(큰 테이블)을 스캔하고, 결합 키가 해시값에 존재하는지 확인

작은 테이블을 스캔하는 이유는 해시 테이블이 **워킹 메모리**에 저장되므로 **조금이라도 작은 것이 효율적**이기 때문이다!



#### 해시방식의 특징

- 결합 테이블로부터 해시 테이블을 만들어서 활용하므로, Nested Loops에 비해 메모리 소모가 크다.
  - 따라서 동시 실행성이 높은 OLTP에는 적절하지 않고, 야간 배치, BI/DWH에 사용한다.
  - **OLTP : 온라인 트랜잭션 처리(Online Transaction Processing)**
- 메모리가 부족하면 저장소를 사용하므로 지연이 일어난다.
- 출력되는 해시값은 입력값의 순서를 알지 못하므로, 등치(=) 결합에만 사용할 수 있다.

**해시가 유용한 경우**

- Nested Loops에서 적절한 구동 테이블(상대적으로 충분히 작은 테이블)이 존재하지 않는 경우
- 구동 테이블로 사용할만한 작은 테이블은 있지만, 내부 테이블에서 히트되는 레코드 수가 너무 많은 경우
- 내부 테이블에 인덱스가 존재하지 않는 경우

> Nested Loops가 효율적으로 작동하지 않는 경우에 사용할 수 있는 차선책이 해시이다.

하지만 해시는 반드시 양쪽의 테이블을 모두 읽어야 하므로 데이터 풀스캔이 사용되는 경우가 많다.
테이블의 규모가 굉장히 크다면, 풀 스캔에 걸리는 시간도 고려해야 한다.

### 3. Sort Merge 알고리즘

![image-20230210234833528](/Users/ysk/study/study_repo/SQL, Database/postgresql/images//image-20230210234833528.png)

**Sort Merge의 작동**

Nested Loops가 비효율적인 경우 Hash 사용 외에 또다른 선택지로 SortMerge라는 알고리즘도 있다.

> SortMerges는 간단하게 Merge 또는 Merge Join이라고 부르기도 한다.

결합 대상 테이블을 각각 결합 키로 정렬하고, 일치하는 결합 키를 찾으면 결합한다.

> 1. 결합 대상 테이블을 각각 결합키로 정렬한다.
> 2. 일치하는 결합키를 찾으면 결합한다.



**Sort Merge의 특징**

- 대상 테이블을 모두 정렬하므로 Nested Loops보다 메모리 소모가 크고, 상황에 따라 Hash보다 많은 메모리를 사용한다.
- 동치 결합뿐 아니라 부등호(=, >=, <, ...)를 사용한 결합에도 사용할 수 있다.
  - 단 부정조건(<>) 결합에는 사용할 수 없다.
- 테이블이 결합키로 정렬되어 있다면 정렬이 필요 업지만, 이는 구현 의존적이다.

테이블 정렬에 많은 시간과 리소스를 요구할 가능성이 있다.

> 테이블 정렬을 생략할 수 있는 경우에는 고려해볼 만 하지만, 그 이외의 경우는 Nested Loop와 Hash를 우선적으로 고려해야 한다.



**Sort Meger가 유효한 경우**

Sort Merge은 결합 자체에 걸리는 시간은 나쁘지 않은 편이지만, 정렬에 많은 시간과 리소스를 소모할 가능성이 있으므로 테이블 정렬을 생략할 수 있는(매우 예외적인) 상황을 제외하고는 Nested Loops나 Hash를 우선적으로 고려하는게 좋다.



## 의도하지 않은 크로스 결합(Cross Join)

의도하지 않는 크로스 결합이 일어나는 경우는 대부분 작성자의 실수 때문이고, 대부분 '삼각 결합'이라 부르는 패턴에서 문제가 발생한다.

![image-20230210235409027](/Users/ysk/study/study_repo/SQL, Database/postgresql/images//image-20230210235409027.png)

```sql
SELECT A.a, B.b, C.c
FROM Table_A A
	INNER JOIN Table_B B
		ON A.a = B.b
	INNER JOIN Table_C C
		ON A.a = C.c
```

만약 조인 조건(결합 조건)이 되는 키가 A - B, A -C에만 존재하고 B - C는 일치하지 않는다고 가정.

* 조인 조건이 일치하지 않는 B-C 를 먼저 결합하고 후 A를 결합하는 순간 결합 조건이 없으므로 크로스 조인해버린다.

> 이러한 형태의 삼각 결합에서 Table_B와 Table_C의 결합이 일어난다면, 둘 사이에는 아무런 결합조건이 없기에 크로스 결합이 일어난다.
>
> 사실 작은 테이블 사이의 크로스 결합은 자주 일어나며, 크게 두려워할 필요는 없다. 하지만 비교적 큰 테이블 끼리의 크로스 결합이 일어나는 경우는 큰 성능 저하가 일어난다.

### 의도하지 않은 크로스 결합을 회피하는 법

불필요한 결합 조건을 추가해서 크로스 결합을 회피한다

```sql
SELECT A.a, B.b, C.c
FROM Table_A A
	INNER JOIN Table_B B
		ON A.a = B.b
	INNER JOIN Table_C C
		ON A.a = C.c
		AND C.c = B.b; # 불필요한 결합 조건을 추가하여 크로스 조인을 회피
```

## 결합이 느리다면 (Join이 느리다면)

### 1. 상황에 따른 최적의 결합 알고리즘

Nested Loops, Hash ,Sort Merge 3가지의 알고리즘의 장, 단점

| 이름         | 장점                                                         | 단점                                                         |
| ------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| Nested Loops | '작은 구동 테이블' + '내부 테이블의 인덱스 '라는 조건이 있다면 굉장히 빠르다.<br />메모리 또는 디스크 소비가 적으므로 내부 테이블의 선택률이 높으면 느리다. | 대규모 테이블들의 결합에는 부적합<br />내부 테이블의 인덱스가 사용되지 않거나 내부 테이블의 선택률이 높으면 느리다. |
| Hash         | 대규모 테이블들을 결합할 때 적합                             | 메모리 소비량이 큰 OLTP에는 부적합<br />메모리 부족이 일어나면 TEMP 탈락 발생<br />등가 결합에서만 사용 가능 |
| Sort Merge   | 대규모 테이블들을 결합할 때 적합<br />비등가 결합에서도 사용 가능 | 메모리 소비량이 큰 OLTP에는 부적합<br />메모리 부족이 일어나면 TEMP 탈락 발생<br />**데이터가 정렬되어 있지 않다면 비효율적** |

옵티마이저는 이런 장단점을 보면ㅅ서 알고리즘을 선택한다. 

* 하지만 옵티마이저도 완벽한 존재는 아니므로, 통계정보가 오래되어 제대로 된 정보를 검색하지 못한다면 최적의 결합알고리즘을 선택하지 못할수도 있다.



최적의 결합 알고리즘을, 결합 대상 레코드 수의 관점에서 정리하면 다음과 같

**소규모 - 소규모**

어떤 알고리즘을 사용해도 큰 차이가 없습니다.



**소규모 - 대규모**

소규모 테이블을 구동 테이블로 하는 경우 Nested Loops를 사용한다.  

하지만 내부 테이블의 결합 대상 레코드가 너무 많다면 구동 테이블과 내부 테이블을 바꾸거나, Hash를 사용하는 것을 검토해 보아라.



**대규모 - 대규모**

Hash를 사용한다. 결합키로 정렬이 되어 있다면 Sort Merge를 사용한다.

> 사용할 수 있는 메모리의 양, 또는 결합 키의 카디널리티 등 세세한 조건에 따라 최적의 방법은 바뀔 수 있다.
>
> 기본적으로는 일단 Nested Loops 잘 안되면 Hash

### 2. 실행 계획 제어

실행 계획을 바꾸는 것은 DBMS 마다 다르다.

- Oracle
  - 힌트 구로 결합 알고리즘을 제어 할 수 있다.
  - 구동 테이블도 지정할 수 있다.
- SQL Server
  - 힌트 구로 결합 알고리즘을 제어할 수 있다.
- DB2
  - 힌트 구가 없으며, 원칙적으로 실행계획을 제어할 수 없다.
- PostgreSQL
  - pg_hint_plan을 이용해 결합 알고리즘을 제어할 수 있다.
  - 서버 매개변수로 데이터베이스 전체를 제어할 수 있다.
- MySQL
  - 결합 알고리즘 자체가 Nested Loops 계열 밖에 없어서 제어의 의미가 없다.

사용자가 실행 계획을 제어하면, 실행 계획이 고정되기 때문에 비효율적일 수 있다.
그렇다고 옵티마이저의 선택이 언제나 옳지도 않다.

**조인(결합)을 피하는 방법**

- 비정규화 (뭔지 모름)
- 결합을 최소화 -> 상관 서브쿼리로 대체, 등



# 7장 서브쿼리

서브쿼리란 SQL 내부에서 작성되는 일시적인 테이블이다.
테이블과 서브쿼리는 기능적인 관점에서 차이가 없기 때문에 **SQL은 두 가지를 모두 같은 것으로 취급**한다.

- 테이블 : 영속적인 데이터를 저장
- 뷰 : 영속적이지만 데이터는 저장하지 않음. (접근할 때마다 SELECT 구문 실행)
- 서브쿼리 : 비영속적인 생존 기간(스코프)이 SQL 구문 실행 중으로 한정

> 성능적인 관점에서 보면 서브쿼리 또는 뷰는 같은 데이터를 저장하고 있는 경우라도 테이블에 비해 성능이 나쁜 경향이 있다.



## 서브쿼리의 문제점

서브쿼리의 성능적 문제는 서브쿼리가 실체적인 데이터를 저장하고있지 않다는 점을 생각하자.

#### 연산 비용 추가

- 데이터를 저장하고 있지 않아, 서브쿼리에 접근할 때마다 SELECT 구문을 실행해서 데이터를 만들어야 한다.
  - SELECT 구문 실행에 발생하는 비용이 추가
  - 서브쿼리의 내용이 복잡하면 복잡할수록 이러한 실행비용은 높아진다.

#### 데이터 I/O 비용 발생

- 연산 결과는 어딘가에 저장하기 위해 써두어야 한다.
- 데이터양이 큰 경우에는 DBMS가 저장소에 있는 파일에 결과를 쓸 때도 있다.
-  메모리가 크다면 상관 없지만 부족하다면 저장소의 파일을 사용하는 경우도 있다. 
  - 이렇게 되면 저장소 성능에 따라 접근 속도가 떨어진다.
- TEMP 탈락 현상이 발생해 접근 속도가 급격하게 떨어진다.

#### 최적화를 받을 수 없음

* 서브쿼리로 만들어지는 데이터는 테이블과 똑같은 구조를 가진다. 

* 하지만 테이블과는 달리 **명시적인 제약이나 인덱스같이 메타 정보가 없다.** 

* **따라서 옵티마이저가 쿼리를 해석하기 위해 필요한 정보를 얻을 수 없다.**

* 필요한 정보를 서브쿼리에서 얻기 위한 해결하기 위한 방법 : 뷰 병합(view merge)

**단점**

1. 서브쿼리를 사용하면 코드가 여러 계층에 걸쳐 만들어져 가독성이 떨어진다.

2. 성능

   - 서브쿼리는 대부분 일시적인 영역(메모리 or 디스크)에 확보되므로 오버헤드 발생

   - 서브쿼리는 인덱스 또는 제약 정보를 가지지 않아 최적화되지 못함

   - 이 쿼리는 결합을 필요로 하기 때문에 비용이 높고 실행 계획 변동 리스크가 발생

   - Receipts 테이블에 스캔이 두 번 발생

### 장기적인 리스크 관리

결합을 사용한 쿼리는 다음과 같은 불안정 요소를 가진다.

- 결합 알고리즘의 변동 리스크
- 환경 요인에 의한 지원 리스크(인덱스, 메모리, 매개변수, ...)

따라서 결합을 사용한다면 이러한 리스크를 안아야 한다. 그러므로 가능하다면 결합을 사용하지 않는 쿼리를 작성하는게 리스크를 줄일 수 있다.



## 서브쿼리 사용이 더 나은 경우

서브쿼리 사용이 성능 측면에서 더 나은 경우는 결합(조인)을 사용 할 때다.

결합을 사용할 때는 최대한 결합 대상 레코드를 줄여야 한다. 

그런데 옵티마이저가 이러한 것을 잘 판별하지 못할 때는 서브쿼리를 사용해 명시적으로 처리해 주는것이 좋다.



# 8장 SQL의 순서

## 레코드에 순번 붙이기

이 부분에서는 여러 경우에 윈도우 함수와 상관 서브쿼리를 이용해 순번을 붙인다.

이전 장에서 설명했듯이 서브쿼리 보다는 윈도우 함수를 사용하는 것이 더 좋은 성능을 가진다.

1. 기본키가 한 개의 필드일 경우
2. 기본키가 여러 개의 필드일 경우
   상관 서브쿼리의 경우 **다중 필드 비교**를 사용한다.
3. 그룹마다 순번을 붙이는 경우
4. 순번과 갱신
   기존 테이블에 순번 필드를 추가하여 순번을 갱신하는 구문을 설명한다.



**윈도우 함수를 이용한 방법 - ROW_NUM**

```sql
SELECT student_id ROW_NUMBER() OVER (ORDER BY student_id) AS seq
	FROM Weights;
```

**상관 서브쿼리를 사용 - MySQL**

```sql
SELECT student_id, 
		(SELECT COUNT(*) 
    	FROM Weight W2
    	WHERE W2.student_id <= W1.student_id) AS seq
FROM Weights W1;
```

> 성능은 윈도우 함수가 더 좋다. 



### 레코드에 순번 붙이기 응용

#### 테이블의 중앙값 구하기

| student_id(학생 ID) | weight(체중 kg) |
| ------------------- | --------------- |
| A100                | 50              |
| A101                | 55              |
| A124                | 55              |
| B343                | 60              |
| B346                | 72              |
| B378                | 72              |
| C563                | 72              |
| C345                | 72              |



**1. 집합 지향적 방법**

테이블을 상위 집합과 하위 집합으로 분할하고 그 공통부분을 검색하는 방법

```sql
SELECT AVG(weight)
	FROM (SELECT W1.weight
       		FROM Weights W1, Weights W2
       	GROUP BY W1.weight
        HAVING SUM(CASE WHEN W2.weight >= W1.weight THEN 1 ELSE 0 END)
        	>= COUNT (*) / 2
        AND SUM(CASE WHEN W2.weight <= W1.weight THEN 1 ELSE 0 END)
        	>= COUNT (*) TMP
       )
```

* CASE 식에 표현한 두 개의 함수 (포함된다면 1 아니면 0)로 상위 집합과 하위 집합을 분할.
* SQL 스러운 방법
* 단점 1 : 코드가 복잡해서 이해하기 힘들다
* 단점 2 : 성능이 나쁘다. 셀프 조인이라서

**개선 - 절차 지향적 방법 1**

```sql
SELECT AVG(weight) as median
	FROM (SELECT weight, 
       				ROW_NUMBER() OVER (ORDER BY weight ASC, student_id ASC) AS hi,
        			ROW_NUMBER () OVER (ORDER BY weight DESC, student_id DESC) AS lo
        FROM weight) TMP
  WHERE hi IN (lo, lo + 1, lo -1)
```

* 양쪽 끝에서 레코드 하나씩 세어 중간을 찾는다.

![image-20230211022750160](/Users/ysk/study/study_repo/SQL, Database/postgresql/images//image-20230211022750160.png)

오름차순, 내림차순한 뒤 같은 속도로 움직인다.
그때 lo와 hi가 똑같은 시점이 중앙값이다.
그런데 왜 lo+1, lo-1 도 있을까? -> 짝수일 경우 2개의 평균으로 구해야 하기 때문.
그래서 AVG도 있는 것이다. 홀수와 짝수의 조건 분기를 IN으로 한꺼번에 수행

그러나 정렬 2회, 테이블 스캔 1회로 성능적으로 더 개선할 수 있다.

**개선 2. 2빼기 1은 1**

```sql
SELECT AVG(weight)
	FROM (SELECT weight,
    			2 * ROW_NUMBER () OVER(ORDER BY weight)
                - COUNT(*) OVER() AS diff
          FROM Weights) TMP
 WHERE diff BETWEEN 0 AND 2;
```

순번을 2배 해 COUNT(*)를 빼주면 홀수개일 경우 1이 중앙값, 짝수개일 경우 0과 2가 중앙 값으로 AVG를 구해준다.

이 쿼리의 실행 계획은 스캔 1회, 정렬 1회로 SQL 표준으로 중앙값을 구하는 가장 빠른 쿼리이다.



### 순번을 사용한 테이블 분할

- 단절 구간 찾기

| num(숫자) |
| :-------- |
| 1         |
| 3         |
| 4         |
| 7         |

#### 집합 지향적 방법

다음 번의 레코드와의 차이가 1이 아니면 단절된 구간이므로 그것을 표시하면 된다.

```sql
SELECT (N1.num + 1) AS gap_start,
		'~',
        (MIN(N2.num) -1) AS gap_end
	FROM Numbers N1 INNER JOIN Numbers N2
    	ON N2.num > N1.num
  GROUP BY N1.num
 HAVING (N1.num + 1) < MIN(N2.num);
```

N1.num 보다 큰 N2.num을 가져온 뒤, N2.num의 최솟값과 1 차이가 나는지 판단한다.

> 코드는 매우 간단하며 집향 지향적인 좋은 방법이다.
>
> - 집합 지향적인 방법은 반드시 **자기 결합**을 사용해야 한다.
>   -> 쿼리의 **비용이 높으며**, **실행 계획 변동 위험**을 안게 된다.

#### 절차 지향적 방법

- 다음 레코드와 비교

```sql
SELECT num + 1 AS gap_start,
	'-',
    (num + diff - 1) AS gap_end
  FROM (SELECT num,
  			   MAX(num)
               	OVER(ORDER BY num
                	ROWS BETWEEN 1 FOLLOWING
                    		 AND 1 FOLLOWING) - num
         FROM Numbers) TMP(num, diff)
  WHERE diff <> 1;
```

윈도우 함수를 이용해 현재 레코드의 다음 레코드를 구한 뒤, 이들 두 레코드의 숫자 차이를 diff 필드에 저장해 연산한다.

FOLLOWING : 다음 행
diff : 다음 레코드[MAX(num)] - num <> 1 이면 연속이 아니다.

테이블 접근 1회 / 윈도우 함수에서 정렬 사용
-> 결합을 사용하지 않으므로 성능이 굉장히 안정적이다.



### 테이블에 존재하는 SEQUENCE, IDENTITY 구하기

이 기능은 모두 최대한 사용하지 않는 것이 좋다.
IDENTITY 필드보다는 시퀀스 객체를 사용하라.

**시퀀스 객체 정의**

```sql
CREATE SEQUENCE testseq
START WITH 1
INCREMENT BY 1
MAXVALUE 10000
MINVALUE 1
CYCLE;
```

시퀀스 객체의 문제점

- 표준화가 늦어, 구현에 따라 구문이 달라 이식성이 없다.
- 시스템에서 자동으로 생성되는 값이므로 실제 엔티티 속성이 아니다.
- 성능적인 문제를 일으킨다.

시퀀스 객체는 유일성, 연속성, 순서성을 만족하기때문에 **락(lock) 메커니즘이 필요**하다.

어떤 사용자가 시퀀스 객체를 사용하고 있다면, 시퀀스 객체를 락해서 다른 사용자로부터의 접근을 블록하는 배타 제어를 수행한다.

따라서 락 충돌로 인해 성능 저하 문제가 발생하고, 연속적으로 사용하면 오버 헤드가 발생한다.

#### 시퀀스 객체로 발생하는 성능 문제의 대처

옵션

- CACHE : 새로운 값이 필요할 때마다 메모리에 읽어들일 필요가 있는 값의 수를 설정, 연속성을 담보할 수 없음 -> 장애가 발생하면 비어있는 숫자 발생
- NOORDER : 순서성을 담보하지 않아 오버 헤드를 줄일 수 있다.

#### 순번을 키로 사용할 때의 성능 문제

**핫 스팟(Hot Spot) 문제**

- 순번처럼 비슷한 데이터를 연속적으로 INSERT하면 물리적으로 같은 영역에 저장된다.
- 저장소의 특정 물리적 블록에만 I/O 부하가 커져 성능 악화가 발생한다.
- 이렇게 부하가 몰리는 부분을 **'Hot Spot'** or **'Hot Block'**라고 한다.

대처

1. 연속된 값을 도입하는 경우에 DBMS 내부에서 변화를 주어 분산할 수 있는 구조 (해시)를 사용하는 것
2. 인덱스에 일부러 복잡한 필드를 추가해 데이터의 분산도를 높이는 방법
   -> INSERT는 빨라지지만, I/O 양이 늘어나 SELECT 구문의 성능이 나빠진다.

> 시퀀스 객체는 최대한 사용하지 말아야 한다.

### IDENTITY 필드

'자동 순번 필드'
테이블에 INSERT가 발생할 때마다 자동으로 순번을 붙여준다.
기능적, 성능적 측면에서 모두 시퀀스 객체보다 심각한 문제를 가진다.

- IDENTITY 필드는 특정한 테이블과 연결된다.
- 구현에 따라 이들을 아예 사용할 수 없거나 제한적으로만 사용할 수 있다.

# 9장 갱신과 데이터 모델

SQL의 대부분은 SELECT 구문이라고 할 수 있다.
반면 UPDATE, DELETE라는 갱신을 위한 기능은 상세하게 다뤄볼 기회가 거의 없다.
그 결과 갱신과 관련된 SQL 구문은 검색 **SQL 구문 이상으로 비효율적으로 성능이 좋지 않은 방향으로 작성**된다.

갱신을 효율적으로 수행할 수 있는 SQL을 보자.

## 1. NULL 채우기

| keycol | seq  | val  |
| ------ | ---- | ---- |
| A      | 1    | 50   |
| A      | 2    |      |
| A      | 3    |      |
| B      | 4    | 60   |
| B      | 5    |      |
| B      | 6    | 72   |
| B      | 7    |      |
| B      | 8    |      |

다음 테이블은 이전 레코드와 같은 값을 가질 경우 생략한 테이블이다.
여기에 값들을 채울 예정이다.
[where val IS NULL] 로 입력해야 한다는 것은 쉽게 알 수 있다.
대부분 커서(cursor) 또는 호스트 언어로 레코드를 하나씩 읽고 반복문을 돌리는, 반복계를 사용한 접근법을 떠올릴 것이다.
-> 좋지 않은 방법이라는 것 또한 대부분 알고 있다.

### 상관 서브 쿼리를 사용한 접근

- 같은 keycol 필드를 가짐
- 현재 레코드보다 작은 seq 필드를 가짐
- val 필드가 NULL이 아님

```sql
UPDATE OmitTbl
	SET val = (SELECT val
    			FROM OmitTb1 OT1
               WHERE OT1.keycol = OmitTbl.keycol1 # 같은 keycol
               	AND OT1.seq = (SELECT MAX(seq)
                				FROM OmitTbl OT2
                               WHERE OT2.keycol = OmitTbl.keycol
                               	AND OT2.seq < OmitTbl.seq #자신보다 작은 seq
                                AND OT2.val IS NOT NULL)) # NOT NULL
  WHERE val IS NULL;
```

내가 이해한 바로 다시 설명

- 자신보다 작은 seq, val != null => 값이 채워져있고, 내 전의 필드
- MAX(seq) -> 내 전의 필드 중 값이 있는 가장 가까운 필드
- 그 필드랑 OT1의 seq랑 똑같을 때
- WHERE val IS NULL -> 비어있는 값을 UPDATE

#### 실행 계획

데이터양이 늘어날 경우 (keycol, seq)를 기본 키 인덱스로 활용할 확률이 높아 반복계에 비해 성능이 높을 수 있다.



## 레코드에서 필드로의 갱신

2개의 테이블을 사용해서 한쪽 테이블의 정보를 편집하고 다른 테이블로 복사하는 UPDATE 구문

| student_id(학생 ID) | subject(과목) | score(점수) |
| ------------------- | ------------- | ----------- |
| A001                | 영어          | 100         |
| A001                | 국어          | 58          |
| A001                | 수학          | 90          |
| B002                | 영어          | 77          |
| B002                | 국어          | 60          |
| C001                | 영어          | 72          |
| C003                | 국어          | 49          |
| C003                | 사회          | 100         |

원하는 필드

| student_id(학생 ID) | score_en(영어 점수) | score_nl(국어 점수) | score_mt(수학 점수) |
| ------------------- | ------------------- | ------------------- | ------------------- |
| A001                |                     |                     |                     |
| B002                |                     |                     |                     |
| C001                |                     |                     |                     |
| C003                |                     |                     |                     |
| D004                |                     |                     |                     |

알 수 없는 값은 NULL을 입력한다.

### 방법 다중 필드 할당

이럴 때 사용할 수 있는 강력한 무기가 바로 다중 필드 할당(Multiple Fields Assignment)이다.
**여러 개의 필드를 리스트화하고 한 번에 갱신하는 방법**

```sql
UPDATE ScoreCols
	SET (score_en, score_nl, score_mt)
    	= (SELECT MAX(CASE WHEN subject = '영어'
        					THEN score
                            ELSE NULL END) AS score_en,
                  MAX(CASE WHEN subject = '국어'
        					THEN score
                            ELSE NULL END) AS score_nl,
                  MAX(CASE WHEN subject = '수학'
        					THEN score
                            ELSE NULL END) AS score_mt,
              FROM ScoreRows SR
            WHERE SR.student_id = ScoreColse.student_id);
```

서브쿼리를 한꺼번에 처리할 수 있어 성능도 향상되고 코드도 간단해진다.
갱신해야 할 필드의 수가 늘어나도, 서브쿼리의 수가 늘어나지 않으므로 성능적으로 악화될 염려가 없다.

> 테이블 접근 3회 -> 1회로 감소
>
> - 유일 검색(INDEX UNIQUE SCAN) -> 범위 검색(INDEX RANGE SCAN)
> - MAX 함수의 정렬이 추가됨

- 스칼라 서브쿼리
  - 서브쿼리 내부를 보면 CASE 식으로 과목별 점수를 검색한다. 이때 중요한 것은 **MAX함수**를 적용하는 것이다.
  - 특정 학생의 레코드가 여러개일 수 있으므로 이렇게 집약해야 한다.

#### NOT NULL 제약이 걸려있는 경우

```sql
UPDATE ScoreCols
	SET (score_en, score_nl, score_mt)
    	= (SELECT COALESCE(MAX(CASE WHEN subject = '영어'
        					THEN score
                            ELSE NULL END), 0) AS score_en,
                  COALESCE(MAX(CASE WHEN subject = '국어'
        					THEN score
                            ELSE NULL END), 0) AS score_nl,
                  COALESCE(MAX(CASE WHEN subject = '수학'
        					THEN score
                            ELSE NULL END), 0) AS score_mt,
              FROM ScoreRows SR
            WHERE SR.student_id = ScoreColse.student_id)
  WHERE EXISTS (SELECT *
  					FROM ScoreRows
                WHERE student_id = ScoreColsNN.student_id);
```

COALESCE -> NULL을 0으로 변경
EXISTS -> 두 테이블에 일치하는 student_id가 존재

### 같은 테이블의 다른 레코드로 갱신

| brand(브랜드) | sale_date(거래일) | price(종가) |
| ------------- | ----------------- | ----------- |
| A철강         | 2008-07-01        | 1000        |
| A철강         | 2008-07-04        | 1200        |
| A철강         | 2008-08-12        | 800         |
| B상사         | 2008-06-04        | 3000        |
| B상사         | 2008-09-11        | 3000        |

주가 테이블(타겟 테이블)

| brand(브랜드) | sale_date(거래일) | price(종가) | trend(트랜드) |
| ------------- | ----------------- | ----------- | ------------- |
|               |                   |             |               |

trend는 이전 종가와 현재 종가를 비교해 올랐다면 '↑', 내렸다면 '↓', 그대로라면 '->' 값을 지정한다. 각 종목을 처음 거래한 날은 연산할 것이 없으므로NULL로 처리한다.



#### 1. 상관 서브쿼리 사용

```sql
INSERT INTO Stocks2
SELECT brand, sale_date, price,
	CASE SIGN(price -
    			(SELECT price
                	FROM Stocks S1
                  WHERE brand = Stocks.brand
                  		AND sale_date =
                        		(SELECT MAX(sale_date)
                                       FROM Stocks S2
                                        WHERE brand = Stocks.brand
                                         AND sale_date < Stocks.sale_date)))
                  WHEN -1 THEN '↓'
                  WHEN 0 THEN '->'
                  WHEN 1 THEN '↑'
                  ELSE NULL
    	END
FROM Stocks;
```

- SIGN 함수 : 양수라면 1, 음수라면 -1, 0이라면 0을 리턴함

실행 계획

- 기본 키 인덱스에 대한 인덱스 온리 스캔
- 기본 키 인덱스를 사용한 테이블 접근
- 테이블 풀 스캔

-> 테이블 접근 횟수를 줄여 성능을 개선할 수 있다.

### 2. 윈도우 함수 사용

```sql
INSERT INTO Stocks2
SELECT brand, sale_date, price,
	CASE SIGN(price - 
    			MAX(price) OVER (PARTITION BY brand
                					ORDER BY sale_date
                                 ROWS BETWEEN 1 PRECEDING
                                 		AND 1 PRECEDING))
       WHEN -1 THEN '↓'
       WHEN 0 THEN '->'
       WHEN 1 THEN '↑'
       ELSE NULL 
    END
  FROM Stocks S2;
```

상관 서브쿼리를 사용한 실행 계획보다 매우 간단해진다.
-> Stocks 테이블에 대한 접근도 풀 스캔 한 번으로 감소한다.

### INSERT SELECT와 UPDATE 비교

INSERT SELECT

장점

- UPDATE에 비해 성능적으로 나으므로 고속 처리를 기대할 수 있다.
- 자기 참조를 허가하지 않는 데이터베이스에서도 INSERT SELECT를 사용할 수 있다. (참조 대상 테이블과 갱신 대상 테이블이 서로 다른 테이블)

단점

- 같은 크기와 구조를 가진 데이터를 두 개 만들어야 한다.
  -> 저장소 용량을 2배 이상 소비한다.

Stocks2 테이블을 뷰로 만드는 방법

- 저장소 용량을 절약할 수 있다.
- 정보를 항상 최신으로 유지할 수 있다.

-> 뷰에 접근이 발생할 때마다 복잡한 연산이 수행돼 Stocks2에 접근하는 쿼리의 성능이 낮아진다.

> **성능과 동기성의 트레이드오프**

### 모델 갱신을 사용하는 방법

> 문제 : 주문, 주문 명세 테이블이 있을 때 주문마다 주문일(주문.orderDate)과 상품의 배송 예정일(주문아이템.delivery_date)의
>
> 차이를 구해 3일 인 로우를 구하고 싶다.  

SQL 구문을 사용하는 것이 무조건 최적의 해답은 아니다. 이 문제는 'SQL에 의지하지 않고'도 해결할 가능성이 있다.

주문 테이블에 배송 지연 플래그 필드를 추가하고, 검색 쿼리는 해당 플래그만을 조건으로 삼으면 매우 간단하게 해결할 수 있다.

* 주문 테이블에 배송 지연 플래그 필드를 업데이트 해야하는 문제가 남아있긴 하다.

SQL을 사용해 해결하려는 것 말고도 다른 해결 방법이 있다는 것을 생각해야 한다.

### 모델 갱신의 주의점

### 높아지는 갱신 비용

위 내용을 예시로 들면 주문 테이블의 배송 지연 플래그 필드에 값을 넣는 처리가 필요하다. 이는 검색 부하를 갱신 부하로 미루는 꼴이다.

만약 주문 테이블에 레코드를 등록할 때 이미 플래그가 정해져 있다면 갱신 비용은 거의 올라가지 않지만, 현실적으로는 주문 접수 시점에 배송 예정일이 정해져 있지 않는 경우가 대부분일 것이다. **따라서 UPDATE를 해야 하므로 갱신 비용이 올라간다.**

### 갱신까지의 시간 랙(Time Rag) 발생

주문 테이블의 배송 지연 플래그 필드와 주문 명세 테이블의 배송 예정일 필드가 실시간으로 동기화되지 않으므로 차이가 발생할 수 있다.

![image-20230211025445958](/Users/ysk/study/study_repo/SQL, Database/postgresql/images//image-20230211025445958.png)

* 특히 야간 배치와 같이 일괄 처리를 한다면 시간 랙은 더 길어진다. 
* 실시간성을 높이게 되면 위 그림에서 3과 4의 간격이 짧아져야 한다. 
  * 완전한 실시간성을 요구하는 경우는 3과 4가 동일한 트랜잭션으로 처리되어야 한다. 
* 하지만 이는 성능과 실시간성 사이에 심각한 트레이드 오프가 발생한다.

# 10장 인덱스 사용

> "가장 일반적이면서 중요한 인덱스 방법은 B-Tree다. 모든 애플리케이션을 만족시킬 수 있는 최적의 메모리 구조란 존재하지 않지만, 그래도 하나를 선택해야 한다면 B-Tree를 선택할 것이라는 뜻이다."

RDB 튜닝에서 인덱스 활용이 제일 중요하다.

애플리케이션을 변경하지 않아도 DB 조작으로 성능을 개선할 수 있다는 높은 편리성과 결과도 굉장히 좋다.

## 인덱스와 B-tree

RDB에서 사용하는 인덱스는 구조에 따라 세 가지로 분류된다.

- B-tree 인덱스
- 비트맵 인덱스
- 해시 인덱스



### 1. 만능형 B-tree

![image-20230211030057867](/Users/ysk/study/study_repo/SQL, Database/postgresql/images//image-20230211030057867.png)

- 데이터를 트리 구조로 저장하는 형태의 인덱스이다.
- 균형잡힌 뛰어난 범용성을 인정받아 가장 많이 사용된다.
- [CREATE INDEX] 구문을 실행하면 모든 DBMS에서 B-Tree 인덱스가 만들어진다.

B-Tree가 검색 알고리즘으로서는 뛰어나게 성능이 좋은 편이 아니지만, 군형이 잘 잡혀있기 때문에 RDB에서 많이 사용된다.

>  대부분의 데이터베이스에서는 트리의 리프 노드에만 키값을 저장하는 **B+Tree**라는 B-Tree의 수정 버전을 채택한다. 

**B+Tree의 검색 성능이 뛰어난 이유**

- 루트와 리프의 거리를 가능한 일정하게 유지하려 한다.
  -> 균형이 잘 잡혀 검색 성능이 안정적이다.
- 트리의 깊이도 대게 3~4 정도의 수준으로 일정하다.
- 데이터가 정렬 상태를 유지하므로 이분 탐색을 통해 검색 비용을 크게 줄일 수 있다.
- 집약하는 함수 등에서 요구되는 정렬을 하지 않은 채 넘어갈 수 있다.



### 기타 인덱스

**비트맵 인덱스**

- 데이터를 비트 플래그로 변환해서 저장하는 형태
- 카디널리티가 낮은 필드에 대해 효과를 발휘한다.
- 갱신 시 오버헤드가 너무 크기 때문에 빈번한 갱신이 일어나지 않을 때 사용

**해시 인덱스**

- 키를 해시 분산해 등가 검색을 고속으로 실행하고자 만들어진 인덱스
- 등가 검색 외에는 효과가 거의 없고 범위 검색을 할 수 없다.
  -> 거의 사용되지 않고, 지원하는 구현도 일부에 불과하다.

## 인덱스를 잘 활용하려면

#### 1. 카디널리티와 선택률

- 어떤 필드에 대해 인덱스를 작성할 것인지 기준이 되는 요소
- 카디널리티가 높고 선택률이 낮을수록 좋은 성능을 가진다.
- 카디널리티가 가장 높은 필드는 모든 레코드에 다른 값이 들어가있는 유일키 필드이다. 
  - **카디널리티가 높다 : 중복도가 낮다 **
- 선택률 : 특정 필드값을 지정했을 때 테이블 전체에서 몇 개의 레코드가 선택되는지

> 최근 DBMS는 대체로 5~10% 이하의 선택률을 가지는 경우 인덱스를 사용하는 것을 권장하고 있다. 
>
> 이 선택률을 저장소 성능 향상과 반비례 한다.

#### 클러스터링 팩터 ( clustering factor )

인덱스의 성능을 결정하는 요인

- 저장소에 같은 값이 어느 정도 물리적으로 뭉쳐 존재하는지를 나타내는 지표
- 높을수록 분산되어 있고, 낮을수록 뭉쳐있다.
  -> 인덱스로 접근할 때 특정 값에만 접근하는 경우가 많아 **클러스터링 팩터가 낮을수록 접근할 데이터양이 적어 좋다.**



### 인덱스 사용 여부 판단

1. 카디널리티가 높을 것
2. 선택률이 낮을 것 ( 대체로 5 ~ 10 % ) -> **5% 미만**이면 인덱스를 작성할 가치가 있다.
   선택률이 10%보다 높으면 테이블 풀 스캔을 하는 편이 더 빠를 가능성이 크다.



> 선택률이 낮다는 것은 한 번의 선택으로 레코드가 조금만 선택된다는 뜻이다.
>
> 선택률이 높다는 것은, 한 번의 선택으로 레코드가 많이 선택된다는 뜻이다.

## 인덱스로 성능 향상이 어려운 경우

#### 1. 압축 조건이 존재하지 않을 경우

```sql
SELECT order_id, receive_date
	FROM Orders;
```

실행 계획을 보지 않아도 테이블 풀 스캔이다.
레코드를 압축하는 WHERE 구가 없으므로 인덱스로 작성할만한 필드도 존재하지 않는다.

#### 2. 레코드를 제대로 압축하지 못하는 경우

```sql
SELECT order_id, receive_date
	FROM Orders
  WHERE process_flg = '5';
```

process_flg의 분포

- 1 : 200만 건
- 2 : 500만 건
- 3 : 500만 건
- 4 : 500만 건
- 5 : 8,300만 건

선택률이 83%로 굉장히 높은 수치이다.

> 인덱스가 제대로 작동하려면 '레코드를 크게 압축할 수 있는 검색 조건'이 있어야 한다.
>
> flag 같은 필드는 특정 종류만을 지정하는 경우가 많으므로 인덱스로 만들기에 적절하지 않은 필드이다.

#### 3. 입력 매개변수에 따라 선택률이 변동하는 경우1

ex) 기간의 범위 검색

```sql
SELECT order_id
	FROM Orders
WHERE receive_date BETWEEN :start_date AND :end_date;
```

* start_date와 end_date 모두 외부에서 매개변수로 받는 값이다.
* 범위를 1개월, 3개월, 6개월, 1년 등 바꿔 선택하면 레코드의 선택률이 높거나 낮아진다

#### 입력 매개변수에 따라 선택률이 변동하는 경우 2

주문받은 점포를 검색 기준으로 입력할 경우

점포별 주문 수를 세는 SELECT 구문

```sql
SELECT COUNT(*)
	FROM Orders
	WHERE shop_id = :sid
```

* 큰 점포일수록 주문을 많이 받는다.

* sid가 큰 점포라면 수천만건, sid가 작은 점포라면 십만건이라고 가정
* 전자의 경우 선택률 10% 후자의 경우 선택률 0.01%
* 따라서 전자의 경우는 인덱스보다 테이블 풀 스캔이 낫다
* 후자만 생각하면 인덱스 스캔이 나을 것이다.

이 때, 무서운 점은 shop_ip 필드에 인덱스가 존재하고, 전자의 경우 인덱스가 사용된다면 오히려 성능 악화를 일으키게 된다는 사실이다.

입력 매개변수에 따라 0.01% ~ 10%의 선택률 차이가 난다고 가정했을 경우 성능 향상을 기대하기 어렵다.

10%일 경우 테이블 풀 스캔을 하고, 0.01%일 경우에는 인덱스 스캔을 하면 좋겠지만, 옵티마이저는 그러지 못한다.

> 선택률이 낮다는 것은 한 번의 선택으로 레코드가 조금만 선택된다는 뜻이다.
>
> 선택률이 높다는 것은, 한 번의 선택으로 레코드가 많이 선택된다는 뜻이다.
>
> 선택률이 10%보다 높으면 테이블 풀 스캔을 하는 편이 더 빠를 가능성이 크다.



## 3. 인덱스를 사용하지 않는 검색 조건

압축할 검색 조건이 있지만, 인덱스를 사용할 수 없는 타입일 경우

- LIKE 연산자에서 중간 일치(%내용%) 이나 후방 일치(%내용)를 사용하는 경우
- 인덱스 필드로 **연산하는 경우** (ex: `WHERE col_1 * 1.1 > 100`)
  - `WHERE col_1 > 100 * 1.1` 이처럼 작성하면 인덱스를 사용할 수 있다.

#### 중간 일치, 후방 일치의 LIKE 연산자

```sql
SELECT order_id
	FROM Orders
  WHERE shop_name LIKE '%대공원%';
```

선택률은 0.005%로 가정한다.
이 경우에 shop_name 필드에 인덱스를 작성하면 효율적인 검색이 가능할까?
LIKE 연산자를 사용하는 경우 인덱스는 **전방 일치**('대공원%')에만 적용할 수 있다.

#### 색인 필드

색인 필드로 연산하는 경우 인덱스를 사용할 수 없다.

```sql
SELECT *
	FROM SomeTable
  WHERE col_1 * 1.1 > 100;
```

그렇지만, 검색 조건의 우변에 식을 사용할 때는 인덱스가 사용된다.

* 이렇게 쓰면 인덱스는 사용된다.

```sql
WHERE col_1 > 100/1.1
```

- IS NULL을 사용하는 경우
- 색인 필드에 함수를 사용하는 경우
  - `LENGTH(name) = 10`
- 부정형을 사용하는 경우 - <>, !=, NOT IN 모두 인덱스를 사용할 수 없다.
  - `WHERE age <> 100;`

## 인덱스를 사용할 수 없는 경우 대처법

### 1. 외부 설정으로 처리

#### UI 설계로 처리

- 점포 ID로 검색하면 반드시 주문일도 함께 입력해야하는 등의 입력제한을 둔다.

특정 기간의 무언가를 조회하고 싶을 때 기간 검색의 최대치를 정해 두는 식으로 제한을 둘 수 있다.

이러한 입력 제한을 만들 때는 사용자 혹은 엔지니어와 충분한 소통이 필요하다.

### 2. 외부 설정을 사용한 대처 방법의 주의점

이러한 제한이 많고, 과도한 경우 협업을 진행하는 다른 직원을 피로하게 하거나 사용자의 만족도를 감소시킬 수 있다.

성능과 사용성의 트레이드오프를 통해 타협점을 찾아야 한다.

### 3.데이터 마트로 대처

데이터 마트 혹은 개요 테이블(Summary Table)이라고도 한다. 

특정한 쿼리(군)에서 필요한 데이터만을 저장하는 상대적으로 작은 테이블을 의미한다.

* 원래 테이블의 부분 집합(또는 서브셋)

이는 필요한 필드의 정보만 불러오기 때문에 I/O 비용을 줄일 수 있다.

![image-20230211031749557](/Users/ysk/study/study_repo/SQL, Database/postgresql/images//image-20230211031749557.png)

접근 대상 테이블의 크기를 작게 해서 I/O 양을 줄이는 것이 데이터 마트의 목적이다 

### 4.  데이터 마트로 대처 시 주의점

- **데이터 신선도** - 데이터 동기시점의 문제
  - 주기적으로 원래 테이블과 동기화를 해야한다. 적절한 성능과 신선도의 트레이드오프가 필요하다.
  - 이 시점이 짧으면 데이터의 신선도가 높지만 성능이 감소한다.
  - 이 시점이 길면 데이터의 신선도가 낮지만 성능이 상대적으로 증가한다.
- **데이터 마트 크기** - 데이터마트의 목적 : 원래 테이블의 크기를 작게 해 I/O양을 줄이는것 
  - 테이블 크기를 줄일 수 없거나, 검색 조건이 압축되지 못하는 경우 성능적인 개선이 불가능하다.
  - GROUP BY 절을 미리 사용하여 집계를 마치고 데이터 마트를 만들면 필드 수와 레코드 수를 크게 줄일 수 있다.
  - GROUP BY에 필요한 정렬 혹은 해시 처리도 사전에 끝내므로 굉장히 효과적이다.
- **데이터 마트 수**
  - 데이터 마트는 애시당초 기능 요건에 의해 만들어지는 엔티티가 아니므로 제대로 관리하기가 어렵다.
  - 데이터 마트 수가 많아질수록 쓸데없이 리소스를 낭비하는 '좀비 마트'가 생길 확률이 높아진다.
- 배치 윈도우
  - 데이터 마트를 만드는 데도 시간이 걸리므로 배치 윈도우를 압박한다.
  - 이러한 처리를 수행하기 위한 배치 윈도우와 JoB Net도 고려해야 한다.

### 5. 인덱스 온리 스캔으로 대처

인덱스 온리 스캔은 SQL 구문에서 필요한 필드를 인덱스로 커버할 수 있는 경우에 테이블 접근을 생략하는 기술이다.

접근하려는 대상의 I/O 감소를 목적으로 한다는 점에서는 데이터 마트와 같다.
인덱스를 사용한 고속화 방법이지만, 기존 인덱스와는 사용 방법이 많이 다르다.

* 데이터 마트에서의 데이터 동기화 문제를 해결할 수 있다.

**커버링 인덱스**

```sql
CREATE INDEX CoveringIndex ON Orders (order_id, receive_date);
```

order_id, receive_date는 SELECT 구문에 포함되어 있으므로, 일반적으로 인덱스의 필드 후보로 되지 않는다.

그러나, 2개의 필드를 커버하는 인덱스가 존재하면, 테이블이 아닌 인덱스만을 스캔 대상으로 하는 검색을 사용할 수 있다. 

* -> INDEX ONLY SCAN

* -> 이러한 인덱스를 커버링 인덱스(Covering Index) 라 한다.

> 실행 계획에서의 INDEX FAST FULL SCAN이 나온다.
>
> 실행 계획에서도 테이블에 접근하지 않아서 테이블 이름이 나오지 않는다.

**장점**

- 인덱스는 테이블 필드의 부분 집합만 저장하므로 원래 테이블에 비해 굉장히 작다.
- 데이터 마트를 만들 시 애플리케이션도 수정해야 하지만, 인덱스를 사용할 경우 그럴 필요가 없다.

> 로우(레코드) 지향 저장소의 DBMS에 유사적으로 컬럼 기반 데이터베이스를 실현하는 것
>
> * 컬럼 기반 데이터베이스란, 데이터 저장 단위를 로우가 아닌 컬럼으로 하여 불필요한 필드를 읽지 않도록 만든 방법이다.
> * 일부 필드만 사용하는 구문에서는 빠르지만 * 같은 다중 컬럼을 조회하면 속도가 매우 느려진다.



### 6. 인덱스 온리 스캔으로 대처 시 주의사항

- DBMS에 따라 사용할 수 없는 경우도 있다.
  - Oracle, SQL Server, PostgreSQL, MySQL 모두 지원하지만 오래된 버전인 경우 지원하지 않을 수도 있다.
- 한 개의 인덱스에 포함할 수 있는 필드 수에 제한이 있다.
  - 애초에 인덱스가 너무 커지면 물리 I/O를 줄이겠다는 목적이 희미해지므로 인덱스 온리 스캔을 사용할 이유가 희미해진다.
- 갱신 오버 헤드가 커진다.
  - 검색 속도와 갱신 성능의 트레이드오프
- 정기적인 인덱스 리빌드 필요
  - 인덱스에만 접근한다는 것은 검색 성능 자체가 인덱스의 크기에 의존한다는 것이다.
  - 따라서 인덱스의 크기에 민감하게 반응하므로 정기적인 모니터링과 리빌드의 대상이 된다.
- SQL 구문에 새로운 필드가 추가된다면 사용할 수 없다.
  - WHERE 구문 변경이 곧 급격한 성능 변화로 이어질 수 있다.
  - SQL 구문에서 사용하는 필드를 모두 커버할 수 없게 된 시점에서 더이상 커버링 인덱스가 아니다.
  - 이러한 점에서 일반적인 인덱스보다 더 유지 보수에 약한 튜닝이다.



> 인덱스 온리 스캔은 일반적인 range 스캔에서는 고속화가 어려운 경우에서도 사용할 수 있다는 장점이 있지만, 주의할 점도 많은 변칙적인 기술이다. 