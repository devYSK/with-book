# 컴퓨터 밑바닥의 비밀

[toc]

# 1장 프로그래밍 언어부터 프로그램 실행까지, 이렇게 진행된다

## 1.1 여러분이 프로그래밍 언어를 발명한다면?

CPU

* CPU는 매우 원시적이다. 데이터를 한곳에서 다른곳으로 옮기고 간단한 연산 후 다시 데이터를 자리로 옮기는 일만 한다.
* 그러나 매우 엄청나게 빠르다. 이 연산은 인간이 따라가지 못한다. 

CPU는 1과 0밖에 읽지 못한다. 1과 0으로만 이루어져 있어 사람들은 읽을 수 없다.

이를 기계어 라고 하는데 이 기계어와 CPU가 하는 명령어들을 인간이 읽을 수 있도록 대응한것이 어셈블리어다

* x86: `MOV`, `ADD`, `CMP`, `JMP`
* ARM: `LDR`, `STR`, `ADD`, `B`

```assembly
section .data         ; 데이터 섹션
    msg db 'Hello, World!', 0   ; 문자열 저장 (0은 종료 문자)

section .text         ; 코드 섹션
    global _start      ; 엔트리 포인트 선언

_start:               ; 프로그램 시작
    mov edx, 13        ; 출력할 문자열 길이
    mov ecx, msg       ; 출력할 문자열 주소
    mov ebx, 1         ; 파일 디스크립터 (1 = 표준 출력)
    mov eax, 4         ; 시스템 호출 번호 (sys_write)
    int 0x80           ; 인터럽트 호출
```

이 어셈블리어를 어셈블러라는 소프트웨어가 기계어인 0과 1로 번역한다. (기계어는 보통 16진수)

그러나 어셈블리어는 기계어와 마찬가지로 여전히 매우 저수준언어이며, 저수준 언어는 모든 세부사항에 신경을 써야한다. 

* 데이터를 한곳에서 다른곳으로 이동, 다시 연산, 

물 한잔 주세요를 기계어로 번역하면

* 오른쪽 다리 앞으로
* 멈춤
* 왼쪽 다리 이동.. 반복
* 물컵 찾고 ~~

인간의 물 한잔 주세요라는 추상적인 언어는 기계가 이해하기에 너무 어렵다. 

특정 규칙에는 어떤 행동 을 행하기 위해 if else가 나왔고

반복을 위한 while이 나왔다. 

반복되는 명령어들은 세부사항만 차이가 있을뿐 동일하므로 parameter라는 개념을 도입하여 세부사항을 외부에서 주고 함수를 만들었다. 

```
if ~~
  blablab
else 
   blablab
   
while
   blablab
```

blablab는 문장이, if else가, while도, 함수 호출도 될 수 있다.

즉 이말은 모든 코드 자체가 재귀적으로 동작할 수 있다. 

세상의 모든 코드는 재귀적이며 결과적으로 구문(syntax)로 귀결된다. 이것을 이용해 프로그래밍 언어가 나오게 된다. 

프로그래밍 언어를 컴퓨터가 인식할 수 있는 기계어로 어떻게 바꿀까?

* tree를 생각하면 된다. 

<img src="./images//image-20241227012349151.png" width = 550 height = 300>

구문 안에 if, bool, 구문이 다시 중첩되고 재귀적으로 반복된다.

코드는 트리형태로 표현할 수 있으며 가장 마지막 리프 노드들을 보면 매우 간단해서 기계어로 번역할 수 있다.

리프 노드를 기계 명령어로 번역하고 그 결과를 리프 노드의 부모 노드로.. 재귀적으로 타고 올라가다보면 전체 트리를 기계 명령어로 번역할 수 있다. 

이것을 담당하는 프로그램이 컴파일러이다. 



다른 문제가 있다.

세상에 CPU는 여러 종류가 있고, 각 CPU마다 기계어를 해석하는 방식이 다르며 각각의 고유 언어가 있다.

이 문제를 해결하기 위해 Acpu가 Bcpu를 CPU 시뮬레이션 하는것이 나오게 되었고 이것이 Vitual machine이다.

표준 공용어를 두고 각각의 CPU 형식마다 가상 머신이 이 명령어들을 자신의 CPU에 맞는 방식으로 해석한다. 

이것들을 요즘의 자바, C++ 같은 고급 언어라고 하며 이 구문 트리를 기계어로 번역하거나 바이트 코드로 변환 후 가상머신으로 넘겨 가상머신에서 실행한다. 

## 1.2장 컴파일러는 어떻게 작동하는 것일까?

사람이 읽을 수 있는 고급 프로그래밍 언어를 컴퓨터가 이해할 수 있는 저수준 언어로 변환하는것이 컴파일러

컴파일러는 각 코드를 쪼갠 후 각 항목이 가지고 있는 추가 정보를 함께 묶어서 관리한다.

``` 
int a = 1;
int b = 2;
```

```
T_Keyword int
T_Identifier a
T_Assign =
T_Int 1
```

각각의 줄은 하나의 토큰이며 T로 시작하는 왼쪽 열은 토큰 의미, 오른쪽은 값이다 

컴파일러는 첫 작업으로 소스 코드를 돌아다니면서 모든 토큰을 찾아 추출하고 정리한다. 

이 소스코드에서 토큰을 추출하는 과정이 lexical analysis라고 한다.



다음으로 구문에 따라 토큰을 처리해야 한다

```
while (표현식) {
  반복 내용 
}
```

while 키워드의 토큰을 찾으면 다음 토큰이 (라는것을 기대하는데, 이때 기대하는값이 아니면 syntax error를 보고한다. 이런식으로 해석(파싱)을하며 빠지지않고 다 검사하게 된다. 

이 구문에 따라 해석해 낸 구조는 트리로 표현되며 이것이 구문 트리이고 트리를 생성하는 과정을 구문 분석이라 한다.

구문을 검사하고 구문이 문제가 없다면 중간 코드를 생성한다. 

```
a = 1
b = 2
goto b
```

* 중간 코드에 최적화를 진행하는 경우도 있다. 미리 계산이 가능하다던가 등

중간 코드를 생성한 이후 어셈블리어 코드로 변환하고, 컴파일러는 이 어셈블리 코드를 기계어로 변환한다.

이게 다가 아니다, 모든 소스파일은 여러개가 있다. 이 대상 파일들을 하나로 합쳐주는 작업이 필요하다

이 작업을 link라고 하며, link를 담당하는 프로그램을 linker라고 한다

## 링커의 말할수 없는 비밀

프로그래머라면 완성된 외부 라이브러리를 가져와 사용한다. 외부 코드 덩어리는 정적, 동적 라이브러리로 제공된다.

이 링커가 이것을 도와준다

링커가 하는 일은, 여러 흩어져 있는 일을 하나로 묶어준다.

주요 역할:

1. **심벌 해석:** 참조된 함수나 변수가 실제로 존재하는지 확인.
2. **재배치:** 참조된 함수나 변수의 실제 메모리 주소를 설정.
3. **실행 파일 생성:** 모든 파일을 합쳐 완전한 실행 파일 생성.

이 과정에서 링커는

- **심벌 해석(Symbol Resolution):** 참조된 함수나 변수가 실제로 존재하는지 확인.
- **재배치(Relocation):** 참조된 함수나 변수의 정확한 메모리 주소를 설정.

예를 들어, `main.c`에서 `print()` 함수를 호출한다고 가정하면, 링커는 `print()` 함수가 `print.c` 파일에 실제로 정의되어 있는지 확인하고 이를 연결한다. 

* 컴파일러는 `main.c`에서 `print()` 함수가 존재한다고 가정하지만, 실제 구현이 `print.c`에 있다. 

`main.c`에서 `print()` 함수 호출 시, `print()`의 위치를 `0x0000`으로 임시 표시하면 링커는 `print()` 함수의 실제 메모리 주소를 확인하고 `0x1234`로 수정한다. 



심벌은 무엇일까?

* 전역 변수와 함수 이름을 포함하는 변수 이름 
* 지역 변수는 외부 모듈에서 어차피 참조 불가능해서 링커의 관심 대상이 아님

링커는 소스 파일에 다른 모듈에서 참조할 수 있는 심벌이 있다는 것과 참조한다는 두가지 정보를 알 고 있어야한다.

이 정보는 컴파일러가 알려준다

컴파일러는 명령어 부분을 코드 영역에 저장하고, 데이터 부분(전역 변수, 외부 변수)등을 데이터 영역에 저장한다. 

그리고 소스 파일마다 외부에서 참조 가능한 심벌들이 어떤것인지 정보를 심벌 테이블에 기록하고 링커에게 알려준다.

심벌 해석이 끝나게 되면 실행 파일을 생성한다.



### 정적 라이브러리, 동적 라이브러리, 실행파일

C언어에서 코드를 별도로 컴파일 한 후, 패키지로 묶고 모든 함수의 선언을 포함하는 헤더파일을 제공하는것을 정적 라이브러리 라고 한다. 이때 미리 컴파일하고 제공하기 때문에 실행파일 생성시 작성한 코드만 컴파일하며, 미리 컴파일되어있는 정적 라이브러리는 다시 컴파일 하지 않고 그대로 링크과정에서 실행파일에 복제된다.

그래서 속도가 빠르며 이과정을 정적 링크라고 한다. 

그러나 정적 링크는 라이브러리를 실행 파일에 직접 복사하기 때문에, 작성한 코드가 여러개라면 생성된 실행 파일이 그만큼 복제되어 낭비된다. 즉 라이브러리 크기가 2MB고 실행 파일이 500개라면 1GB가 중복된다. 

* 링커는 각 실행 파일이 필요로 하는 정적 라이브러리의 코드를 실행 파일 내부에 포함시키며, 정적 라이브러리는 실행 파일이 독립적으로 동작할 수 있도록 필요한 코드를 모두 넣기 때문임. 
* 즉 장점은 독립성과 호환성이지만 단점은 중복이 되는것 

이 문제를 동적 라이브러리를 사용하여 해결한다. 

<img src="./images//image-20241227015737957.png" width = 350 height = 350>

동적 라이브러리는 실행 파일이 라이브러리 코드를 포함하지 않고 실행 시 공유 라이브러리를 참조한다.

참조된 동적 라이브러리 이름, 심벌 테이블, 등 필수 정보만 실행 파일에 포함 시키기 때문에 실행 파일의 크기를 줄일 수 있다.

<img src="./images//image-20241227020100834.png" width = 350 height = 350>

* 윈도우에서 exe 실행시 정적 라이브러리는 exe에 포함되는데, 동적 라이브러리는 dll로 exe 빌드시 포함되지 않고 실행 시점에 dll을 로드하여 사용하는것.

> 왜 이렇게 되었을까?
>
> 컴퓨터 초창기에는 시스템간 호환성 동적 연결한 기술이 모자를뿐더러 실행파일이 독립적으로 동작해야 했기 때문. 
>
> 그러나 점점 발전하는데, 메모리와 디스크도 여전히 제한적이지만 동일 코드를 복제하는것은 큰 낭비이므로 동적 연결 개념이 등장하게 된것. 

동적 라이브러리에 의존하는 프로그램은 동적 링크(링킹)을 프로그램 실행시점까지 미룬다 

두가지 방식이 있는데

1. 프로그램이 메모리에 로딩될떄 동적 링크 진행 (로더라는 전용 프로세스 실행)
2. 런타임에 코드가 동적 링크를 직접 실행해서 필요할떄 사용 -> 런타임 동적 링크 



동적 라이브러리의 장점은

효율성과 유지보수, 즉 라이브러리만 교체하면 되며 플러그인 처럼 확장도 쉬워지게 된다. 

파이썬은 느린데, 빠르게 쓰고싶다면 높은 성능이 요구되는 부분을 C C++로 작성하고 컴파일하여 동적 라이브러리 만든 후 링킹하는것이다. 

* 자바의 native도 네이티브 c, c++로 작성된 네이티브 라이브러리를 사용

그러나 단점은 약간 성능이 떨어질 수 있단것이다. 



### 재배치 : 심벌의 실행시 주소 결정하기 - 링커

모든 변수나 함수는 메모리에 할당되며 메모리 주소가 있다.

링커가 실행 파일 생성시 함수가 적재될 메모리주소를 확정해야 하는데  어떻게 알 수 있을까?

링커는 프로그램에서 호출되거나 참조되는 모든 심벌이 실제로 **어디에 정의되었는지** 확인하는데

컴파일러가 남겨둔 정보를 바탕으로 심벌의 위치를 확인한 후 실행 파일에서 심벌이 위치할 실제 메모리 주소를 결정한다. 

#### **컴파일러가 남긴 단서**

- 객체 파일 구조

  - `.text`: 기계 명령어가 들어 있는 코드 영역.
  - `.data`: 전역 변수 등의 데이터 영역.
  - `.relo.text`: 재배치 정보를 기록하는 영역.
  - `.relo.text`에는 "주소를 수정해야 할 명령어 위치와 관련 심벌 이름" 정보가 저장

  * 예시 : foo함수 호출 

  - 컴파일러가  ```call foo``` 명령어를 생성하며,  ```foo```

    의 주소를 아직 모른다면:

    - `call 0x00`으로 임시 주소를 기록.
    - ```.relo.text```에 다음 정보 추가:
      - 명령어 위치: `코드 영역의 오프셋 60바이트`.
      - 심벌 이름: `foo`.

이렇게 메모리 주소를 수정하는 과정을 재배치라고 한다.

근데, 어떻게 실행 전인데 실행된 후의 변수 등의 메모리 주소를 어떻게 알 수 있을까?

*  이 문제를 가상 메모리를 이용한다

### 가상 메모리와 프로그램 메모리 구조

<img src="./images//image-20241227021416188.png" width=300 height = 300>

그림 상으로, 모든 프로그램은 실행 된 후 코드 영역이 예외없이 메모리주소 0x400000에서 시작한다. 

* 일반적으로 Linux 시스템에서 실행 파일(ELF 포맷)은 **코드 영역을 메모리 주소 0x400000**에 로드한다고 한다.

두 프로그램이 동시에 실행된다면 둘다 어떻게 같은 메모리 주소에 로드될까?

이것은 물리적으로 존재하지 않는 가짜 메모리 덕분이다. 이 가짜 메모리 기술을 가상 메모리 기술이라고 한다.

가상 메모리는 운영 체제가 제공하는 기술로, 프로그램마다 **독립적인 메모리 공간**을 할당받는 것처럼 보이게 만든다.

실제로는 여러 프로그램이 하나의 물리적 메모리(RAM)를 공유하지만, 각 프로그램은 자신의 **고유한 메모리 공간**만 사용하는 것처럼 동작한다. 

* CPU가 명령어를 실행할 때, 가상 메모리 주소를 **MMU(Memory Management Unit)**가 **물리 메모리 주소**로 변환한다.
* 운영 체제는 각 프로그램마다 **페이지 테이블**을 유지하여, 가상 주소와 물리 주소 간의 매핑 정보를 저장하는데, 프로그램이 실행될 때, 이 페이지 테이블을 참조하여 가상 주소를 물리 주소로 변환한다.

CPU가 프로그램 A를 실행하고 메모리 주소에 접근하면, 페이지 테이블을 참조하여 실 물리 메모리 주소로 변환한후 접근하게 된다. 

<img src="./images//image-20241227022252674.png" width = 650 height = 650>

특징

* 모든 프로세스의 가상 메모리는 표준화 되어있다.
* 실제 물리 메모리에는 힙, 스택 영역을 구분하진 않음. 운영체제마다 다를 순 있음
* 모든 프로세스는 자신만의 페이지 테이블을 가지며, 같은 가상 메모리 주소라서 페이지 테이블을 확인하여 서로 다른 물리 메모리 주소를 획득한다. 



가상 메모리는 현대 운영 체제의 핵심 기술로, 다음과 같은 장점을 제공

- **프로그램 간 메모리 보호**: 각 프로그램이 자신의 메모리만 접근할 수 있도록 보장.
- **효율적인 메모리 사용**: 실제 메모리를 적게 사용해도 많은 프로그램을 동시에 실행 가능.
  - 메모리 침범으로 인한 충돌과 오류 방지.
- **코드 재사용**: 동일한 실행 파일을 여러 프로세스에서 공유 가능.
- **메모리 주소의 일관성**
  - 모든 프로그램은 자신의 메모리 주소를 0부터 시작하는 것처럼 인식.
  - 개발자가 물리적 메모리 주소를 신경 쓰지 않아도 됨.



가상 메모리는 OS와 MMU, CPU의 협업으로 구현된다.

* 결국 운영체제가
  * **프로세스별 가상 주소 공간 생성**: 각 프로세스에 독립적인 메모리 공간을 할당.
  * **페이지 테이블 관리**: 가상 주소와 물리 주소 간의 매핑 정보를 저장.
  * **메모리 할당 및 해제**: 필요한 메모리를 할당하고, 사용하지 않는 메모리를 반환.
  * **페이지 교체 알고리즘 실행**: 물리 메모리가 부족할 때, 어떤 데이터를 디스크로 스왑할지 결정.

* 메모리 관리 유닛(MMU)

  * MMU**는 CPU와 메모리 사이에서 동작하는 하드웨어 모듈.
    - 가상 주소를 물리 주소로 변환.

* 이덕분에 스왑 메모리도 사용 가능 

* 단점으로는

  * **성능 저하**:

    - 가상 주소를 물리 주소로 변환하는 과정에서 CPU와 MMU의 추가 작업 발생.
    - 디스크에서 데이터를 불러오는 **페이지 폴트**가 발생하면 속도 저하.

    **복잡성 증가**:

    - 운영 체제가 페이지 테이블, 주소 변환, 스왑 공간 등을 관리해야 하므로 구현이 복잡.

    **디스크 의존성**:

    - 물리 메모리가 부족할 경우, 디스크 접근이 많아져 시스템 성능이 크게 감소(스왑 현상).

  * 이 단점들이 다 커버가 될 정도로 장점이 많다. 

## 컴퓨터 과학에서 추상화가 중요한 이유

복잡한 소프트웨어를 추상화를 통해 복잡도를 제어할 수 있다.

모듈 기반 소프트웨어 사용시 각 모듈이 API를 추상화하면 내부 구현 상관없이 추상화된 API에만 집중

고급 언어는 추상화 되어있어 저급 언어와 CPU제어 및 기계어를 신경쓰지 않아도 되고

I/O장치는 파일로 추상화 되어있어서 세부사항은 신경쓰지 않아도 되고

프로그램은 프로세스로 추상화되어 단일 CPU도 수많은 프로세스를 실행 가능하며

물리 메모리와 파일은 가상 메모리로 추상화 되어 메모리를 안심하며 쓰고

네트워크는 소켓으로 추상화되어 패킷 해석 및 네트워크 카드가 데이터를 송수신하는지 신경쓸 필요가 없다. 



그러나 과연 저수준에 대해 알지 못해도 될까? 

# 2장 프로그램이 실행되었지만, 뭐가 뭔지 하나도 모르겠다[]

## 운영체제, 프로세스, 스레드의 근본 이해하기

CPU는 스레드 프로세스 운영체제 같은 개념을 전혀 모른다.

다음 두가지만 안다.

1. 메모리에서 명령어(instruction을) 하나 dispatche한다
2. 이 명령어를 execute 한 후 1.로 돌아간다

![image-20241227150009944](./images//image-20241227150009944.png)

CPU는 program counter라는 register에 저장되어있는 명령어 주소를 가져온다. 

pc 레지스터는 cpu에서 다음에 실행할 명령어의 주소를 저장하는 레지스터이다.

**역할**:

- CPU가 명령어를 처리하는 동안, 다음에 실행할 명령어의 위치를 알려줌.
- 명령어가 실행되면, PC 레지스터는 자동으로 다음 명령어의 주소로 업데이트.

**저장되는 내용**:

- **현재 실행 중인 명령어의 다음 명령어의 주소**.
- 이는 메모리 상의 명령어 주소이며, 보통 프로그램이 로드된 위치를 기준으로 상대적이거나 절대적인 주소

PC 레지스터가 저장하는 주소는 기본적으로 1씩 자동 증가한다. 그러나 if else, 함수 호출 명령어를 만나면 순차적인 실행순서는 파괴된다. 이런 명령어 실행시 CPU는 지정한 점프할 대상 주소에 따라 PC 레지스터 값을 동적으로 변경한다.

프로그램 시작시 주소는 운영체제가 설정한다.

### CPU에서 운영 체제까지

CPU는 한번에 한가지 일만 할 수 있다. 여러 프로세스가 동시에 동작하는건, 잠깐 그 사이에 일시 중지하고 다른 프로세스를 시작해서다. CPU 전환 빈도가 매우 빠르므로 동시에 실행되는것처럼 보이는것이다. 

CPU가 어떤 기계 명령어를 실행했는지, 어디까지했는지를 Context에 저장해놓고 저장된 정보를 이용해서 실행을 재개한다. 

그리고 프로그램을 자동으로 메모리 적재, 멀티태스킹 가능, 프로세스 관리 등을 지원하는것이 운영체제이다. 

### 프로세스는 매우 훌륭하지만 아직 불편하다

```c
int main() {
  int resA = funcA();
  int resB = funcB();
  
  print(resA + resB);
  
  return 0;
}
```

* A함수 B함수 각각 3분 ,4분이 걸린다면 너무 오래걸린다
* 실행 속도를 높이는 법은 프로세스를 나누어 병렬로 처리하고 합치면 된다.

그러나 이 프로세스 간 통신은 단점이 있다.

* 프로세스 생성시 오버헤드, 프로세스마다 자체적인 주소 공간이 있으므로 이 프로세스간 통신은 복잡함.

이를 해결하기 위해 스레드라는 개념이 나오게 됌.



프로세스의 단점은 진입 함수(entry function)이 main밖에 없어 프로세스의 기계 명령을 하나의 CPU에서만 실행할 수 있다. CPU 여러개가 동일한 프로세스의 기계 명령어를 실행하게 할 방법은 없을까?

PC 레지스터가 main 함수를 가르켜 실행하듯이, PC 레지스터가 다른 어떤 함수를 가르켜 실행할 수 있으면 새로운 실행 흐름을 형성할 수 있다. 

즉 하나의 프로세스에 속한 명령어들을 CPU 여러개에서 동시에 실행할 수 있다. 

```c

int resA;
int resB;

int main() {
  resA = thread funcA().join();
  resB = thread funcB().join();
  
  print(resA + resB);
  
  return 0;
}
```

두 값을 더하는 과정에서 프로세스간 통신이 없다.

스레드 사이에는 통신이라는 개념이 없는데 다른 주소공간이 아닌 동일한 프로세스 주소 공간 내에 속해 실행되기 때문이다. 즉 스레드끼리는 자신이 속해있는 프로세스의 주소 공간을 공유하며 스레드가 훨씬 가볍고 생성속도가 빠른 이유기도 하다. 

이것은 큰 편의성을 제공하지만, 여러 스레드들이 공유 리소스에 접근할 때 오류가 발생하는 문제이기도 하다.

상호 배제와 동기화를 이용해서 명시적으로 해결해야 한다.

### 다중 스레드와 메모리 구조

CPU의 PC 레지스터에는 프로세스 또는 스레드의 명령어 주소를 저장할 수 있고 이것에 스레드의 진입 함수를 넣으면 스레드를 실행시킬 수 있다. 

* 스레드는 실행 단위이며 CPU는 스레드를 실행하기 위해 PC 레지스터를 사용 

스레드와 메모리의 관련

* 함수가 실행될때 필요한 정보 : 매개변수, 지역변수, 반환주소

이 정보들은 스택에 저장되며 모든 함수는 실행시 자신만의 runtime stack frame을 가진다.

스레드라는 개념이 존재하기 전 프로세스 내 실행 흐름은 하나만 존재, 스택도 하나만 있었다.

스레드를 사용하고 나서 프로세스에 여러 entry point가 존재할 수 있게 되었고 이것은 곧 동시에 실행 흐름이 여러개 존재할 수 있게 되었다. 

즉 프로세스의 주소 공간에 각 스레드마다 스레드를 위한 스택 영역이 별도로 있게 되는것이다. 

### 스레드 활용 예

라이프사이클 관점에서 볼때 스레드는 긴 작업, 짧은작업 두가지 유형이 있다. 

* 긴 작업 : 워드 실행
* 짧은 작업 : 네트워크, 데이터베이스 등

서버가 하나의 요청을 받으면 스레드를 하나 생성하고 처리가 완료되면 스레드를 종료하는것이 thread per request이다.

이는 대량의 작업에선 큰 단점이 있따.

* 스레드의 생성과 종료에 리소스가 듦
* 스레드마다 독립적인 스택영역이 필요한데, 이는 많은 메모리 사용량을 잡아먹게 됌
* 스레드 수가 많으면 스레드간 전환에 따른 부담이 증가 

이때문에 스레드를 생성하고 사용 후 삭제하는것이 아닌 스레드 풀 개념이 나오게 되었다.

스레드 풀의 스레드 수는 몇개가 적합할까?

* 너무 적으면 CPU 최대 활용 불가능
* 너무 많으면 리소스 낭비, 시스템 성능 저하, 컨텍스트 스위칭간 부하 발생

CPU intensive 테스크와 io intensive task로 구분하여 적합하게 설정하는것이 좋다.

## 스레드간 공유되는 프로세스 리소스

### 스레드 전용 리소스

스레드는 사실 함수 실행이며 하나의 시작점이 존재하고 이 시작점이 진입 함수다. 

CPU는 진입 함수에서 실행을 시작하여 실행 흐름을 생성하는데 이것이 스레드이다.

<img src="./images//image-20241227170237959.png" width = 350 height = 350>

스레드는 자신만 사용할 수 있는 스택 영역을 가지므로 스레드 여러개가 있을떄는 여러 스택 영역이 존재한다.

이외에도 

* 다음에 실행될 명령어 주소를 저장하는 PC 레지스터,
* 스레드 스택 영역에서 스택 상단을 가리키는 스택 포인터

등도 스레드의 현재 실행 상태에 속한다.

이 모든 정보를 thread context 라고 한다.

스레드는 스택 영역을 제외한 나머지 힙, 데이터, 코드 영역을 공유한다. 

### 코드 영역 : 모든 함수를 스레드에 배치하여 실행할 수 있다.

코드 영역은 프로그래머가 작성한 코드, 정확하게는 컴파일한 후 생성된 실행가능한 기계명령어가 저장된다.

코드 영역은 스레드 간에 공유되므로 어떤 함수든 스레드에 적재하여 실행될 수 있고 특정 함수를 특정 스레드에서만 실행하는것은 불가능하다.

코드 영역은 read-only이기때문에 어떤 스레드도 변경할 수 없어서 스레드 세이프하다.

* 데이터 영역이나 힙에서 상태를 공유하는 자원이 없는 한, 코드 실행은 스레드 세이프
* 왜 코드영역은 불변이냐? 프로그램 실행 중 코드 영역에 저장된 명령어(기계어)가 변경된다면, 예상치 못한 동작이 발생할 수 있기 때문 
* 여러 스레드 또는 프로세스가 동일한 코드를 실행하는 경우, 한쪽에서 코드가 수정되면 다른 쪽에서도 영향을 받게됌 이 문제를 원천적으로 방지. 
*  **코드 영역을 읽기 전용으로 설정하고 여러 프로세스 간 공유하면 메모리 사용량을 줄이고 캐시 효율을 높임. 
  * 매개변수는 자기 스레드만의 스택 프레임에 저장 즉 매개변수는 독립적으로 관리됌 지역변수도 마찬가지. 
  * 그러나 매개변수로 참조나 포인터가 전달되면 공유될 수 있음 



### 스레드 전용 저장소

스레드에는 각자 전용 저장소가 있음.

- ﻿﻿이 영역에 저장된 변수는 모든 스레드에서 접근할 수 있다.
- ﻿﻿모든 스레드가 동일한 변수에 접근하는 것처럼 보일 수 있지만, 사실 변수의 인스턴스는 각각의 스레드 에 속한다.. 따라서 하나의 스레드에서 변수 값을 변경해도 다른 스레드에는 반영되지 않는다.

## 스레드 세이프 코드는 어떻게 작성해야 할까

- ﻿﻿전용 리소스를 사용하는 스레드는 스레드 세이프를 달성할 수 있다.
- ﻿﻿공유 리소스를 사용하는 스레드는 다른 스레드에 영향을 주지 않도록 하는 대기 제약 조건에 맞게 공유 리소스를 사용하면 스레드 세이프를 달성할 수 있다.

공유 리소스?

함수의 지역 변수, 스레드의 스택, 스레드 전용 저장소는 스레드 전용 리소스이다.

나머지는 공유 리소스다.

* 힙 영역: 메모리의 동적 할당에 사용되는 영역으로, C/C++ 언어의 malloc 함수와 new 예약어가 요청하는 메모리는 이 영역에 할당.
*  데이터 영역: 전역 변수가 저장되는 영역.
* 코드 영역: 이 영역은 읽기 전용으로, 프로그램이 실행되는 동안은 코드를 수정할 방법이 없으므로 이부분은 신경 쓸 필요가 없다.

공유 리소스를 안전하게 사용하는법은 순서를 따르면서 각종 잠금이나 세마포어를 이용해 다른 스레드가 건드리지 못하도록 해야한다.

1. 전역 리소스 사용해야 하는 경우 스레드 로컬에 저장하여 사용할 수 있는지 확인
2. 읽기 전용으로 수정 불가능하게 한다
3. atomic 연산을 이용한다. **Atomic 연산**은 컴퓨터에서 **더 이상 나눌 수 없는, 중단될 수 없는 연산**으로 즉, **다른 스레드가 간섭할 수 없는 단위 작업**으로, 연산이 시작되면 완료될 때까지 보장된다. 
4. 뮤텍스, 락, 세마포어를 이용해 잠금을 건다

## 프로그래머는 코루틴을 어떻게 이해해야 할까

**코루틴(Coroutine)**은 프로그래밍에서 **비동기 작업을 처리하기 위한 실행 단위**로, 특정 지점에서 **실행을 일시 중단(yield)**하고, **다시 이어서 실행(resume)**할 수 있는 특성을 가진 함수 또는 프로그램 구성 요소이다.

```python
def func():
  print("a")
  // 일시 중지 및 반환
  print("b")
  // 일시 중지 및 반환
  print("c")
  // 일시 중지 및 반환
```

코루틴은 자신의 실행상태를 저장하기 때문에 일시 중지된 지점에서 다시 이어서 실행할 수 있다.

일반 함수는 반환된 후 프로세스 주소 공간의 스택 영역에 더이상 ㅓㅇ떤 함수 실행시 정보도 저장하지 않는다.

코루틴이 반환될때는 함수의 실행시 정보를 저장하는데, 코루틴이 실행이 멈추었던 지점에서 다시 실행시 이정보가 필요하기 때문

```python
def simple_coroutine():
    print("Coroutine started")
    x = yield "First yield"  # 첫 번째 일시 중단, 값을 반환
    print(f"Coroutine resumed with x = {x}")
    y = yield "Second yield"  # 두 번째 일시 중단
    print(f"Coroutine resumed with y = {y}")
    yield "Coroutine finished"

```

실행 과정

```
# 코루틴 객체 생성
coro = simple_coroutine()

# 1. 코루틴 시작
print(next(coro))  # "Coroutine started" 출력 후 "First yield" 반환

# 2. 첫 번째 재개
print(coro.send(10))  # "x = 10" 출력 후 "Second yield" 반환

# 3. 두 번째 재개
print(coro.send(20))  # "y = 20" 출력 후 "Coroutine finished" 반환
```

결과

```
Coroutine started
First yield
Coroutine resumed with x = 10
Second yield
Coroutine resumed with y = 20
Coroutine finished

```

코루틴의 상태는 **스택 프레임**과 **레지스터 정보** 등 실행 컨텍스트를 포함하며, 이 정보는 **메모리의 특정 영역**에 저장한다. 

* 스택 또는 힙 에 저장한다.
  * 현재 실행 위치(PC), 지역변수 매개변수, 콜스택(현재까지 실행 정보), 레지스터 값 등 

### 함수는 그저 코루틴의 특별한 예에 불과하다

코루틴이 일반 함수와 다른 점은 자신이 이전에 마지막으로 실행된 위치를 아는것일 뿐.

코루틴과 스레드는 비슷하다. 스레드도 일시중지 될 수 있으며, 운영체제가 먼저 실행중이던 스레드의 실행 상태를 저장했다가 다른 스레드를 실행하고 다시 기존 스레드를 실행하고 반복한다. 이는 스케줄링과 같다. 

컴퓨터 시스템은 주기적으로 타이머 인터럽트를 생성해서 현재 스레드의 일시정지 여부를 결정한다.

그러나 이 유저 모드로 작성된 코드는 타이머 인터럽트가 없기 때문에 언어의 특정한 예약어를 이용해서 어디서 일시 중지하고 리소스를 내어줄 것인지 명시적으로 지정해야 한다.

### 코루틴은 어떻게 구현될까?

코루틴은 일시중지 및 다시시작될 수 있으므로 상태 정보를 기록해야 하며 이를 기반으로 실행한다

<img src="./images//image-20241227172920728.png" width = 450 height =350>

CPU 레지스터 정보, 함수 실행시 상태 정보가 포함된다.

<img src="./images//image-20241227173014497.png" width = 450 height =350>

스택 영역은 스레드를 위한 공간이므로, 코루틴의 스택은 힙에 저장할 수 있다.

그림상으로, 실행 흐름이 일반스레드 한개 코루틴 2개가 있는게 보인다.

이론적으로 메모리 공간이 충분하다면 코루틴 수에 제한이 없으며 코루틴 간 전환이나 스케줄링은 사용자 상태에서 일어난다. 저장 또는 복구되는 정보도 훨씬 가볍기 때문에 효율성도 높다.



코루틴의 중요한 역할은, 동기 방식으로 비동기 프로그래밍을 가능하게 한단것이다.

어떻게 스레드, 코루틴을 사용할 것인가 알기 전에 콜백 함수, 동기, 비동기, 블로킹, 논블로킹 개념을 아아야 한다

## 콜백 함수 이해

콜백이 왜 필요할까? 

```
int a = 10;

void func hey() {
  ~~~ a; 
}
```

코드에서 숫자 10을 직접 사용하면 이 값을 바꾸려면 다른 코드를 전부 바꾸어야 한다.

그런데 변수에 담아 사용하면 변수만 사용하면 된다. 바꾸어야 할떄는 변수만 바꾸면 된다.

함수도 변수처럼 사용할 수 있다.

```
void func hey(func a) {
   a();
}
```

콜백 함수의 정의는 다른 함수에 인자로 전달되어, 특정 조건에서 호출되는 함수인데, 넘기는 함수만 바꾸면 어떤 함수든 특정 함수 내에서 실행될 수 있다.



그런데, hey를 호출하고 바로 리턴해야 하는데 5분 이내에 끝나야 하고, a함수가 6분이상 걸린다면?

요구사항을 어기게 된다.

a함수의 반환값이 필요가 없이 바로 끝내고 싶다면? 각 함수를 다른 스레드에서 병렬로 실행하면 된다 

### 비동기 콜백은 새로운 프로그래밍 사고방식

일반적으로 함수 호출시 사고방식은 다음과 같다

1. 함수를 호출하고 결과를 획득
2. 획득한 결과를 처리

이것이 일반적인 동기 방식이다. 

비동기 방식은 호출한 함수의 결과에 신경쓰지 않고 다른 스레드에서 실행시키는 것이다. 

### 블로킹 논블로킹 콜백

동기 콜백은 블로킹 콜백이라고도 하며, 함수 A를 호출할 때 콜백 함수를 매개변수로 전달한다고 가정하면 함수 A가 반환되기 전에 콜백 함수가 실행된다. 

비동기 콜백은 함수 A가 호출되고 바로 종료되며, 다른 스레드에서 실행되는것이다. 

비동기 콜백의 단점은 콜백 지옥에 빠질 수 있따.

```
func a() {
  func b() {
    func c() {
    /...
    } 
   }
}
```

코드가 너무 복잡해지기 때문이다.

이 비동기 콜백의 효율성 + 동기 콜백의 코드 단순성을 얻을 수 있는 방법이 코루틴이다.



동기는, 종속적, 연관, 기다림과 엮이고

비동기는 비종속적, 무관, 기다릴 필요 없는, 동시 발생 과 같은 단어와 엮인다. 

동기는 특정 함수가 다른 함수를 호출했을떄 종료를 기다리며, 비동기는 다른 함수를 호출했을떄 종료를 기다리지 않는다



비동기 호출하면 좋아보이지만, 작업이 언제 완료되었는지 어떻게 알고 어떻게 처리해야 할지 다음 두상황을 고려할 수 있다.

1. 호출자가 실행 결과를 전혀 신경쓰지 않을때
2. 호출자가 실행 결과를 알아야 할때



호출자가 실행 결과를 알 필요가 없으면 콜백 함수를 사용하면 된다

호출자가 실행 결과를 알아야하면, 호출자에게 완료를 알리는 신호나 메시지를 보내야 한다. 

동기 비동기는 호출자에 의존하냐 안하냐이며 

블로킹 논블로킹을 함수를 호출할때, 호출자의 스레드를 일시중지시킨다면 블로킹, 일시중지 시키지 않으면 논블로킹이다 

| **특징**      | **동기**                                   | **비동기**                                    |
| ------------- | ------------------------------------------ | --------------------------------------------- |
| **작업 방식** | 요청 후 작업 완료까지 대기                 | 요청 후 다른 작업 처리 가능                   |
| **블로킹**    | 요청 후 작업 완료 전까지 호출자 멈춤       | 호출자가 작업 완료 전에도 다른 작업 수행 가능 |
| **논블로킹**  | 요청 즉시 결과 반환, 작업 완료 여부는 대기 | 요청 즉시 결과 반환, 완료 시 알림으로 응답    |

이를 잘 이해하면 고성능 서버를 만들 수 있다

### 높은 동시성과 고성능을 갖춘 서버 구현

### 1. 다중 프로세스

부모 프로세스가 fork를 통해 자식 프로세스를 생성하여 요청을 처리함

장점

1. 프로그래밍 간단
2. 개별 프로세스 공간은 격리되어 특정 프로세스가 종료되더라도 다른 프로세스에는 영향이 없음
3. 다중 코어 리소스 최대 활용 가능

단점

* 프로세스간 통신 난이도 증가
* 프로세스 생성시 부담이 큼

### 2. 다중 스레드

스레드는 프로세스 주소 공간을 공유해서 별도의 통신 이 필요없다.

각 요청에 대응하는 thread-per-request를 사용 가능하다. 

다중 프로세스에 비해 다중 스레드가 유리하지만, C10K 문제에 따르면 동시 요청 수가 많을 때는 다중 스레드로 감당이 어렵다

* **C10K**는 **Concurrent 10,000 Connections**의 약자로, **한 대의 서버가 동시에 10,000개의 클라이언트 연결을 처리할 수 있는가?**

### 이벤트 순환과 이벤트 구동 

event based concurrency, event driven programming이다. 

이벤트 프로그래밍에는 두가지 요소가 필요하다

1. 이벤트 : 입출력에 관련된 것 (네트워크 요청 응답 파일 등 )
2. 이벤트 처리 함수 : 이벤트 핸들러

![image-20241227180047408](./images//image-20241227180047408.png)

이벤트가 들어오면 적합한핸들러를 찾아 이벤트를 처리함. 

이걸 계쏙 수신하고 처리하는것을 도와주는것이 event loop(이벤트 순환 )

### 이벤트 루프의 첫번째 문제 : 이벤트 소스와 입출력 다중화

리눅스와 유닉스는 모든것이 파일로 취급된다. 파일 디스크립터를 사용하여 입출력작업을 실행하며 소켓도 파일이다.

사용자 연결이 열개면 소켓도 열개가 필요하다.

운영체제한테 소켓 디스크립터 10개를 감시하고 있다가 데이터가 들어오면 프로세스한테 알려달라고 하는것이 입출력 다중화이며 epoll이다.

```
#include <stdio.h>
#include <stdlib.h>
#include <unistd.h>
#include <sys/epoll.h>
#include <fcntl.h>

#define MAX_EVENTS 10

int main() {
    int epoll_fd = epoll_create();  // 1. epoll 인스턴스 생성
    if (epoll_fd == -1) {
        perror("epoll_create");
        exit(EXIT_FAILURE);
    }

    // 2. 파일 디스크립터 등록
    int fd1 = 0; // 예: 표준 입력
    struct epoll_event ev, events[MAX_EVENTS];
    ev.events = EPOLLIN;  // 읽기 이벤트
    ev.data.fd = fd1;

    if (epoll_ctl(epoll_fd, EPOLL_CTL_ADD, fd1, &ev) == -1) {
        perror("epoll_ctl");
        exit(EXIT_FAILURE);
    }

    while (1) {
        // 3. 이벤트 감지
        int n = epoll_wait(epoll_fd, events, MAX_EVENTS, -1);
        if (n == -1) {
            perror("epoll_wait");
            exit(EXIT_FAILURE);
        }

        // 4. 이벤트 처리
        for (int i = 0; i < n; i++) {
            if (events[i].events & EPOLLIN) {
                // 읽기 이벤트 처리
                char buffer[1024];
                ssize_t count = read(events[i].data.fd, buffer, sizeof(buffer));
                if (count > 0) {
                    printf("Read: %s\n", buffer);
                } else if (count == 0) {
                    printf("Connection closed\n");
                    close(events[i].data.fd);
                }
            }
        }
    }

    close(epoll_fd);
    return 0;
}

```

**epoll**은 Linux 커널에서 제공하는 **I/O 멀티플렉싱 API**로, **대량의 파일 디스크립터(file descriptor, FD)**를 효율적으로 관리하기 위해 설계

### 두번째 문제 : 이벤트 순환과 다중 스레드

이벤트 루프는 이벤트 핸들러와 동일한 스레드에서 실행된다.

모든 요청이 단일 스레드에서 순차적으로 처리되는데 요청이 오래 처리되면 다른 핸들러는 일을 하지 못하게 된다.

![image-20241227180938540](./images//image-20241227180938540.png)

이 처리를 멀티 스레드에 맡겨 독립적인 워커 스레드에 분배하여 처리를 맡긴다. 

이런 설계 방법은 반응자 패턴이라는 이름이 붙었다

### 비동기와 콜백 함수

서버가 다른 서버로 요청을 보낼때 동기 방식으로 호출하면 리소스가 비효율적으로 사용된다

`GetUserInfo(request, response)`와 같은 RPC 호출은 **블로킹** 방식.

요청을 보낸 뒤 응답이 올 때까지 함수가 멈춤 → 스레드도 중단됨.

여러 RPC 호출이 연속되면 CPU 리소스가 비효율적으로 사용.

`GetUserInfo(request, callback)`와 같이 **비동기 호출**로 전환.

요청 후 즉시 반환 → 스레드는 다른 작업을 수행 가능.

응답이 도착하면 **콜백 함수**를 호출하여 결과 처리.

CPU 리소스를 효율적으로 활용.

스레드가 응답 대기 시간 동안 다른 작업 수행.

```
void handler_after_GetStorkInfo(response) {
    // 서버 C 응답 후 작업
    H;
}

void handler_after_GetQueryInfo(response) {
    // 서버 B 응답 후 작업
    F;
    // 서버 C에 요청 (콜백: handler_after_GetStorkInfo)
    GetStorkInfo(request, handler_after_GetStorkInfo);
}

void handler_after_GetUserInfo(response) {
    // 서버 A 응답 후 작업
    C;
    D;
    // 서버 B에 요청 (콜백: handler_after_GetQueryInfo)
    GetQueryInfo(request, handler_after_GetQueryInfo);
}

void handler(request) {
    // 초기 작업
    A;
    B;
    // 서버 A에 요청 (콜백: handler_after_GetUserInfo)
    GetUserInfo(request, handler_after_GetUserInfo);
}

```

그런데 이렇게 관리하다 보면 콜백안에 콜백이 포함되어 이해하기 어려운 코드가 된다.

이걸 해결하기 위해 코루틴이 나온다.

### 코루틴 : 동기방식의 비동기 프로그래밍

이전 코루틴을 이야기 할떄 yield로 CPU 제어권을 반환할 수 있다고 하였다.

코루틴이 일시 중지되더라도 작업자 스레드가 블로킹 되지 않는다.

코루틴이 일시 중지되면 워커 스레드는 준비 완료된 다른 코루틴을 실행하며, 다시 자기의 스케줄링 차례를 기다리고 다시 차례가 돌아오면 중지했던곳에서 실행하게 된다.

![image-20241227181547030](./images//image-20241227181547030.png)

이벤트 루프는 요청을 받은 후 각 핸들러 함수를 코루틴에 담아 스케줄링과 실행을 위해 작업자 스레드에 배포한 후 핸들러를 실행한다. 각 작업자 스레드는 블로킹 되지 않기 때문에 시스템 리소스를 효율적으로 사용한다. 

### CPU - 코루틴 - 스레드 관계

<img src="./images//image-20241227190554170.png" width = 450 height = 400>

CPU는 컴퓨터를 움직이고 스레드는 CPU를 할당받으며, 코루틴은 스레드에 따라 CPU 시간을 할당하는데 여기서 스케줄링되어 할당된다.

즉 코루틴은 2차 할당되는것이며 사용자 스레드라고 할 수 있다.

<img src="./images//image-20241227190727245.png" width = 500 height = 400> 

## 데이터, 코드, 콜백, 클로저에서 컨테이너, 가상머신까지

**클로저(Closure)**는 프로그래밍 언어에서 **함수와 그 함수가 선언된 환경(Scope)의 조합**을 의미

즉, **함수가 자신이 선언된 시점의 변수나 상태를 "기억"하고, 나중에 호출될 때도 해당 상태를 사용할 수 있는 기능**

* 함수가 종료된 이후에도 클로저는 외부 함수의 변수에 접근 가능
* 외부 함수의 변수를 저장하고 이를 유지하며 필요할때 업데이트 가능

#### **JavaScript 예제**

```javascript
function outerFunction(outerVariable) {
    return function innerFunction(innerVariable) {
        console.log(`Outer: ${outerVariable}, Inner: ${innerVariable}`);
    };
}

const closureFunc = outerFunction("Hello");
closureFunc("World"); // Outer: Hello, Inner: World
```

- 분석
  - `outerFunction`은 내부 함수 `innerFunction`을 반환.
  - `closureFunc`는 `outerFunction`의 실행 환경과 함께 캡처된 `outerVariable`을 기억.
  - `closureFunc` 호출 시에도 `outerVariable`을 사용할 수 있음.

클로저의 용도

1. **상태 유지**:
   - 클로저를 사용해 외부 변수의 상태를 유지하며, 함수 호출 간에 데이터를 저장.
2. **정보 은닉**:
   - 외부에서는 접근할 수 없는 데이터를 클로저 내부에서만 관리.
3. **콜백 함수**:
   - 클로저를 사용해 특정 상태를 가진 콜백 함수 생성.
4. **함수 공장(Function Factory)**:
   - 클로저를 사용해 설정이 다른 함수를 동적으로 생성.

### 컨테이너와 가상머신 기술

프로그램이 구성, 라이브러리, 실행환경으로 묶인 것을 컨테이너라고 함.

컨테이너는 가상화 기술로서 운영체제를 가상화 함. 

OS 계층 수준에서 소프트웨어를 가상화 함. 



# 3장 저수준 계층? 메모리라는 사물함에서부터 시작해 보자

## 메모리의 본질, 포인터와 참조

메모리는 1과 0으로 이루어진 수많은 매모리 셸로 이루어져 있고, 이는 각 1비트이며 이를 8개 묶으면 1바이트가 된다. 

1바이트마다 번호를 붙이게 되면, 모든 바이트는 메모리 내 주소를 가지게 되며 이를 memory address 라고 한다.

그러나 1바이트는 8비트이므로 최대 표현할 수 있는 수는 0~255인 266개 뿐이라서, 이걸 4개 묶어 4바이트로 표현하고 흔히 아는 integer로 표현한다. 하지만 키, 몸무게 ,신체 등과 같은 여러 정보를 표현하려면 더 많은 바이트가 필요하고 이를 조합해서 사용하는데 이것을 객체 또는 구조체 라고 한다.



8바이트 메모리가 주어졌을 때 1 + 2를 계산하고 싶다고 가정하자.  

먼저 숫자 1과 2를 메모리 저장해야 CPU가 값을 읽어 레지스터에 저장해야 연산을 수행할 수 있다.

메모리 주소 0~ 7이있을 때,  주소 6번에 1을 저장한다고 표현하면 주소 6 = 숫자 1이다.

근데 주소 6은 인간에게 익숙하지 않으므로 a = 1 이라고 표현할 수 있다. 이 a가 변수이다.

b 변수에 a 값을 저장하려면   b = a 로 표현할 수 있다.

 여기서 만약 a 변수가 구조체나 객체처럼 여러 바이트를 차지한다면?

<img src="./images//image-20250126184614622.png" width = 250>

이는 전체 8비트 중 절반 이상을 차지하며 b = a를 표현해야 한다면 어떻게 할 수 있을까? 

불필요한 데이터 복사본 필요 없이, 변수가 저장된 메모리 공간을 가리키면 된다.

변수를 사용할때의 장점은, b 변수가 실제로 어디 저장되어 있는지는 관심 없고 변수가 가리키는 값이 메모리주소 어디에 저장되어있지만 알면 된다.

### 포인터의 힘과 파괴력

포인터를 지원하지 않는 언어에서 c = a + b 라는 코드는 주소 개념이 없다. 존재한다만 알면 된다.

반면 포인터를 지원하는 c언어 등에서는 값과 메모리 주소를 모두 저장할 수 있다.

그말인즉슨 메모리같은 하드웨어를 직접 조작할 수 있따는 이야기이다. 

반면 포인터 연산에 오류가 있을 경우 프로그램 실행 상태를 직접 파괴가 가능한 위험성이 있다. 

포인터는 메모리 주소를 추상화한 것이고, 참조는 포인터를 한번 더 추상화 한것이다. 

그러나 메모리 자체도 한번 더 추상화 될 수 있다. 이것이 가상 메모리이다.

가상 메모리를 지원하는 현대 시스템에서 프로세스가 사용하는 메모리 주소는 실제 물리 메모리 주소가 아니다. 

## 프로세스는 메모리 안에서 어떤 모습을 하고 있을까?

64비트 시스템에서 메모리 내 프로세스 구조는 그림과 같다.

<img src="./images//image-20250126185346272.png" width = 350>

코드영역과 데이터 영역은 실행 파일 초기화시 생성되는 영역

힙영역은 프로세스 실행 중 동적 메모리 할당에 사용

스택 영역은 함수 호출에 사용되며 매개 변수, 반환 주소, 레지스터 정보 등을 포함한 정보들을 저장하는데 사용된다.

### 가상 메모리

위 그림에서 보면 모든 프로세스의 코드 영역이 0x4000000에서 시작한다. 서로 다른 프로세스가 둘다 0x7f64cb8을 반환하는 등 동일한 시작 주소를 반환하는데, 그럼 겹쳐서 문제되지 않을까?

이것은 문제되지 않는다. 이 메모리 주소는 가짜 주소이며, 메모리 조작이 일어나기 전에 실제 물리 메모리 주소로 변경된다. 오히려 실제 물리 메모리에서 보여주는 모습은 아래와 같다.

<img src="./images//image-20250126185629223.png" width = 250>

이렇게 동일한 크기의 조각으로 무작위로 흩어져 있어도 서로의 주소 공간을 방해하지 않는다

이것은 가상 메모리와 물리 메모리 사이의 사상(mapping)과 page table 덕분이다

### 페이지와 페이지 테이블

가상 메모리 주소와 물리 메모리 주소의 매핑이 유지되는 한 프로세스 주소 공간의 데이터가 실제 물리 메모리 어디에 저장되는지 전혀 신경 쓸 필요가 없다.

모든 가상 주소를 물리 주소에 매핑하는 대신 프로세스의 주소 공간을 동일한 크기의 조각으로 나누고, 이 조각을 page 라고 부른다. 

<img src="./images//image-20250126191339493.png" width = 400>

두 프로세스가 동일한 메모리 주소에 기록하더라도, 동일하게 보이는 메모리 주소는 가상 메모리이며 실제로는 서로 다른 물리 메모리에 저장된다. 

## 스택 영역 : 함수 호출은 어떻게 구현될까?

### 스택 프레임 및 스택 영역 : 거시적 관점

모든 함수는 실행 시 자신만의 작은 상자가 필요하다. 이 상자 안에는 함수 실행시 사용되는 여러 정보가 저장되어 있으며 스택 구조로 구성된다.

각각의 작은 상자를 stack frame 또는 call stack 이라고 한다. 

![image-20250126192140948](./images//image-20250126192140948.png)

꼭 알아야 할 점은 프로세스 스택 영역의 높은 주소가 위에 있고 스택 영역은 낮은 주소로 커진다.

즉 스택 영역이 차지하는 메모리는 함수 호출 깊이에 따라 증가하고, 함수 호출이 완료될 수록 감소한다.

### 함수 점프와 반환은 어떻게 구현될까?

함수 A가 함수 B를 호출하면, 제어권이 함수 A에서 B로 옮겨진다. 제어권이란, 실제로 CPUrㅏ 어떤 함수에 속하는 명령어를 실행하는지를 의미한다. 

제어권이 이전될때는 다음 두 가지 정보가 필요하다

* return : 어디에서 왔는지에 대한 정보
* jump : 어디로 가는지에 대한 정보.

함수 A가 B를 호출할때는

* 함수 A의 기계 명령어가 어디까지 실행되었는지(어디에서 왔는지)
* 함수 B의 첫번째 기계 명령어가 위치한 주소(어디로 가는지)

이 정보들은 스택 프레임에 저장된다.

![image-20250126194456519](./images//image-20250126194456519.png)

실행할 B 함수의 첫번째 기계 명령어 메모리 위치와,  실행이 끝나고 돌아올 위치인 A함수 내의 B함수를 실행한 다음 위치를 함수 A의 스택 프레임에 넣는다

![image-20250126234605498](./images//image-20250126234605498.png)

함수 B를 호출했기 때문에 새로운 스택 프레임이 추가되며 스택 영역이 차지하는 메모리 크기도 증가된다.

만약 함수 B에서 다시 다른 함수가 호출되면 새로운 스택 프레임이 생성되며 프로세스의 스택 영역은 또 한번 늘어난다.

이런식으로 함수 B는 마지막 기계 명령어인 ret까지 계속 실행되고, ret이 실행되면 (return) 함수 A의 스택 프레임에 저장된 반환 주소 점프하도록 전달하도록 한다. 



이렇게 함수가 다른 함수를 실행할때 제어권이 이동된다. 

### 매개변수 전달과 반환값

함수 호출시 함수 이름 외에도 매개변수와 반환값을 어떻게 전달할까?

x86-64 시스템 아키텍처의 경우 매개변수의 전달과 반환값을 가져오는 작업을 레지스터로 한다.

함수 A가 함수 B를 호출한다면 함수 A는 매개변수를 상응하는 레지스터에 저장하며, CPU가 함수 B 실행시 이 레지스터에서 매개변수 정보를 얻을 수 있다.

* 레지스터(Register)는 컴퓨터 프로세서(CPU) 내부에 있는 **고속 저장 장치**. 데이터를 임시로 저장하고 처리하는 아주 빠른 메모리 공간
* 일반적으로 CPU에는 몇 개에서 몇십 개의 레지스터만 존재

매개변수 수가 레지스터 수보다 많으면 나머지 매개변수는 스택 프레임에 직접 넣을 수 있기 때문에 새로 호출된 함수가 이전 함수의 스택 프레임에서 매개변수를 가져온다. 

<img src="./images//image-20250127001651584.png" width = 350>

* 매개변수는 원래 스택에 저장된다고 했었다.

### 지역 변수는 어디에 있을까

함수 내부의 지역 변수도 레지스터에 저장되거나 레지스터 수보다 많으면 이 변수들도 스택 프레임에 저장된다.



레지스터는 CPU 내부에 있고, 서로 다른 함수 A,B가 레지스터 사용시 지역 변수 정보를 덮어 쓰는 문제가 생기지 않을까? 

<img src="./images//image-20250127161142087.png" width = 250>

이를 방지하기 위해, 레지스터에 지역변수를 저장하기 전 먼저 레지스터에 저장되었던 초깃값을 꺼냈다가 레지스터를 사용하고 나면 그 초깃값을 스택 프레임에 저장한다.

### 큰그림

<img src="./images//image-20250127161312112.png" width = 450>

* 스택 프레임은 스택 영역에 존재하며 스택 영역은 프로세스 내에 존재한다.

아래 코드는 자기 자신을 1억번 호출한다

```c
void func(int a) {
  if (a > 100000000) {
    return;
  }
  
  int arr[100] = {0};
  func(a + 1);
}

void main() {
  func(0);
}
```

함수호출 단계가 증가함에 따라 스택 영역은 계속 메모리를 추가 사용하지만 스택 영역의 크기에는 제한이 있다.

이 제한을 초과하면 stack overflow가 발생하므로, 너무 큰 지역변수를 만들거나 함수 호출 단계가 너무 많으면 안된다.

### **매개변수가 레지스터에 저장되지 못한다는 것을 어떻게 알 수 있을까요?**

컴파일러는 함수 호출 규약(**Calling Convention**)에 따라 매개변수를 레지스터에 저장하거나 스택에 저장할지를 결정합니다. 각 아키텍처는 특정한 호출 규약을 정의하고, 이에 따라 매개변수를 저장합니다.

예를 들어:

- **x86 아키텍처 (32-bit)**: 보통 레지스터 수가 제한되어 있어, 대부분의 매개변수는 스택에 저장됩니다.
- **x86-64 아키텍처 (64-bit)**: 일반적으로 처음 몇 개의 매개변수는 레지스터(RDI, RSI, RDX 등)에 저장되고, 초과하는 매개변수는 스택에 저장됩니다.
- **ARM 아키텍처**: 매개변수는 레지스터 R0~R3에 저장되고, 초과하는 매개변수는 스택에 저장됩니다.

따라서 컴파일러는 **매개변수의 개수**와 **사용 가능한 레지스터 수**를 바탕으로 레지스터에 저장 가능한지 여부를 알 수 있습니다.

### **스택 영역의 증가와 감소는 구체적으로 어떻게 구현될까요?**

스택 영역의 증감은 프로세서의 **스택 포인터 (Stack Pointer, SP)**를 조작함으로써 이루어집니다. 매개변수가 레지스터에 모두 저장되지 못할 경우, 초과된 매개변수는 함수 호출 시 스택에 저장되며, 이 과정에서 스택 포인터가 업데이트됩니다.

구체적인 구현 과정은 다음과 같습니다:

1. 함수 호출 시:
   - 초과 매개변수를 스택에 **푸시(push)**합니다.
   - 스택 포인터가 감소합니다 (스택은 메모리 상에서 높은 주소에서 낮은 주소로 성장).
2. 함수가 종료될 때:
   - 스택에서 값을 **팝(pop)**하여 제거합니다.
   - 스택 포인터가 증가합니다.

### **이를 구현하는 책임은 누구에게 있을까요?**

이를 구현하는 책임은 주로 **컴파일러**에 있습니다. 컴파일러는 함수 호출 규약을 준수하여:

- 어떤 매개변수를 레지스터에 저장하고,
- 어떤 매개변수를 스택에 저장할지를 결정합니다.

컴파일러는 호출 규약에 따라 함수 호출 시 필요한 스택 조작 코드를 생성합니다. 이외에도 운영 체제와 CPU 아키텍처가 제공하는 규약을 준수하여 코드가 올바르게 실행되도록 보장합니다.



## 3.4 힙 영역: 메모리의 동적 할당은 어떻게 구현될까?

스택 프레임은 스택 영역 내에 구성되기 때문에 함수의 호출 단계가 증가할 때마다 스택 영역이 차지하는 메모리가 늘어난다. 함수 호출이 완료되면 기존 스택 프레임 정보는 더이상 사용되지 않으므로 메모리는 줄어든다.

개발자는 앞의 내용을 기반으로 두 가지 내용에 주의해야 한다 

1. 함수 A가 함수 B 호출시 스택 내용들은 무효화 되므로 무효화된 스택 내용에 대해서 어떤 가정도 하면 안된다. 
2. 지역변수의 life cycle은 함수와 동일하다. 함수 호출이 완료되면 스택 프레임이 무효화 되며 해당 사용된 메모리들은 다른 함수에서 가져와 사용할 수 있으므로 직접 지역 변수가 사용할 메모리의 할당과 반환 문제에는 신경 쓸 필요가 없다. 

### 힙 영역이 필요한 이유

특정 데이터가 여러 함수에 걸쳐 사용해야 한다면? 프로그래머가 해당 메모리 영역이 사용이 완료되엇다고 하기 쩐가지 유효하게 유지할라면? 이 경우에 동적 메모리 할당과 해제를 사용한다.

메모리 수명주기에는 프로그래머가 직접 제어할 수 있는 매우 큰 메모리 영역을 heap segement 라고 한다.



메모리 할당시 4가지 문제가 있다.

1. 메모리 요청시 메모리에서 적절한 크기의 여유 메모리를 찾아야 하는데 어떤 메모리가 할당되어있고 어떤 메모리가 여유 메모리인지 어떻게 알 수 있을까?
2. 메모리가 16바이트, 32바이트, 8바이트 등으로 조각화 되어있을때 어떤 여유 메모리 조각을 반환해야 할까?
3. 16바이트 메모리를 요청해야 하는데 찾은 여유 메모리 조각이 32바이트라면 할당하고 남은 메모리는 어떻게 처리할까?
4. 할당되엇던 메모리 사용이 완료되면 어떻게 처리해야 할까?

### 여유 메모리 조각 관리하기

연결리스트를 통해 메모리 사용 정보를 메모리 조각 자체에 함께 저장한다. 이 연결리스트에는 다음 노드가 어디있는지 알려주는 포인터는 없지만 메모리 사용 정보로 다음 노드 위치를 유추하는 것이 가능하다. 다음 두가 지정보만 기록하면 된다.

* 메모리 조각이 비어있는지 알려주는 flag
* 메모리 조각의 크기를 기록한 숫자

<img src="./images//image-20250127170456119.png" width = 300>

* 16/1은 16바이트가 할당
* 32/0은 여유 메모리가 32바이트가 있다
* 마지막 0/1는 tail node라는 의미

즉 전체 힙 영역을 쉽게 추적할 수 있다. 

### 어떻게 여유 메모리 조각을 선택할 것인가 : 할당 전략

4바이트 메모리 요청시 8바이트 조각과 32바이트 중 어떤것을 반환해야 할까?

#### 1. 최초 적합 방식

처음부터 탐색하다가 가장 먼저 발견된 요구사항을 만족하는 항목 반환

* 장점 : 단순

* 단점 : 메모리 단편화가 발생할 가능성이 매우 높음 

#### 2. 다음 적합 방식 (next fit)

최초 적합과 유사하지만, 처음부터 검색하는 대신 적합한 여유 메모리 조각이 마지막으로 발견된 위치에서 시작한다느점이 다름. 그러나 다음 적합 방식의 메모리 사용률은 최초 적합 방식에 미치지 못한다는 것이 연구로 밝혀짐.

#### 3. 최적 적합 방식

사용 가능한 메모리 조각을 모두 찾은 후 그중 요구 사항을 만족하면서 크기가 가장 작은 조각을 반환. 

* 장점 : 메모리를 더 자활용
* 단점: 사용가능한 모든 메모리 조각을 탐색해야 하므로 최초 적합보다 빠르진 않음

### 메모리 해제하기

메모리 해제 함수(free)에 메모리 요청시 얻은 주소를 전달하기만 하면 머리 정보 크기인 4바이트를 빼는 것으로 메모리 조각의 머리 정보를 얻어 할당을 해제한다. 

## 메모리를 할당할 때 저수준 계층에서 일어나는 일

### 커널 상태와 사용자 상태 

 이 두 모드는 컴퓨터의 **하드웨어 보호**를 위해 나왔다. 

CPU가 OS의 코드 실행시 커널 상태에 놓인다. 커널 상태에서는 CPU가 모든 기계 명령어 실행 및 모든 주소 공간에 접근할 수 있다.

반면 프로그래머가 작성한 일반적인 코드를 CPU가 실행할 때는 사용자 상태이다. 사용자 상태에서는 여러 제한을 받으며 특정 주소 공간에는 절대 접근할 수 없다.  프로세스가 잘못된 동작을 하더라도, 커널과 다른 프로세스에 영향을 주지 않도록 보호된다. 

**사용자 상태와 커널 상태의 전환**

사용자 상태에서 커널 상태로 전환되는 일반적인 과정은 다음과 같다.

1. 시스템 호출 (System Call)
   - 응용 프로그램이 하드웨어 리소스에 접근하거나 운영 체제의 기능을 사용하려고 할 때 발생한다.
   - 예: 파일 열기, 네트워크 통신, 메모리 할당 등.
2. 인터럽트 (Interrupt)
   - 하드웨어 또는 소프트웨어 이벤트가 발생하여 CPU의 제어가 커널로 넘어갈 때 발생한다.
   - 예: 키보드 입력, 타이머 인터럽트, I/O 작업 완료 등.
3. 예외 (Exception)
   - 실행 중인 프로세스에서 오류(예: 0으로 나누기, 잘못된 메모리 접근 등)가 발생했을 때 커널로 전환된다.

### 시스템 호출(System call)

파일 읽기 쓰기 네트워크 송수신 등 커널 상태에서는 호출 할 수 있지만 사용자 상태에서는 운영체제의 코드를 실행할 수 없다. 응용 프로그램이 파일, 메모리, 네트워크, 하드웨어 리소스 등과 같은 **운영 체제에서 제공하는 서비스**를 사용해야 할 때 **시스템 콜**을 통해 요청한다. 

리눅스, 윈도우, 맥 등의 시스템간 차이를 감추기 위해 언어의 라이브러리들은 저수준의 시스템콜 계층을 감추고 api를 제공한다. 

### 힙 영역의 메모리 부족시

<img src="./images//image-20250127175345428.png" width = 450>

힙 영역과 스택 사이에는 여유 공간이 있는데, 스택이 함수 호출시마다는 아래로 공간을 잡아먹고 힙은 위로 공간을 잡아먹는다.  힙 영역이 모자를때, 힙 영역의 최상단을 가르키는 brk(break) 변수를 brk 시스템 콜을 이용하여 힙 영역의 크기를 늘리거나 줄인다. 

malloc 호출시 메모리가 모자르다면, malloc이 brk를 호출하여 운영체제에 메모리를 추가 할당을 요청하게 된다

### 가상 메모리

지금까지 설명한 brk 등을 이용해 확장한 힙 영역은 사실 모두 실제 메모리가 아닌 가상 메모리 공간이다.

실제 물리 메모리는 실제 할당한 메모리가 사용되는 순간에 물리 메모리를 할당하게 된다. 

이때 가상 메모리가 아직 실제 물리 메모리와 연결되어 잇지 않으면 페이지 폴트가 발생하고 사상을 수정하고 다시 실제 물리 메모리를 할당한다.



## 왜 SSD는 메모리로 사용할 수 없을까?

최신 ssd를 보면 순차 읽기 속도가 최대 7.5Gb/s에 달한ㄷ(pcie 4.0 기준, pcie 5.0은 그 두배)

이렇게 빠른데 왜? 메모리로 사용할 수 없을까?

5세대 ddr(ddr5)메모리 최대 대역폭은 60gb/s를 훨씬 넘는다.. 

### 메모리 읽기/쓰기와 디스크 읽기/쓰기 차이

메모리의 주소 지정 단위는 바이트이며 각 바이트마다 메모리 주소가 부여되어 CPU가 이 주소를 이용하여 해당 내용에 직접 접근할 수 있다.

반면 SSD는 그렇지 않다, SSD는 조각 단위로 데이터를(페이지, 4kb처럼) 관리하며 이 조각크기는 매우 다양하다. 

다시말해, CPU가 파일의 특정 바이트에 직접 접근할 수 없게 된다. 

메모리는 바이트단위, 디스크는 조각 단위로 지정된다.



### 가상 메모리의 제한

최신 os의 메모리 관리는 가상 메모리 기반이다.

주소 지정 범위(Address Space)는 컴퓨터 시스템에서 메모리 주소를 표현할 수 있는 범위를 의미하는데, CPU가 데이터를 저장하거나 읽을 수 있는 메모리 위치를 지정하는데 사용된다

32비트 시스템은 2^32 = 42억 (4gb)

64비트는 2^64 = 16엑사바이트

* 128비트는 비현실적으로 커서.. 2^128

32비트는 4gb라서 1tb가진 ssd를 메모리로 사용 불가, 64비트는 가능하다

### ssd 수명 문제

ssd는 수명이 있다. slc, mlc, tlc, qlc 가 있는데 보통 tlc이며 tlc는 1개의 셀에 3비트를 저장하며 약 1천번~3천번 쓰기/지우기가 가능하다. 때문에 프로그램 실행시마다 썻다/지웠다 하느 특성상 ssd는 ram을 대체하기에 적합하지 않다.





# 4장 트랜지스터에서 CPU로, 이보다 더 중요한 것은 없다

## 4.1 이 작은 장난감을 CPU라고 부른다

트랜지스터 : 단자 한쪽에 전류를 흘리면 나머지 단자 두개에 전류가 흐르게 할 수도 흐르지 못하게 할 수도 있다.

### 논리곱(AND), 논리합(OR), 논리부정(NOT)

트랜지스터라는 스위치를 기초로 블록을 만들 수 있음.

* **AND (논리곱)**: 두 스위치가 **모두** 켜져야(1) 전류가 흐르고 불이 켜짐
* **OR (논리합)**: **하나라도** 켜지면(1) 전류가 흐르고 불이 켜짐
* **NOT (논리부정)**: 스위치를 **닫으면**(0) 전류가 흐르고 불이 켜짐, **열면**(1) 전류가 차단되어 불이 꺼짐

### CPU의 연산 능력은 어디서 나올까

CPU는 0과 1 2진법만 안다. 2진법 덧셈은 다음과 같다

2진수에서 덧셈을 할 때, 다음 규칙이 성립

* 0 + 0 = 0, carry(자리 올림수) 0
* 0 + 1 = 1 자리 올림수 0
* 1 + 0 = 1 자리 올림수 0 
* 1 + 1 = 0 자리 올림수 1

**결과(Sum)**: 두 입력이 다를 때 1 → **XOR 연산**

**자리 올림(Carry)**: 두 입력이 모두 1일 때 1 → **AND 연산**

#### 신기한 기억 능력

특정 연산의 출력값이 다른 연산의 입력값으로 이용될 수 있다.

**부정 논리곱(NAND) 게이트** 두 개를 서로 연결하면 **기억하는 회로**가 됨

이 회로는 **S(설정, Set)과 R(리셋, Reset) 단자**를 사용하여 정보를 저장

**동작 방식**

- **S = 0, R = 1 → 출력(A) = 1** → "1을 저장"
- **S = 1, R = 0 → 출력(A) = 0** → "0을 저장"
- **S = 1, R = 1 → 기존 상태 유지**

즉, **한 번 입력된 값이 계속 유지되며, 필요할 때만 변경 가능**함 → **기억 능력!**



<img src="./images//image-20250302141841335.png" width = 550>

* **WE(Write Enable)** 신호를 추가하면, 저장 여부를 선택 가능
* 이 방식으로 **1비트 메모리(레지스터)**를 만들 수 있음
* D 단자가 0이면 전체 회로는 0이며 그렇지 않으면 1임. 

위 회로는 1비트를 저장할 수 있으며 더 많은 비트를 저장하려면 복제하여 붙여넣기만 하면 된다.

이 조합 회로가 레지스터이다. 이런 방식을 원리로 메모리가 구성된다.

### 하드웨어의 기본 기술 : 기계 명령

CPU에게 개발자가 명령어를 알려줘야 1 + 1을 계산한다. 

CPU는 덧셈 연산의 연산 능력만 제공하고, 개발자는 피연산자를 제공한다. 

### 소프트웨어와 하드웨어 간 인터페이스 : 명령어 집합

명령어 집합(instruction set)은 CPU가 실행할 수 있는 opcode와 각 명령어에 필요한 operand를 묶은것이다.

![image-20250302142539278](./images//image-20250302142539278.png)

* 이 명령어는 16비트
* 처음 4비트는 CPU에 수행할 작업. => 기계 명령어는 2 ^4 = 16개 설계 가능 
* R6과 R2를 더한 후 R6에 기록한다. 

### 회로에는 지휘자가 필요하다.

회로는 연산 능력, 저장 능력을 갖추고 있으며 명령어를 전달해서 회로에 무엇을 해야하는지 알려줄 수 있다.

각 부분의 회로가 함께 작업할 수 있도록 조정하거나 동기화 하려면 지휘자가 필요하다

이 지휘자 역할이 바로 Cluck signal이다.

CPU의 클럭 주파수(dock rate)는 클럭 주파수는 1초 동 안 지휘봉을 몇 번 흔드는가를 의미하며, 클럭 주파수가 높을수록 CPU가 1초에 더 많은 작업 을 할 수 있음은 자명합니다

## 4.2 CPU는 유휴 상태일 때 무엇을 할까

### 프로세스 관리와 스케줄링

CPU 사용률을 보면 대부분 매우 낮으며 대부분 프로세스는 아무 작업 안하고 실행 상태로 있따. 특정 이벤트가 발생해야 CPU가 할당되고 작업을 하게 된다.

윈도우에서 System Idel Process항목이 CPU 사용률이 90%가 넘으면 해당 프로세스가 거의 모든 CPU 시간을 소모하고 있음을 의미한다.

운영체제는 내부적으로 대기열을 두고 우선수누이에 따라 스케줄러가 스케줄링 할 수 있도록 대기열에 프로세스를 넣어 실행한다.

### 대기열 상태 확인

만약 대기열이 비어 있따면, 실행해야 할 프로세스가 없고 CPU가 유휴 상태에 있따는 것을 의미한다.

```
if (queue.empty()) {
	do_someting();
}
```

매번 이렇게 체킹하면 코드가 매우 번잡해보일 수 있따.

이를 더 간단하게 해결하기 위해서는 대기열을 매번 가득 채워 스케줄러가 대기열에서 항상 실행할 수 있는 프로세스를 찾으면 된다.

이와 같이 커널 설계자는 idle worker라는 프로세스를 만들었는데, 이것이 윈도우의 system IdelProcess이다 

* Linux에서는 **PID 0**이 **CPU가 유휴 상태일 때 실행되는 idle 프로세스
* 이는 커널 내부에서 실행되며, **실제 사용자 공간에서 보이지 않는 프로세스**

실행할 프로세스가 없을때 이 유휴 프로세스를 꺼내어 실행한다, 항상 준비 완료 상태에 있으며 우선순위는 가장 낮다.

이렇게 해서 코드의 복잡도를 줄일 수 있게 된다.

### 모든것은 CPU로 돌아온다

CPU 설계자는 시스템에 유휴 상태가 존재할 가능성을 고려했기 때문에 하나의 기계 명령어를 설계했는데

이 기계 명령어가 정지를 의미하는 halt(hlt) 명령이다(x86 )

이 명령어는 CPU 내부 일부 모듈을 절전으로 배치하여 전력 소비를 줄인다. 

sleep 같은 함수랑 다르다. sleep은 해당 함수를 호출한 프로세스만 중지되고 cpu는 다른 프로세스를 실행하러 간다. 

내부적으로 아래처럼 된다고 생각하자

```
while (1) {
  while(!need_resched()) {
    cpuidle_idel_call();
  }
}
```

### 무한 순환 탈출 : 인터럽트

저렇게 무한 순환 구조인데 내부에는 break도 없고 return도 없는데 어떻게 순환을 빠져나올까?

os는 타이머 인터럽트를 통해 프로세스 스케줄링을 제어할 수 있다.

## 4.3 CPU는 숫자를 어떻게 인식할까?

컴퓨터 시스템이 2진법인 이유는 컴퓨터 저수준 계층이 각각 키고 끄는 상태만 있는 트랜지스터로 구성되있기 때문

### 숫자 0과 양의 정수

로마 숫자는 0이라는 개념이 없다. 숫자 205를 숫자 체계로 표기하면, 로마는 CCV, 아라비아는 205다

205 = 2 x 100 + 0 x 10 + 5 x 1

아라비아 숫자 체계에서는 값과 숫자의 위치가 직접적인 관계가 있는데, 이를 위치 기수법 (positional notation)이라고 한다 

* **위치 기수법**(Positional Notation)이란 **숫자의 위치에 따라 그 값이 결정되는 숫자 표기법**을 의미
* 일반적으로 사용되는 **십진법(Decimal, Base 10)**, **이진법(Binary, Base 2)**, **팔진법(Octal, Base 8)**, **십육진법(Hexadecimal, Base 16)** 등이 이에 해당

컴퓨터 2진법도 마찬가지로 위치기수법을 사용하며 숫자 5는

5 = 1 x 2^2 + 0 x 2^1 + 1 x 2 ^0

#### 부호 있는 정수

양의 정수의 표현은 간단함. k비트 = 2^k개

최상위 비트를 음수 양수 표현에 할당하면 양수 음수 표현이 모두 가능하다.

* 0은 양수 1은 음수

만약 4비트 1000이 -0을 의미하면 뭔가 이상하다. 0과 -0은 같기 때문이다. 이를 해결하기 위해1 의 보수가 나왔다.

**1의 보수(One’s Complement)**와 **2의 보수(Two’s Complement)**는 이진법에서 음수를 표현하는 두 가지 방

**1의 보수(One’s Complement)**

**방법:**
각 비트를 반전(**0 → 1, 1 → 0**)하여 구함.

**예제 (8비트 기준):**

```
  5  =  00000101 (원래 값)
 -5  =  11111010 (1의 보수)
```

그러나

*  **0이 두 가지 존재 (00000000 = +0, 11111111 = -0) → 비효율적**
*  **덧셈 연산 후 보정 필요**

이를 해결하는 

**2의 보수(Two’s Complement)**

**방법:**

1. 1의 보수를 구한 후 +1을 더함.
   - 즉, `1의 보수 + 1 = 2의 보수`

**예제 (8비트 기준):**

```
  5  =  00000101 (원래 값)
 -5  =  11111010 (1의 보수)
        +       1
       -----------------
 -5  =  11111011 (2의 보수)
```

**특징:**
✅ **0이 하나만 존재 (00000000 = +0)** → 더 효율적
✅ **덧셈 연산이 간편 (보정 없이 연산 가능)**
✅ **현대 컴퓨터에서 기본적으로 사용됨**

### CPU는 정말 숫자를 알고있을까?

컴퓨터의 가산기는 계산 과정에서 숫자가 양수인지 음수인지 신경쓰지 않는다. 의미도 이해하지 못한다.

0과 1로 구성된 01001100이라는 값을 우리 소프트웨어가 특정 색상이나 값이라는것을 해석하는것 뿐이다.

## 4.4 CPU가 IF 문을 만났을 때

리눅스에서 C언어로 크기가 10,000인 정수 배열을 만들고 배열에서 128보다 큰 모든 요소 합을 계 산시

정렬되어있다면 분기 예측 실패율이 0.02프로지만 정렬되지 않았다면 14.12%이다.

### if가 파이프라인을 만나면

CPU가 일할때, 명령어 인출, 명령어 해독, 실행, 다시 쓰기 등 이 단계가 끈임없이 교차적으로 반복하게 되어있다.

만약 분기 점프 명령어가 실행을 완료하기 전 다음 명령어는 파이프라인에 들어가 있어야 하는데, 없다면 빈 공간이 생겨 프로세서의 리소스를 100% 사용할 수 없다. 왜냐하면 어떤 분기의 명령어를 파이프라인에 넣어야 할지 모르기 때문이다.

이를 CPU는 미리 예측을 해서 처리한다.

만약 분기 예측이 틀리면, 실행중이던 잘못된 분기 명령어 전부를 무효화 하며 이것을 바로 성능 손실이 발생한다.

배열이 정렬되어 있으면 if > 256은 특정 이후부터는 매우 규칙적이다.

그렇지 않다면, 결과도 뒤죽박죽이므로 CPU는 추측하기 어렵다.

만약 높은 성능을 요구하는 코드를 작성하고 이 안에 if 문을 사용한다면, 추측 할 수 있도록 코드를 작성해야 한다.

## 4.5 코어 수와 스레드 수의 관계는 무엇일까

CPU는 하드웨어이고 스레드는 소프트웨어 개념, 실행 흐름이자 작업이다.

때문에 서로 알 필요가 없다.

스레드 사용하는 몇가지 상황인데, 일부 상황에서는 CPU 코어 수에 주의가 필요하다

### 4.5.3 다중 코어와 스레드

스레드 개념은 2003년부터 유행했다. 단일 코어의 성능을 더 끌어 올리기가 어려웄기 때문이다.

다중 프로세스는 프로세스간 통신, 컨텍스트 스위칭 등이 매우 어렵고 복잡하기 때문에 더 가벼운 스레드 개념이 유행하게 됐다.

만약 스레드가 순수하게 연산을 위하고 입출력 동기화 같은 작업이 없다면 코어당 스레드 하나가 적절하다.

다수의 입출력과 동기화가 필요하다면 스레드 수를 적당히 늘려 성능을 향상시킬 수 있다.

## 4.6 CPU 진화론 (상)

첫번째 명령어 집합 유형이자 처음으로 탄생했던 집합인 CISC가 있다.

CISC는 x86 아키텍처를 사용하는 인텔과 AMD cpu이다.

<img src="./images//image-20250302151818883.png" width = 450>

코드도 저장 구조를 차지한다. 기본적으로 폰 노이만 구조를 따르며, 핵심 사상은 '저장 개념에서 프로그램과 프로그램이 사용하는 데이터에 어떤 차이도 없이, 모두 저장 장치 안에 저장될 수 있어야 한다'

 과거 메모리는 수 kb밖에 안됐고 코드의 저장 공간을 절약해야 했기 때문에 복잡 명령어 집합을 만들었어야 했다.

그러나 명령어를 실행하는데 매우 효율적이지만 유연성이 떨어져 변경에 대응하기 어려웠다.

소프트웨어는 이와 다르게 쉽게 변경했다. 대부분의 명령에 포함된 연산을 더 간단한 명령어로 구성된 작은 프로그램을 CPU에 저장해 전용 하드웨어 회로를 설계할 필요를 없앴다. 이가 ROM이다.

단, 이것도 문제가 생겼는데 ROM도 코드이므로 버그가 생겼다. 이를 해결하기는 매우 어려웠다. 

때문에 다른 방식을 찾게 되었다.

## 4.7 CPU 진화론 (중)

시간이 흐르고 메모리 가격이 낮아지고 용량이 커졌다. 컴파일 기술도 발전을 이루어 쓸만해졌다.

직접 어셈블리어로 코드를 작성할 필요도 없어졌다.

### 복잡합을 단순함으로

파레토의 법칙인 전체 결과 80%가 원인의 20%에서 나온다는 법칙이, CPU에도 적용됌.

CPU는 약 80% 시간 동안 명령어 집합의 기계 명령어중 20%를 실행함.

복잡 명령어 집합에서 복잡한 명령어 중 일부는 자주 사용되지 않는다.

복잡 명령어 집합에 대한 반성을 바탕으로 축소 명령어 집합의 철약이 탄생했다.

1. 축소 명령어 집합의 특징 중 하나는 복잡 명령어를 제거하고 대신 간단한 명령어 여러개로 대체한다. 명령어 집합을 줄인다는 아이디어는 명령어 수가 줄어든다는 것이 아닌, 연산이 더 간단해진다는것
2. 축소 명령어 집합 사용시 컴파일러에게 더 많은 세부 사항을 알려준다
3. 축소 명령어 집합 명령어는 레지스터 내 데이터만 처리 가능하며 메모리 내 데이터는 직접 처리할 수 없다. 

### 복잡 명령어 집합과 축소 명령어 집합의 차이

#### <img src="./images//image-20250302152851501.png" width = 450>

오른쪽은 기계 명령어와 데이터를 저장하는 메모리고, 

왼쪽은 CPU이며, CPU 내부에 있는 것은 레지스터와 연산 장치인 ALU

축소 명령어 집합의 경우 연산시,

```
1. 메모리 주소 A의 데이터를 읽어 레지스터에 저장합니다.
2. 메모리 주소 B의 데이터를 읽어 레지스터에 저장합니다.
3. ALU가 레지스터 값을 이용하여 곱셈 연산을 수행합니다.
4. 곱셈 결과를 다시 메모리에 씁니다.


LOAD RA, A
LOAD RB, B
PROD RA, RB
STORE A, RA

```

* LOAD STORE만 가능하며 나머지는 CPU 내부의 레지스터만 처리가능함 

결국 명령어는 4개로 길어지지만, 컴파일러한테 맡기고 컴파일러가 구체적인 기계 명령어를 자동으로 생성한다.



결국 축소 명령어 집합 사용하는 상용 CPU들이 복잡 명령어 집합 구조를 사용하는 x86 들을 찢어 발겼다

그러나 인텔과 AMD의 엔지니어들이 이것을 반격했다.

## 4.8 CPU 진화론(하)

프로그래머는 인터페이스 개념을 알고있다. 함수의 큰 장점은, 함수의 인터페이스가 변경되지 않는 한 함수를 사용하는 코드는 변경될 필요가 없다.

이를 이용해, 인터페이스에 해당하는 명령어 집합은 바꿀 수 없지만 내부 구현 즉 명령어 실행 방식은 변경이 가능했다.

### 이길 수 없다면 함께해라 : RISC와 동일한 CISC

복잡 명령어 집합의 명령어를 CPU 내부에서 축소 명령어 집합의 간단한 명령어로 변환한다 이를 마이크로 명령어라고 한다. 컴파일러가 실행 파일 생성시 복잡 명령어 집합을 사용하지만, CPU 내부에서 명령어 실행시 축소 명령어 집합과 유사핟.

### 하이퍼 스레딩

하이퍼 스레딩 -> 하드웨어 스레드라고 하는데, 하이퍼 스레딩을 사용해서 운영체제에 환각을 심어 컴퓨터 시스템 코어 하나당 논리 CPU 코어가 여러개 있다고 인식하게 한다.

그 비밀은 하이퍼스레딩 기술이 탑재된 CPU는 한 번에 스레드 두 개에 속하는 명령어 흐름을 처리할 수 있으며, 이를 통해 CPU 코어 한 개가 CPU 코어 여러 개인 것처럼 보이게 할 수 있 다는 것.

원리는 파이프라인.

파이프라인은 항상 채워지지 않고 대부분 유휴 상태로 대기, 이를 이용해 추가 명령어 흐름을 도입해서 조금이라도 더 채워 실행하는것. 운영체제는 이게 진짜 인지 아닌지 상관없이 명령어를 전달한다.



### 오늘날

오늘날 스마트폰은 대부분 축소 명령어 집합 cpu인 ARM 프로세서를 달고 있다. 

## 4.9 CPU 스택과 함수 호출, 스레드 전환, 인터럽트 처리 통달하기

### 4.9.1. 레지스터

레지스터 : CPU에  있는 매우 빠른 캐시 메모리. 메모리와 속도가 100배정도 차이남.

레지스터가 훨씬 빠르지만 비싸기 때문에 프로세스 실행 정보를 모두 메모리에 저장하고, CPU가 사용할때만 레지스터에 데이터를 보관함.

몇가지 레지스터 데이터를 보자. 

#### 스택 포인터

스택의 가장 중요한 정보는 stack top, 이 정보는 stack bottom을 가리키는 stack pointer에 저장됌

함수 실행시, 내부에 정의된 로컬 변수와 매개변수 등을 저장하는 독립적인 메모리 공간을 스택 프레임이라고 함.

함수 호출이 깊어질수록 프레임 수도 증가하며, 완료시 반대로 줄어든다.

함수의 실행 시간 스택 정보는 명령어 주소 레지스터가 가지고 있다.

Progam counter 또는 IP, 또는 PC 레지스터 라고도 부른다.

프로그램 실행시 첫 번쨰로 실행할 기계 명령어 주소가 PC 레지스터에 저장되며, CPU는 레지스터에 저장되어 있는 주소에 따라 메모리에서 명령어를 가져와 실행한다. 

### 상태 레지스터

스택 레지스터와 명령어 주소 레지스터 외에도 status register가 있다.

올림수(carry)가 발생하거나 overflow 발생시 이 정보를 상태 레지스터에 저장한다.

그리고 커널 상태와 사용자 상태를 저장한다. 

### 상황 정보 (컨텍스트일까?)

프로그램 중지 및 재개시 필요하다.

프로그램은 엄격하게 순서대로 실행하지 않기 때문 (분기처리, 인터럽트 등 등 )

이 모두 스택을 이용해 상태 정보를 저장한다.

### 함수 호출과 실행시간 스택

모든 함수 실행시 독점적인 자신만의 저장공간을 가지고 있으며, 이 안에 함수 상태 정보를 저장하는데 이 공간을 스택 프레임이라고 한다.

### 시스템 호출과 커널 상태 스택

시스템 콜 요청시 운영체제가 내부 함수들을 호출하기 때문에 이 요청 처리 함수가 또 필요하다.

이 내부 함수 호출 실행시간 스택은 커널 상태 스택에 저장되어있다.

모든 사용자 상태 스레드는 커널 상태에 대응하는 커널 상태 스택을 가지고 있다.

시스템 콜이 호출되면 CPU는 사용자 상태에서 커널 상태로 전환되고, 사용자 상태 스레드에 대응하는 커널 상태 스레드를 찾아 기존 사용자 상태 스레드의 실행 상황 정보는 커널 상태 스택에 저장된다. 

ㅇㅣ것이 컨택스트 스위칭이다.

### 인터럽트와 인터럽트 함수 스택

키보드 입력, 마우스 입력, 네트워크 수신 등은 인터럽트 작동 방식을 통해 처리된다.

인터럽트는 CPU 실행 흐름을 끊고, 특정 인터럽트 처리 함수로 점프하며, 인터럽트가 끝나면 원래 위치로 점프한다.

인터럽트 함수의 실행 시간 스택은 2가지로 구현이 된다.

- ﻿﻿인터럽트 처리 함수에 자체적인 실행 시간 스택이 없는 경우 인터럽트 처리 함수는 커널 상태 스택을 이용하여 인터럽트 처리를 실행
- ﻿﻿인터럽트 처리 함수에 인터럽트 처리 함수(interrupt service routine) 스택, 즉 ISR 스택이라는 자 체적인 실행 시간 스택이 있는 경우가 있다. 인터럽트를 처리하는 것은 CPU라서 이때는 모든 CPU가 자신만의 인터럽트 처리 함수 스택을 가진다.

시스템 콜은 사용자 상태 프로그램이 직접 실행하지만, 인터럽트 처리는 외부 장치가 실행한다.

### 스레드 전환과 커널 상태 스택

시스템에 두 스레드 A,B가 있고 스레드 A는 현재 실행중이라고 가정, B는 일시중지 

![image-20250302161635342](./images//image-20250302161635342.png)

CPU가 다른 스레드 실행시 다음 두가지 작업을 포함한다

1. 주소 공간을 전환. 서로 다른 프로세스에 속해있을 수 있고 만약 서로 다른 프로세스라면 주소 공간은 다르다
2. 두번째 부분은 CPU를 스레드 A에서 B로 전환, 이 작업은 A의 CPU 상황 정보를 저장하고 B CPU 상황 정보를 복원한다.

모든 리눅스 스레드에는 각각에 대응하는 task_struct 구조체가 있으며 그 구조체 안에 CPU 상황 정보를 저장한다.

이렇게 서로 대응되는 커널 상태 스택에 정보를 저장하고 왔다리 갔다리 하면서 실행된다.

# 5장 작은 것으로 큰 성과 이루기, 캐시

폰 노이만 구조에서는 기계 명령어와 명령어에서 사용하는 데이터가 메모리에 저장되어야 하며, CPU가 기계 명령어를 실행할 때 먼저 명령어를 메모리에서 읽어야 한다.

또 명령어를 실행하는 과정에서 데이터를 메모리에서 읽어야 할 수도 있고, 계산 결과 저장은 메모리에 저장해야 한다.

즉 폰 노이만 구조는, CPU가 실행하는 기계 명령어와 처리하는 데이터가 모두 메모리에 저장되어 있어야 한다.

## 5.1 캐시, 어디에나 존재하는것

폰 노이만 구조는, CPU가 실행하는 기계 명령어와 처리하는 데이터가 모두 메모리에 저장되어 있어야 함.

* CPU는 최대한 레지스터에 있는 데이터만 가지고 연산하려고하는데, 레지스터에 해당 데이터들이 없으면 메모리에 접근하게됌. 그러나, 레지스터랑 메모리는 속도차이가 엄청나기 때문에 속도가 느릴 수 밖에 없음. 
  * 레지스터 1ns 이하, L1캐시 : 1~4ns, L2캐시 3~10ns, L3캐시 10~30ns 그러나 RAN은 50~100ns = 50~500배 차이 

메모리 벽(wall) 또는 폰 노이만 병목 현상이라고 함. 

### CPU와 메모리의 속도 차이 

보통 CPU 속도는 클록 속도 를 이야기 함. 3.5GHZ는 1초에 35억번 연산 수행

* 그러나 클럭 사이클당 처리할 수 있는 명령어 수 코어 수, 등 다른 요소들도 있어서 위는 그냥 일반적인 숫자 이야기

- CPU 내부에서 연산할 때는 **레지스터에 있는 데이터**를 사용하면 빠름 (**1ns 이하**).
- 하지만 메모리(RAM)에서 데이터를 불러오려면 **최소 100ns 이상 걸림**.
- CPU가 3.5GHz라면 **1클록(사이클)** 에 약 0.3ns 정도 걸리는데, RAM 접근이 100ns라면 **CPU는 300클록 이상을 그냥 기다려야 함**.

즉, **CPU 속도가 빨라도 RAM이 느리면 CPU가 메모리 접근 대기 시간 때문에 실제 성능이 떨어질 수 있음**.

CPU랑 주 상주 메모리 속도 차이가 너무 심하니, 매우 저용량의 L1, L2, L3 캐시를 둬 병목 현상을 완화 하고자 했음.

* 캐시 안의 캐시?같은개념 

* L1캐시 접근 속도 : 대략 4클럭 = 1ns, L2 : 10클럭 : 3ns, L3 : 50클럭 : 15ns 
  * 1(1초) / 35억 = 0.2857ns = 0.29ns  

CPU가 메모리 사용시 순서대로 레지스터 -> L1 -> L2 -> L3 -> 메모리 순으로 뒤지고 나서 

찾으면 다시 캐시에 저장하여 캐시에 데이터를 갱신함 

* 일반적으로 **L1 → L2 → L3 순으로 갱신됨** (CPU의 캐시 정책에 따라 다를 수 있음).
* **CPU는 먼저 L1 캐시에 데이터를 저장하고, L2/L3에도 복사할 수 있음**.

### 공짜 점심은 없다 : 캐시 갱신

CPU는 캐시에 기록하여 일을 처리하는데, 갱신시 문제가 캐시 데이터는 갱신되었지만 메모리 데이터는 아직 과거 데이터로 불일치 문제가 발생할 수 있다.

캐시를 사용하는 모든 컴퓨터가 가진 문제이다.

이 문제를 해결하는 가장 쉬운 방법은 캐시 갱신시 메모리도 함께 갱신하는 wirte-through 방법이다

그러나 이 방법도 단점이 있다. 일관성은 보장되지만 캐시 갱신 이후 메모리를 갱신해야 하는데 이때 레이턴시 때문에 느리고 멈춰있게 된다. -> 동기식 방식

이를 최적화 하는 방법은 캐시 갱신과 메모리 갱신을 비동기로 처리하는것.

메모리에 기록할 때는 캐시를 직접 갱신하지만, 갱신이 완료를 기다리지 않고 다음 명령어를 수행하고 비동기로 갱신하면 된다. (Write-back 방식)

즉

* Write-Through 방식 : CPU가 데이터를 캐시에 쓰면 즉시 RAM에도 반영. 데이터 일관성이 보장되지만 속도가 느림. 
* Write-Back 방식 (일반적으로 많이 사용됨) CPU가 데이터를 캐시에만 저장하고, RAM에는 나중에 반영. 성능이 뛰어나지만, 전원이 꺼지면 데이터 손실 위험이 있음. 
  * 즉, **CPU는 데이터를 캐시에만 저장하고 RAM 갱신은 캐시 컨트롤러라는 다른 하드웨어가 비동기적으로 수행**

### 다중 코어 캐시의 일관성

CPU가 메모리 접근 성능을 올리기 위해 캐시를 추가했지만, 코어 여러개를 사용하는 모던 시스템 상 다른 문제가 발생할 수 있음. 여러 코어가 같은 X변수를 바라볼때 중복 업데이트나 더티 연산이 발생할 수 있는것. -> 동시성 문제 

이 문제의 해결방법은, 캐시 한 개에서 갱신된 변수가 다른 CPU 코어의 캐시에도 존재하면 해당 캐시도 함께 갱신하는것. 변수 업데이트시, 자체 캐시랑 메모리만 신경쓰는것이 아니라 해당 변수가 다른 코어의 캐시에도 있는지 확인하고 있다면 갱신해야 함.

* 딱 봐도, 성능을 떨어트리는 연산임.

해결하는 대표적인 방법이 **MESI 프로토콜 (Modified, Exclusive, Shared, Invalid)**

* Modified (M, 수정됨)	이 캐시에서만 데이터를 가지고 있으며, RAM과 다름. 다른 캐시는 이 데이터를 갖지 않음.
* Exclusive (E, 독점적)	이 캐시에서만 데이터를 가지고 있으며, RAM과 같음. 다른 캐시는 이 데이터를 갖지 않음.
* Shared (S, 공유됨)	여러 캐시에서 같은 데이터를 가지고 있으며, RAM과 같음.
* Invalid (I, 무효화됨)	다른 코어가 데이터를 수정했으므로, 이 캐시의 데이터는 더 이상 유효하지 않음.

* 각 캐시에 상태를 두고  **CPU가 데이터를 변경하면, 다른 코어의 캐시 데이터를 무효화함 (Invalid 상태로 변경)** 해서 서로 알아서 동기화 하게 함

### 5.1.5 메모리를 디스크의 캐시로 활용하기

| 저장장치                 | **지연 시간 (Latency)**             | **대역폭 (Bandwidth, 속도)** | **특징**                               |
| ------------------------ | ----------------------------------- | ---------------------------- | -------------------------------------- |
| **CPU 레지스터**         | **1ns 이하**                        | **수백 GB/s**                | 가장 빠른 저장 공간                    |
| **L1 캐시**              | 1~4ns                               | 수백 GB/s                    | CPU 코어 내부                          |
| **L2 캐시**              | 3~10ns                              | 수십~수백 GB/s               | CPU 코어 근처                          |
| **L3 캐시**              | 10~30ns                             | 수십 GB/s                    | 여러 코어 공유                         |
| **RAM (DDR4, DDR5)**     | **50~100ns**                        | **25~100GB/s**               | 캐시보다 느리지만 SSD보다 수천 배 빠름 |
| **SSD (NVMe, PCIe 4.0)** | **100,000ns (100μs)**               | **2~7GB/s**                  | RAM보다 1,000배 느림                   |
| **SSD (SATA3)**          | **0.1~~0.5ms (100,000~~500,000ns)** | **500MB/s**                  | NVMe SSD보다 4~10배 느림               |
| **HDD (하드디스크)**     | **10ms (10,000,000ns)**             | **100~200MB/s**              | SSD보다 50~100배 느림                  |

디스크는 탐색(seek)를 위해 10ms 가량이 소요됌. 

* 메모리 vs 디스크 속도 차이 : 10ms=10×1,000,000ns=10,000,000ns =100,000 배
  * 1 밀리세컨드(ms) = **1,000 마이크로세컨드(μs)** = **1,000,000 나노세컨드(ns)**

파일을 읽을라면 데이터를 디스크에서 메모리오 옮겨야 CPU가 파일의 데이터를 읽을 수 있음. 

너무 느리기 때문에 OS는 일부 공간이 남아있는 메모리를 디스크 캐시로 활용하여 디스크에서 데이터 읽어 오는 일을 최소화 함. 이것이 리눅스의 페이지 캐시의 기본 원리

캐시가 추가되면 반드시 캐시 갱신 문제가 발생하므로, 대부분 입출력 라이브러리가 동기화 또는 flush(캐시비우기) 함수 제공함.

### 가상 메모리와 디스크

프로세스는 자체적인 표준 크기의 주소 공간을 가져, 물리 메모리와는 관련 없이 물리 메모리 크기를 초과하는 주소 공간을 가지고 있음. 

시스템에 N개 프로세스가 있을 때, N개가 실제 물리 메모리를 모두 사용한다면, 새로운 프로세스는 N + 1 메모리를 요청하면 이걸 어떻게 처리할 수 있을까?

일부 프로세스에서 사용하던, 자주 사용하지 않은 메모리 데이터를 디스크에 기록하고 이 데이터가 차지하던 물리 메모리를 해제하고 N + 1 프로세스한테 할당한다.

* 즉 디스크를 메모리의 창고 역할을 하게 함

### CPU는 메모리를 어떻게 읽을까

CPU가 볼 수 있는것은 모두 가상 메모리 주소. 즉 모든 연산에 사용하고 나오는 결과도 다 가상 메모리 주소임.

실제 물리 메모리 주소로 변환 해야 어디에 저장, 읽을지 알 수 있음.

* 가상 메모리 → 물리 메모리 변환 과정은 MMU(Memory Management Unit)와 운영체제가 담당

변환이 완료되면 캐시 -> 메모리 -> 디스크 순으로 접근함.

### 5.1.8 분산 저장 지원

한대의 컴퓨터로 대용량 데이터를 모두 저장할 수 없음. 그래서 분산 파일 시스템을 이용함

로컬 디스크는 원격 분산 파일 시스템에서 전송된 파일을 저장함. 

<img src="./images//image-20250309000828706.png" width = 350>

이를 사용할 때는 네트워크를 통하지 않고 로컬에 접근하여 원격의 분산 파일 시스템의 캐시로 간주 가능. 

물론 응답 속도를 높이려고 카프카처럼 원격에 있는 데이터를 메모리로 끌어올 수도 있음. 

자 이제 캐시에 대해 알았고, 각 계층에 대한 저장 용량은 반드시 다음 계층 보다 작아야 함.

이유

1. L3가 메모리보다 크면 걍 L3를 메모리로 쓰면됌
2. 메모리보다 L3, L2, L1 캐시가 상대적으로 매우 비쌈
3. 성능 문제 : 캐시가 커지면 속도도 느려짐. 
4. 지역성의 원칙 : 캐시가 효과적으로 작동하려면 Locality 원칙을 따라야 함. 계층이 작을수록 빠르게 필요한 데이터를 찾을 확률이 높아짐
   - **공간 지역성(Spatial Locality)**: 가까운 데이터가 자주 사용됨.
   - **시간 지역성(Temporal Locality)**: 최근 사용한 데이터가 다시 사용됨.
5. CPU 내부에 그렇게 큰 캐시를 넣을 공간이 없음.

## 5.2 어떻게 캐시 친화적인 프로그램을 작성할까?

캐시 적중률이 높아져야 결국 캐시를 사용하는 의미가 있다. 없어서 다른데 뒤지면 그것은 그 나름대로 손해. 바로 그냥 다른데 뒤지고말지.

이는 지역성의 원칙을 생각하면서 설게하는것이 좋다. 

### 5.2.1 지역성의 원칙

locality of reference 의 본질은 프로그램이 매우 규칙적으로 메모리에 접근함.

프로그램이 메모리 특정 부분에 접근하고 나서 해당 부분을 여러 번 참조하는 경우를 temporal locality 라고 함.

또한 프로그램이 특정 부분 참조시, 인접한 메모리도 참조할 수 있는데 이를 spatial locality라고 함. 

* 배열, 구조체(struct), 연속적인 메모리 블록에서 자주 발생

### 5.2.2 캐시 친화 프로그래밍 원칙 몇가지

c / c ++에서 malloc, new 사용하면 힙 영역의 이곳 저곳에 흩어질 가능성이 높아 공간 지역성이 별로 안좋음

메모리 풀 기술을 이용하여 커다란 메모리를 할당받고 이 안에서 다 쓰고 모자르면 또 하는것이 좋음.

* 메모리 풀 기술은 그냥 큰 객체를 구조체로 만들어 놓고, 여기서 할당받아 쓰는것을 의미하는 듯?

### 5.2.6 다차열 배열 순회

**잘 못된 코드 (지역성 활용 X)**

```c
int arr[1000][1000];

// 열 우선 방식 (캐시 비효율적)
for (int j = 0; j < 1000; j++) {
    for (int i = 0; i < 1000; i++) {
        arr[i][j] += 1;
    }
}
```

- **공간 지역성을 활용하지 못함** (캐시 미스 증가)
- **메모리 접근 성능 저하**

왜? 메모리는 행 우선으로 데이터를 저장하므로 특정 행 읽으면 해당 배열의 가로부분을 같이 읽어와 캐시처럼 사용 가능 . 반면 당연히 세로(열)로 접근하면 , 행을 건너 뛰어서  한번에 읽어오지 못하기 때문에 캐시 적중을 할 수가 없음. 



캐시 사용시, 성능 분석 도구를 이용해서 캐시 적중률이 시스템 성능의 병목이 되는지 판단해야 한다 .

| 도구                | 플랫폼            | 기능                               |
| ------------------- | ----------------- | ---------------------------------- |
| **Redis MONITOR**   | Redis             | 캐시 적중률 분석 및 키 조회율 추적 |
| **Memcached Stats** | Memcached         | 캐시 미스율 및 요청 분석           |
| **JVM VisualVM**    | Java (Heap Cache) | JVM 메모리 캐시 분석               |
| **Python cProfile** | Python            | 함수별 캐시 사용률 분석            |

캐시가 다중 스레드를 만나면 새 문제가 생긴다

## 5.3 다중 스레드 성능 방해자

### 5.3.1 캐시와 메모리 상호 작용의 기본 단위: 캐시 라인

공간 지역성 원리로, 접근해야 할 데이터만 캐시에 저장하는것보다 해당 데이터가 있는 곳의 묶음 데이터를 캐시하는것이 더 좋다.

이 묶음 데이터는 cache line 이라는 이름을 가지고 있따.

이 묶음 크기는 일반적으로 64바이트이다.

### 5.3.2 첫번째 성능 방해자 : 캐시 튕김 문제

1번 프로글매

```c++
#include <iostream>
#include <thread>
#include <atomic>

std::atomic<int> a;

void threadf() {
    for (int i = 0; i < 500000000; i++) {
        ++a;
    }
}

void run() {
    std::thread t1(threadf);
    std::thread t2(threadf);
    
    t1.join();
    t2.join();
}

```

2번 프로그램

```c++
#include <iostream>
#include <atomic>

std::atomic<int> a;

void run() {
    for (int i = 0; i < 1000000000; i++) {
        ++a;
    }
}
```

2번 프로그램이 1번 프로그램보다 일반적으로 빠름.

리눅스 perf로 분석해보면, 한 클럭 주기당 기계 명령어 실행 횟수가 1번이 훨씬 적음 => 느리다

왜 그럴까?

* 이전에 이야기한 MESI 프로토콜 때문임. (동기화)

캐시 일관성 보장 위해, 두 코어의 캐시에 전역변수 a가 모두 저장됌. 

첫번째 스레드가(코어) a 변수 연산 실행시, 두번째 코어(스레드의) 캐시의 a변수를 무효화(invalid) 처리 해야 한다.

* 여기서 첫번째 캐시 튕김 발생

이후 두번쨰 코어(c2) 연산시 캐시 무효화여서 어절 수 없이 메모리에서 a 변수 값을 읽어야 함. 마찬가지로 연산을 해야하므로 첫번째 코어(c1)의 변수를 무효화 해야 하고, 또 캐시 튕김 발생 

이렇게 끈임없이 서로 상대 캐시를 무효화 하면서 튕겨 내어서 이를 캐시 튕김 또는 핑퐁 이라고 함

때문에 여러 스레드 사이에 데이터 공유를 피할 수 있다면 가능한 판 피하자

* 물론 예제에서는 다른 이유도 있을 수 있다. atomic 변수이기 때문에
  * CAS 연산등을 통한 연산이면, 스핀락처럼 무한정 재시도하면서 맞추기 때문임. 

### 5.3.3 두번째 성능 방해자 거짓 공유 문제

예제 코드 생략. 

```c++
struct data {
  int a;
  int b;
}
```



* 구조체에 a,b 변수를 두고 첫번째 프로그램은 멀티스레드로 각각 a,b 연산, 두번째는 한 함수 내에서 순차적으로 a,b 연산

공유하지 않는 변수 a,b를 각각 업데이트 함. 

마찬가지로 첫번째 프로그램이 더 느림. 

캐시는 64바이트씩 지역성의 원리 중 공간성의 원리 때문에 한 캐시라인씩 올리기 때문에, 하나의 캐시 라인을 공유하고 있을 수 있기 때문. (아닐 수도 있음. 다만 높은 확률임. int4바이트 둘이 붙어있으니 )

개선 방법은 구조체를 분리하거나, a , b 변수 사이에 큰 데이터 int arr[16] (64바이트) 이렇게 두면 한 캐시 라인을 공유하지 않게 되므로 해결 가능. 

## 5.4 봉화희제후와 메모리 장벽

이 이야기와 코드가 강조하는 것은, **스레드 간 동기화 없이 공유 변수를 사용할 경우 예상치 못한 버그가 발생할 수 있다**는 점

### 5.4.8 잠금 프로그래밍과 잠금 없는프로그래밍

꽤 중요한 내용.

### **잠금 프로그래밍 vs. 잠금 없는 프로그래밍 요약**

| 구분          | 잠금 프로그래밍 (Lock-based)                                 | 잠금 없는 프로그래밍 (Lock-free)                             |
| ------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **개념**      | 공유 자원 보호를 위해 상호 배제(Mutex) 사용                  | 공유 자원을 사용할 때도 대기하지 않음                        |
| **방식**      | 한 스레드가 리소스를 사용하면 다른 스레드는 대기             | 리소스 사용 중이면 다른 작업을 수행 후 재시도                |
| **종류**      | 1) **Mutex (운영 체제 대기 상태)**  2) **Spinlock (반복 검사, 바쁜 대기)** | 1) **CAS(Compare-and-Swap)**  2) **Lock-free data structures** |
| **장점**      | 구현이 쉽고 직관적                                           | 대기 상태 없이 성능 향상 가능                                |
| **단점**      | 성능 저하(대기 발생), 데드락 위험                            | 코드가 복잡하고 ABA 문제 등 해결 필요                        |
| **사용 예시** | 일반적인 멀티스레드 환경                                     | 실시간 시스템, 높은 성능이 필요한 경우                       |

------

### **설명**

#### **1. 잠금 프로그래밍 (Lock-based Programming)**

- 다중 스레드 환경에서 공유 리소스를 보호하기 위해 **상호 배제(Mutex)** 를 사용한다.
- 한 스레드가 리소스를 사용 중이면, 다른 스레드는 반드시 대기해야 한다.
- 종류:
  - **Mutex**: 운영 체제가 다른 스레드를 대기 상태로 변경 (비효율적일 수 있음)
  - **Spinlock**: 다른 스레드가 계속해서 잠금이 풀렸는지 확인 (CPU 사용량 증가)
- ReentrantLock, Mutex, Semaphore

#### **2. 잠금 없는 프로그래밍 (Lock-free Programming)**

- 공유 리소스를 사용 중이어도 다른 스레드는 대기하지 않고 다른 작업을 수행한다.
- 스레드가 대기하지 않기 때문에 **실시간 시스템**에서 중요하게 사용된다.
- **CAS(Compare-and-Swap)** 같은 원자적 연산을 사용하여 공유 리소스를 안전하게 업데이트한다.
- 구현이 어렵고 **ABA 문제** 같은 추가적인 해결책이 필요하다.
  - A 변수에 대해 연산을 하는 경우, A 변수가 안바뀌었으면 성공, 바뀌었으면 실패 및 재시도 처리를 해서 성공할때까지 재시도 하는것. 단 A 변수가 5 -> 6 -> 5 ( A -> B -> A)로 바뀐 것에 대해선 감지할 수 없음.
    - 일반적으로 **락을 획득하려는 것이 아니라, 데이터를 원자적으로 업데이트하는 목적**.
    - 이경우 뭐 버전 태그? 등을 사용해서 비교하기도 함. 
- Atomic 클래스들. 

즉 일반적인 상황에서는 **잠금 프로그래밍**을 사용하지만, 성능이 중요한 경우에는 **잠금 없는 프로그래밍**을 고려해야 한다.

잠금 없는 방식이 성능이 좋을 수 있지만, 코드가 복잡하고 디버깅이 어려울 수 있다.

**목표는 최소한의 잠금으로 효율적인 동기화를 구현하는 것**이다.





# 6장 입출력이 없는 컴퓨터가 있을까?

## 6.1 CPU는 어떻게 입출력 작업을 처리할까?

CPU 내부에 레지스터가 있는것처럼, 각각 IO 장치마다 장치 레지스터가 있다. 

* 서버에서는 ssd,hdd 데이터 저장장치, 네트워크 카드, 이더넷 포트, 라우터 스위치 등

CPU 레지스터는 메모리에서 읽은 데이터를 임시 저장하고

장치 레지스터는 데이터를 저장하는 레지스터와 제어 정보와 상태 정보를 저장하는 레지스터가 있따. 

### 6.1.1 입출력기계 명령어

어떻게 장치 레지스터를 읽고 쓰는가? 기계 명령어로 실행한다.

* x86은 IN, OUT 기계 명령어

장치마다 고유한 주소가 부여되어 입출력 명령어에 장치 주소를 지정한다. 

### 6.1.2 메모리 사상 입출력

LOAD, STORE 명령어는 메모리를 읽고 쓰는지, 장치 레지스터를 읽고 쓰는 것인지 구분할 수 없다. 

메모리 주소 공간 일부를 장치에 할당하여, 메모리를 읽고 쓰는것처럼 장치를 제어한다.

* 이 방법이 ememory mapping input and output 이다. 

즉 컴퓨터 저수준 계층에서는 두가지 입출력 구현 방법이 있따.

1. 특정 입출력 기계 명령어 사용
2. 메모리의 읽기 쓰기 명령어를 함께 쓰지만, 주소 공간의 일부를 장치에 할당한다.

CPU는 특정한 방법을 사용하여 장치의 작업 상태를 얻어온다. 

장치의 작업 상태는 장치 상태 레지스터에 저장하고 CPU가이걸 읽어간다

### 6.1.4 폴링 계속 검사하기

키보드에서 누른 키의 데이터를 레지스터가 oxfe01 위치에 사상

상태 레지스터는 주소 공간의 oxfe 00위치에 사상이라면

```
START
    LOAD R1, 0xFE00    ; R1 레지스터에 메모리 주소 0xFE00의 값을 로드
    BLZ START          ; R1이 0보다 작으면(음수이면) START로 분기 (무한 루프 가능성)
    LOAD R0, 0xFE01    ; R0 레지스터에 메모리 주소 0xFE01의 값을 로드
    BL OTHER_TASK      ; 무조건 OTHER_TASK 라벨로 분기 (브랜치 명령)
```

BLZ START는 현재 키보드 상태 레지스터 값이 0일때 즉 아무것도 누르지 않으면 시작 위치 START로 점프하여 다시 확인하는 루프이다.

이 방법은 폴링 방식이라고 부른다.

폴링은 동기식 설계방식이며, 매우 비효율적으로 보인다. 이를 비동기로 바꾸어 처리할 수 있다.

### 6.1.5 인터럽트 처리 (중단 처리)

<img src="./images//image-20250309012406074.png" width = 450>

CPU가 특정 프로세스의 명령어를 처리하고 있을 때, 네트워크 카드에 데이터가 들어오면 인터럽트 신호를 보낸다.

CPU는 현재 작업의 우선순위가 인터럽트 요청보다 높은지 판다하고 인터럽트가 높다면 현재 작업을 중단하고 인터럽트를 처리 그리고 원래 작업으로 돌아온다.



즉 이것을 봤을때 비동기로 계속 중단 -> 실행 -> 리턴 -> 중단 -> 실행 -> 리턴이 반복되는것이다.

그러나

CPU는 인터럽트 신호가 오는것을 어떻게 감지할까?

중단된 프로그램의 실행 상태를 저장하고 복원하는 방법은?

### 6.1.7 CPU는 인터럽트 신호를 어떻게 감지할까.

CPU가 명령어를 실행하는 과정은, 명령어 인출, 해독, 실행, 다시쓰기 같은 단계로 나뉜다.

여기에 인터럽트 신호를 감지하는 단계까 추가되어야 한다.

**하드웨어 인터럽트 (Hardware Interrupt)**

CPU 외부에서 **입출력 장치(I/O Device), 타이머, 오류 등**이 발생했을 때 **하드웨어 신호**를 통해 인터럽트를 전달

- 예: 키보드 입력, 마우스 클릭, 네트워크 패킷 도착 등
- **CPU가 인터럽트 요청(IRQ - Interrupt Request) 라인을 주기적으로 확인하면서 감지**.

🛠 **인터럽트 감지 과정**

1. CPU가 명령을 실행하는 도중 **인터럽트 컨트롤러(Interrupt Controller)** 가 인터럽트 신호를 보냄.
2. 현재 실행 중인 명령어를 완료한 후, **인터럽트 벡터 테이블(Interrupt Vector Table, IVT)** 에서 해당 인터럽트 핸들러(Interrupt Handler) 주소를 가져옴.
3. **해당 핸들러(Interrupt Service Routine, ISR)를 실행**하여 인터럽트 처리를 수행.
4. 인터럽트 처리가 끝나면 원래 실행 중이던 명령어로 복귀.



인터럽트 처리는 일반 함수의 호출과 비교했을 때 점프와 반환을 포함하고 있따는 것이 함수와 유사하다.

그리고 인터럽트도 인터럽트에 의해 중단될 수 있는데 어떻게 실행 상태를 저장하고 복원할까

자세히 보면 스택으로 구현할 수 있다. 

각 상태를 PC 레지스터와 상태 레지스터에서 복원하면서 계속 실행한다.

## 6.2 디스크가 입출력 처리시 CPU가 하는 일은?

최신 컴퓨터의 경우 디스크가 입출력 처리시 CPU 개입이 필요하지 않다.

CPU는 다른 스레드를 실행중이거나 커널 모드에서 커널 프로그램을 실행하느라 바쁠 수도, 유휴 일수도 있따.

![image-20250309013134247](./images//image-20250309013134247.png)

디스크의 입출력 처리와 CPU가 실행하는 작업은 그림 6-9와 같이 서로 의존하지 않는 독립적 인 두 작업이므로 병행 처리가 가능하다.

디스크가 입출력 요청을 처리하는 전체 과정에서 왜 CPU 개입이 필요하지 않을까?

이를 이해하려면 장치 제어기, 직접 메모리 접근, 인터럽트의 관계를 이해해야 한다.

### 6.2.1 장치 제어기

디스크같은경우, 자체 마이크로 컴퓨터 시스템으로 발전하여 자체적인 프로세서와 펌웨어를 갖추고 있다.

이와 동시에 자신만의 버퍼나 레지스터를 갖추고 있어 장치에서 읽은 데이터나 장치에 저장할 데이터를 저장할 수 있다.

* 장치 드라이버 : 운영체제에 속한 코드
* 장치 제어기: 장치 드라이버에서 명령을 받아 외부 장치를 제어하는 하드웨어

장치 제어기는 OS에 해당하는 드라이버와 외부 장치를 연결하는 다리이며,

장치 제어기가 점점 더 복잡해지는 목적 중 하나가 바로 CPU를 해방시키는 것

### CPU가 직접 데이터를 복사해야 할까

CPU입장에서 데이터를 직접 복사하는 일은 리소스를 낭비하는 일이다.

그러나 데이터는 항상 장치와 메모리 사이에 전송되어야 한다. 이 작동 방식을 직접 메모리 접근이라고 한다

### 직접 메모리 접근

**직접 메모리 접근(DMA, Direct Memory Access)** 는 **CPU의 개입 없이** I/O 장치가 직접 메모리(RAM)와 데이터를 주고받을 수 있도록 하는 기능

1. **CPU가 DMA 컨트롤러(DMA Controller, DMAC)에 작업을 요청.**
2. **DMA 컨트롤러가 직접 I/O 장치와 RAM 간 데이터 전송 수행.**
3. **전송이 끝나면 인터럽트(IRQ)로 CPU에게 알림.**
4. **CPU는 데이터 전송이 완료된 후 결과만 확인.**

📌 **장점:**

*  CPU가 데이터 전송 과정에서 **직접 개입하지 않아 부하 감소**
*  I/O 장치와 RAM이 직접 데이터를 교환하여 **전송 속도 증가**
*  고속 데이터 처리가 필요한 **디스크, 네트워크, 멀티미디어 처리에 최적화**

### 6.2.4 전 과정 정리

스레드 1이 시스템 콜로 입출력 요청 시작시, OS는 스레드 1을 일시 중단하고 CPU를 스레드 2에 할당 하고 시작

이때 디스크가 동작하여 데이터 준비가 완료되면 DMA 장치가 직접 장치와 메모리 사이에서 데이터를 전송

데이터 전송이 완료되면 DMA 장치가 인터럽트를 이용해 CPU에게 알리고, CPU는 스레드 2의 실행을 일시 중지하고 DMA의 인터럽트를 처리. 

이때, OS는 스레드 1이 요청한 IO 작업이 처리된것을 확인했기 때문에 CPU를 다시 스레드 1에 할당하기로 결정하며 이어서 실행됌.

**여기에서 핵심은 디스크가 입출력 요청을 처리할 때 CPU가 그 자리에서 기다리지 않고 다른 스레드를 실행시키는것** 

## 6.3 파일을 읽을 때 프로그램에는 어떤 일이 발생할까?

### 6.3.1 메모리 관점에서 입출력

입출력은 사실 데이터 복사다. 디스크 <-> 메모리간 데이터를 복사하는것이다.

### 6.3.2 read 함수는 어떻게 파일을 읽을까.

파일을 읽는 코드는 일반적으로 데이터를 저장하는 버퍼를 정의한 후 read 계열의 함수를 호출한다.

외부장치가 io 작업을 실행하는것은 느리기 때문에 끝나기 전까지 프로세스는 진행될 수 없어 이것이 블로킹이다.

<img src="./images//image-20250309015115861.png" width = 450>

read가 호출되면 버퍼를 매개채로 비어두고, OS는 DMA에 요청을 해서 특정 메모리 영역으로 데이터를 복사한다.

이 메모리 영역이 매개체인 버퍼다.

* 블로킹 대기열 외에도 os에는 준비 완료 대기열이 존재하는데, 이 대기열은 다시 실행될 조건이 준비된 작업이 들어간다.

복사하는 동안 cpu는 프로세스 B를 실행하며 디스크(DMA)는 프로세스 A의 메모리 버퍼에 데이터를 쓴다.

이렇게 OS가 매우 바쁘게 CPU를 스케줄링 한다.

프로세스 A의 버퍼에 복사하는 작업이 완료되면 디스크는 CPU에 인터럽트 신호를 보내고, CPU는 해당 함수로 점프하여 프로세스 A가 이어 실행될 수 있다. 

OS가 준비 완료 대기열에 프로세스 A를 넣고 OS는 기존 실행되던 프로세스 B의 CPU 할당 시간이 끝나거나 인터럽트 받으면, 그제서야 A를 꺼내서 실행한다.

**그리고 파일 데이터는 직접 프로세스의 주소공간이 아닌, 일반적으로 먼저 OS 내부로 복사되며, 이후 운영 체제가 프로세스의 주소 공간으로 복사한다.** 

물론, OS를 우회하여 직접 데이터를 프로세스 주소공간에 복사하는 zero-copy 기법도 있다.

* 외부 api 호출시에도 프로세스 -> os -> NIC 카드로 전송되는데 여기서도 zero-copy 기법이 쓰일 수 있음. 
* zero-copy는

java.nio.FileChannel 사용시 zero-copy 가능

```java
import java.io.FileInputStream;
import java.io.FileOutputStream;
import java.nio.channels.FileChannel;

public class ZeroCopyExample {
    public static void main(String[] args) {
        String sourceFile = "source.txt";
        String destFile = "destination.txt";

        try (FileChannel sourceChannel = new FileInputStream(sourceFile).getChannel();
             FileChannel destChannel = new FileOutputStream(destFile).getChannel()) {
            
            long position = 0;
            long size = sourceChannel.size();

            // Zero-Copy 전송
            sourceChannel.transferTo(position, size, destChannel);
            
            System.out.println("Zero-Copy 전송 완료!");
        } catch (Exception e) {
            e.printStackTrace();
        }
    }
}

```

읽는 방식

```java
import java.io.RandomAccessFile;
import java.nio.MappedByteBuffer;
import java.nio.channels.FileChannel;

public class ZeroCopyMemoryMapped {
    public static void main(String[] args) {
        String filePath = "source.txt";

        try (RandomAccessFile file = new RandomAccessFile(filePath, "r");
             FileChannel fileChannel = file.getChannel()) {

            // 파일 크기 확인
            long fileSize = fileChannel.size();

            // Memory-mapped I/O (Zero-Copy 방식)
            MappedByteBuffer buffer = fileChannel.map(FileChannel.MapMode.READ_ONLY, 0, fileSize);

            // 데이터를 읽음
            for (int i = 0; i < fileSize; i++) {
                System.out.print((char) buffer.get());  // 파일 내용을 출력
            }

            System.out.println("\nZero-Copy Memory-Mapped I/O 완료!");

        } catch (Exception e) {
            e.printStackTrace();
        }
    }
}
```

### **Java NIO (New I/O)**

- `FileChannel.transferTo()` → `sendfile()` 기반 Zero-Copy 지원.
- `MappedByteBuffer` → `mmap()`을 사용하여 메모리 매핑.
- `DirectByteBuffer` → Heap을 거치지 않는 DirectBuffer 사용.

kafka, netty, cassandra, 등이 제로 카피 많이 사용함

## 6.4 높은 동시성의 비결 : 입출력 다중화

유닉스/리눅스 세계는 모든 것이 파일이다.

모든 입출력 장치도 파일이라는 개념으로 추상화 된다. 

* 디스크, 네트워크 데이터, 터미널, pipe...

모든 입출력 작업은 파일 읽기와 쓰기인 open, read, write를 이용해 읽고 쓸 수 있다.

seek를 이용해 읽고 쓰는 pointer 위치를 변경할 수 있으며 close로 파일을 닫을 수 있다.

### 6.4.1 파일 서술자 - file discriptor

```
read(buffer)
```

이상태로 어디서 읽는지 알 숙 ㅏ없다.

때문에 리눅스/유닉스 세계에서 파일의 위치를 지정해야 하는데, 파일 디스크립터를 이용한다.

파일을 열면 커널은 파일 디스크립터를 반환하며, 파일 작업 실행시에도 해당 디스크립터를 반환해야 한다.

커널은 이 파일 디스크립터(파일 숫자)로 파일을 찾아 작업을 완료한다.

```java
char buffer[LEN];
int fd = open(file_name);

read(fd_buffer)

-- 자바는 내부적으로 FileDescriptor 객체 사용함. 
import java.io.FileDescriptor;
import java.io.FileInputStream;
import java.io.IOException;
import java.lang.reflect.Field;

public class FileDescriptorExample {
    public static void main(String[] args) {
        try (FileInputStream fis = new FileInputStream("example.txt")) {
            FileDescriptor fd = fis.getFD();

            // FileDescriptor에서 실제 파일 디스크립터 번호 얻기 (리눅스에서만 가능)
            int fdInt = getFileDescriptorInt(fd);
            System.out.println("파일 디스크립터 번호: " + fdInt);

        } catch (IOException e) {
            e.printStackTrace();
        }
    }

    private static int getFileDescriptorInt(FileDescriptor fd) {
        try {
            Field field = FileDescriptor.class.getDeclaredField("fd");
            field.setAccessible(true);
            return field.getInt(fd);
        } catch (Exception e) {
            throw new RuntimeException("파일 디스크립터 번호를 가져올 수 없음", e);
        }
    }
}
```

### 6.4.2 다중 입출력을 어떻게 효율적으로 처리할까

http 통신은 소켓을 사용한다. 리눅스에서 소켓도 파일이다. 때문에 파일 디스크립터를 이용ㅎㄴ다.

3 way handshake 성공시 accept 함수 호출하여 연결을 얻고 파일 디스크립터를 얻는다.

이것으로 사용자와 통신한다

```
int conn_fd = accept(...);

if (read(conn_fd, buff) > 0) {
  작업(buff);
}
```

read는 일반적으로 블로킹이다.

때문에 여러 스레드를 만들어 각각 요청마다 사용해서 스레드가 블로킹되도 다른 스레드나 사용자 요청에 영향을 덜 미치게 할 수 있다.

이 방법이 스레드 퍼 리퀘스트 모델이며, 단점은 스레드 수가 많아질 수 있고 자원을 많이 소모하며, 전환에 많은 부담이 가해진다. 즉 높은 동시성을 얻을 수 없다.

이 문제의 핵심은, 파일 디스크립터 하나에 대응하는 IO 장치가 읽을 수 있는 상태인지, 미리 알수 없다는 것이다.

읽을 수 없거나 쓸 수 없는 상태에서 IO 요청을 보내봤자 스레드가 블로킹 되어 일시 중지 되기 때문이다.

아래처럼 생각해보자

### 6.4.3 상대방이 아닌 내가 전화하게 만들기.

read함수를 사용하면, 해당 파일 디스크립터에 대응하는 파일을 읽을 수 잇는지 여부를 매번 물어봐야 한다.

이보다 나은 방법은, 내가 관심잇는 대상인 파일 디스크립터를 커널에 알려주고, 커널에게 너가 대신 감시하다가 사용할 수 있는 디스크립터가 있으면 알려줘 라고 알려준다.

이 방법이 동시에 많은 수의 파일 디스크립터를 다룰 수 있는  입출력 다중화 기술이다. 

### 6.4.4 입출력 다중화

입출력ㄷ ㅏ중화는 다음과 같은 과정을 의미한다

1. 파일 디스크립터 획득.
2. 특정 함수 호출하여 커널에 알림. 이 함수를 먼저 리턴할테니, 이 파일 디스크립터를 감시하다 읽거나 쓸 수 있으면 반환해줘
3. 해당 함수가 반환되면 준비가 완료된 파일 디스크립터를 얻으므로, 이를 얻어 처리함

**리눅스 세계에서 입출력 다중화 기술을 사용하는 방법에는 select, poll, epoll 세 가지가 있다. 매우 중요** 

### 6.4.5 select, poll, epoll

비동기 IO 이벤트 처리 방식으로, 네트워크 소켓, 파일 디스크립터, IPC(Inter-Process Communication) 같은 **입출력(I/O) 이벤트를 감시하는 기법**

**주요 차이점은 성능, 확장성, 구현 방식**

| 방식       | 구조            | 성능                 | 단점                                        |
| ---------- | --------------- | -------------------- | ------------------------------------------- |
| **select** | 비트마스크 기반 | **느림 (O(N))**      | 파일 디스크립터(FD) 개수 제한 (1024~4096개) |
| **poll**   | 배열 기반       | **느림 (O(N))**      | FD 개수 제한 없음, 하지만 매번 전체 스캔    |
| **epoll**  | 이벤트 기반     | **빠름 (O(1)~O(N))** | 리눅스 전용 (Windows에서 지원 안 됨)        |

**즉, `select` → `poll` → `epoll` 순으로 성능이 개선됨.**
 **FD 개수가 많을수록 epoll이 유리함 (수천 개 이상의 FD를 다룰 때).**



select는 1024개 제한이 있으며, 호출은 블로킹된다. 파일 디스크립터 중 하나라도 읽기,쓰기 가능 이벤트가 나타나면 호출한 프로세스가 깨어난다. 하지만 프로세스가 깨어났을 때 어떤 파일 디스크립터가 사용가능한지 몰라서 처음부터 끝까지 확인해야 해서 O(n)의 시간이 걸리며, 효율이 매우 떨어지는 근본적인 원인이다.



poll과 select는 유사하며, poll이 select에 비해 최적화된 점은 감시 가능한 파일 디스크립터가 제한만 없다는것. 하지만 여전히 O(n)이다.



이 문제를 해결하려고 epoll이 나왔다.

파일 디스크립터 목록을 감시하다가, 특정 이벤트가 발생하면 해당 프로세스를 깨우면서 준비 완료된 파일 디스크립터가 준비 완료 목록에 추가된다. 프로세스가 처음부터 끝까지 확인할 필요 없이 바로 준비 완료된 파일 서술자를 직접 획득할 수 있다.

* **이벤트 기반 I/O 감지 (O(1)~O(N))** → FD 변경이 적으면 O(1)
* **epoll_ctl()을 통해 동적으로 FD 등록 및 삭제 가능**
* 두 가지 모드 지원
  1. **LT(Level-Triggered)**: 이벤트가 발생하면 계속 알림
  2. **ET(Edge-Triggered)**: 이벤트가 발생할 때만 알림 (고성능)

```c
#include <sys/epoll.h>
#include <stdio.h>
#include <unistd.h>

int main() {
    int epfd = epoll_create1(0);
    struct epoll_event event;
    event.events = EPOLLIN;
    event.data.fd = STDIN_FILENO;
    
    epoll_ctl(epfd, EPOLL_CTL_ADD, STDIN_FILENO, &event);

    struct epoll_event events[1];
    int ret = epoll_wait(epfd, events, 1, 5000);  // 5초 대기

    if (ret > 0) {
        printf("입력 감지됨\n");
    } else if (ret == 0) {
        printf("타임아웃\n");
    } else {
        printf("오류 발생\n");
    }

    close(epfd);
    return 0;
}
```

**장점**

- **이벤트 기반 비동기 I/O 감지** (`epoll_wait()`)
- **동적 FD 관리 가능 (`epoll_ctl()`)** → FD 추가/삭제 가능
- **O(1) 또는 O(N) 복잡도** (FD 변경이 적을수록 빠름)
- **레벨 트리거(LT) & 엣지 트리거(ET) 지원**
- **비블로킹 I/O 지원** (polling 없이 효율적 처리)

**단점**

- **리눅스 전용 (Windows에서 지원 안 됨)**
- **ET 모드는 구현이 까다로움**

```
epoll 주요 함수
epoll_create() → epoll 인스턴스 생성
epoll_ctl() → fd를 epoll에 추가/삭제/수정
epoll_wait() → 등록된 fd에서 이벤트가 발생할 때까지 대기

epoll은 내부적으로 red-black tree(RB 트리) 와 ready list(링크드 리스트) 를 사용

epoll_ctl() → RB 트리를 사용하여 fd를 저장 및 관리
epoll_wait() → ready list에서 이벤트가 발생한 fd를 O(1)로 조회 가능

epoll_wait()는 이벤트가 준비된 fd만 반환하므로, 감시하는 fd 개수와 관계없이 O(1) 성능을 유지할 수 있음.
fd 개수가 10개든 10만 개든 이벤트가 적으면 epoll_wait()는 O(1)로 동작
```

**fd 변경이 많으면 왜 느려지는가?**

epoll의 `epoll_ctl()`은 **RB 트리를 사용하여 fd를 관리**하는데,
 **RB 트리에 새로운 fd를 추가하거나 삭제하는 작업은 O(log N)의 시간 복잡도**가 발생

### **비교: epoll_wait() vs epoll_ctl()**

| 연산                     | 시간 복잡도 | 설명                                 |
| ------------------------ | ----------- | ------------------------------------ |
| `epoll_wait()`           | O(1)        | ready list에서 이벤트 발생 fd만 조회 |
| `epoll_ctl(ADD/DEL/MOD)` | O(log N)    | RB 트리에서 fd 추가/삭제/수정        |

따라서,

- **fd 변경이 적다면 `epoll_wait()`만 사용되므로 O(1)로 빠름**
- **fd 변경이 많다면 `epoll_ctl()`이 자주 호출되면서 O(log N) 비용이 누적됨 → 성능 저하**

특히 **대규모 네트워크 서버**에서 수천 개의 소켓이 빠르게 열리고 닫히는 경우(예: 웹 서버, 프록시 서버),
 **`epoll_ctl()` 호출이 많아져 CPU 사용량이 증가**하고 성능이 떨어질 수 있음.

변경이 많은경우 Linux 5.1 이상에서는 **io_uring**이 `epoll`보다 성능이 우수

* `io_uring`은 **Linux 커널 5.1**(2019년)에서 도입된 **비동기 I/O 프레임워크**로,
   기존의 `epoll()`, `select()`, `poll()` 같은 동기적 I/O 모델을 개선하여 **고성능 비동기 I/O를 지원**하는 기술

## 6.5 Mmap : 메모리 읽기 쓰기 방식으로 파일 처리

`mmap()`은 **파일 또는 디바이스의 데이터를 메모리에 매핑하는 시스템 호출**로,
 파일 I/O 성능을 최적화하고, 프로세스 간 공유 메모리(IPC)로도 사용할 수 있는 강력한 메커니즘



직접 메모리에 디스크의 파일을 올려 읽고 쓸 수 있다.

### 6.5.1 파일과 가상 메모리

메모리처럼 디스크에 저장된 파일도 연속된 공간에 저장되는것처럼 보인다.

가상 메모리를 이용해서 이 둘을 연관 지을 수 있다.

프로세스가 사용하는 주소 공간은 가상이기 때문에, 개념적으로 연속된 디스크 공간에 저장되어 있을 수 있다고 연속적으로 직접 사상할 수 있다. 

길이가 200바이트인 파일을 가상 메모리에 600~799 범위에 사상하면 된다

### 운영체제 덕분

600~799 주소를 읽을 때, 메모리에 적재되지 않았따면 페이지 폴트 인터럽트가 발생할 수 있다.

이때 os가 인터럽트를 처리하고, 실제 디스크를 읽고 파일을 메모리로 읽고 나서 가상 메모리랑 매핑한다.

쓰기작업은

이 메모리를 직접 수정하고, OS는 백그라운드에서 해당 내용을 디스크에 기록한다.

실제 디스크를 읽고 쓰는건 os가 하며, 프로그램은 일반 메모리를 이용하는것처럼 보인다.

### 6.5.3 mmap vs read/write

read 함수 사용시 커널 상태에서 사용자 상태로 복사해야 하며

write 함수 사용시 사용자 상태에서 커널 상태로 복사해야 한다. 이 작업들은 큰 리소스를 먹는다.

mmap은 이런 문제가 없다. 시스템 콜 부담이 없기 때문.

그러나 페이지 폴트 인터럽트 요청에 부함이 있고 커널마다 구현 방식이 달라서 

mmap이 항상 성능 면에서 read/write 함수보다 낫다고 할 수는 없다. 





**mmap()의 장점**

1. **Zero-Copy 지원 (불필요한 복사 제거)**
   - 기존 `read()` / `write()` 방식은 데이터를 유저 공간으로 복사해야 함
   - `mmap()`은 직접 메모리에 매핑하여 복사 없이 사용 가능
   - 메모리 매핑이 되어 있으므로, `memcpy()` 등의 연산을 수행하면 속도가 훨씬 빠름
2. **페이지 폴트(Page Fault) 기반 I/O (성능 최적화)**
   - `mmap()`은 처음부터 모든 데이터를 로드하지 않고, 필요한 데이터만 로드 (Page Fault 발생 시)
   - 대용량 파일을 로드할 때 매우 유용
3. **프로세스 간 공유 메모리 (IPC) 가능**
   - `MAP_SHARED` 플래그를 사용하면 여러 프로세스 간 메모리 공유 가능
   - IPC(Inter-Process Communication) 기법으로 활용 가능
4. **HugePage, Direct I/O 등과 조합하여 성능 최적화 가능**
   - `mmap()`은 HugePage(대용량 페이지 지원)와 함께 사용하면 TLB(Cache Miss)를 줄여 성능 향상 가능
   - `O_DIRECT` 플래그와 조합하여 디스크 I/O 성능을 극대화할 수 있음

**mmap() vs read() / write() 비교**

| 기능             | `mmap()`                     | `read()` / `write()`       |
| ---------------- | ---------------------------- | -------------------------- |
| I/O 방식         | 메모리 직접 접근 (Zero-Copy) | 시스템 콜 필요 (복사 발생) |
| 성능             | 빠름                         | 느림                       |
| 메모리 사용      | 파일 크기만큼 필요           | 버퍼 크기만큼 사용         |
| 페이지 폴트 활용 | 지원 (필요할 때만 로드)      | 불가능                     |
| 프로세스 간 공유 | 가능 (`MAP_SHARED`)          | 불가능                     |

즉, **`mmap()`은 대용량 파일 처리와 IPC에 적합**하고, **`read()`는 간단한 파일 처리에 적합**함.

### mmap()의 단점

1. **대량의 작은 파일 처리에는 부적합**
   - `mmap()`은 파일을 메모리에 매핑하는 과정에서 오버헤드 발생 가능
   - 작은 파일을 반복해서 처리해야 한다면 `read()`가 더 적합할 수도 있음
2. **메모리 부족 가능성**
   - 파일 크기만큼 가상 메모리가 필요하므로, 메모리가 부족하면 `mmap()`이 실패할 수 있음
   - 스왑이 많이 발생할 경우 성능 저하 가능
3. **SIGBUS 오류 발생 가능**
   - `mmap()`한 메모리를 접근할 때, 파일이 삭제되거나 변경되면 `SIGBUS` 오류 발생 가능
   - `read()`는 파일 변경에도 안전하지만, `mmap()`은 데이터 손실 가능성 있음

즉 대형 파일에서는 mmap이 좀 더 낫다!

## 6.6 컴퓨터 시스템의 각 부분에서 얼마큼 지연이 일어날까?

갓 제프딘 성님께서 정리한 통계 자료이다.

* 2012년 기준이라서 현대는 더 빠르겠지만.. 그래도 시스템 수치 계싼시 쓸모있다.

**시간 단위: ns = 나노초, μs = 마이크로초, ms = 밀리초, s = 초**)

* 1 밀리초(ms) = 1,000 마이크로초(μs)
* 1 마이크로초(μs) = 1,000 나노초(ns)
* 1 밀리초(ms) = 1,000,000 나노초(ns)**

| 작업                                           | 지연 시간 (latency) | 설명                                                  |
| ---------------------------------------------- | ------------------- | ----------------------------------------------------- |
| **CPU L1 캐시 접근**                           | **0.5 ns**          | CPU 내부의 가장 빠른 캐시 (한 사이클 내 접근)         |
| **CPU L2 캐시 접근**                           | **7 ns**            | L1보다 느리지만 여전히 매우 빠름                      |
| **CPU L3 캐시 접근**                           | **30 ns**           | 여러 코어가 공유하는 캐시                             |
| **RAM (메인 메모리) 접근**                     | **100 ns**          | L3 캐시보다 3배~10배 느림                             |
| **1MB 데이터 순차적 읽기 (RAM 내)**            | **250 ns**          | CPU가 캐시 미스를 내고 데이터를 메모리에서 가져올 때  |
| **SSD에서 데이터 읽기**                        | **100 μs (0.1 ms)** | 최신 NVMe SSD는 더 빠를 수도 있음                     |
| **HDD에서 데이터 읽기 (디스크 탐색 포함)**     | **10 ms**           | 회전하는 디스크에서 특정 데이터를 찾는 데 걸리는 시간 |
| **1GB 네트워크 전송 (데이터센터 내)**          | **500 μs (0.5 ms)** | 데이터센터 내부의 네트워크 전송                       |
| **데이터센터 내 RPC 호출**                     | **500 μs - 1 ms**   | 서버 간 원격 프로시저 호출 (RPC)                      |
| **데이터센터 간 네트워크 왕복 시간 (대륙 내)** | **30 ms**           | 미국 내 데이터센터 간 네트워크                        |
| **데이터센터 간 네트워크 왕복 시간 (대륙 간)** | **150 ms**          | 미국 ↔ 유럽 간 네트워크                               |
| **1GB 데이터 디스크에서 읽기 (HDD, 순차적)**   | **10 ms**           | HDD의 순차적 읽기                                     |
| **1GB 데이터 디스크에서 읽기 (SSD, 순차적)**   | **1 ms**            | SSD의 순차적 읽기                                     |
| **1GB 데이터 네트워크 전송 (WAN, 대륙 간)**    | **150 ms**          | 미국 ↔ 유럽 간 데이터 전송                            |



네트워크 지연시간 공식 = 

```
지연 시간 (ns) = (거리 (km) / 광케이블 속도 (km/s)) × 10^9

왕복 지연 시간 (RTT, ns) = 단방향 지연 시간 × 2

- 10^9 는 1,000,000,000 (10억)
- 일반적으로 광섬유(광케이블) 내 빛의 속도는 약 200,000 km/s
```

| 도시 간 거리           | 거리 (km) | 단방향 지연 시간 (ms) | 왕복 지연 시간 (ms) | 계산 공식                          |
| ---------------------- | --------- | --------------------- | ------------------- | ---------------------------------- |
| **서울 ↔ 부산**        | 325       | 1.625                 | 3.25                | (325 km) / (200,000 km/s) * 10^9   |
| **서울 ↔ 제주**        | 450       | 2.250                 | 4.50                | (450 km) / (200,000 km/s) * 10^9   |
| **서울 ↔ 베이징**      | 955       | 4.775                 | 9.55                | (955 km) / (200,000 km/s) * 10^9   |
| **서울 ↔ 일본 (도쿄)** | 1160      | 5.800                 | 11.60               | (1160 km) / (200,000 km/s) * 10^9  |
| **서울 ↔ LA**          | 9600      | 48.000                | 96.00               | (9600 km) / (200,000 km/s) * 10^9  |
| **서울 ↔ 실리콘밸리**  | 9500      | 47.500                | 95.00               | (9500 km) / (200,000 km/s) * 10^9  |
| **서울 ↔ 뉴욕**        | 11000     | 55.000                | 110.00              | (11000 km) / (200,000 km/s) * 10^9 |

